{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Optimizing Mind 2023\n",
    "Optimizing Mind Jupyter notebook for benchmarking transfer learning.\n",
    "This code compares transfer learning for cats and dogs and is based on the tesor flow tutorial and code.\n",
    "The code: \n",
    "1) initiates the Optimizing Mind API \n",
    "2) downloads cats dogs data \n",
    "3) runs the tensor flow example learning code\n",
    "4) runs OM API learnin on the exact same data samples as in 3\n",
    "5) displays verification and learning curves\n",
    "\n",
    "Optimizing Mind User Guide for API https://docs.google.com/document/d/1Wc2j1Uq6euhuYCsEbvcC8V5xvr5Z4JBf\n",
    "Steps to run Optimizing Mind API\n",
    "1) Register and obtain API token https://om-learn-api.azurewebsites.net/\n",
    "2) copy your token and place in \\<token\\>\n",
    "\n",
    "Changes from original TF tutorial code\n",
    "1) top layer changed from binary to multiclass to allow for multiple labels (no penalty in performance)\n",
    "2) batch size is changed from the original 32 to 1 in order to be able to better observe OM learning curve (no penalty in performance)\n",
    "3) TF training loop code changed to a manual one (for loop instead of fit)\n",
    "4) options added for different datasets\n",
    "5) options added for different bases\n",
    "original code: https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/images/transfer_learning.ipynb\n",
    "\n",
    "Note code is slow because lots of validation is run on both models, but it produces detailed learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Send_to_API(base_name,dataset_name, top_layer_inputs, labels,num_outputs,verbose=False):\n",
    "    try:\n",
    "        top_layer_inputsL = top_layer_inputs.tolist() # inputs\n",
    "        labelsL = labels.tolist() #labels\n",
    "        num_outputsL = str(num_outputs) \n",
    "\n",
    "    except Exception as e:\n",
    "        if verbose: print(e)\n",
    "        return(e) \n",
    "\n",
    "    username = os.getenv('JUPYTERHUB_USER')\n",
    "    payload = {\"model_name\": \"test_model99\",'inputs': top_layer_inputsL, 'labels': labelsL,'num_outputs':num_outputsL}\n",
    "    headers = { 'content-type': 'application/json', 'accept': 'application/json','Authorization':'Bearer <token>' }\n",
    "\n",
    "    r = requests.post(url = 'https://om-learn-api.azurewebsites.net/api/train/', data = json.dumps(payload), headers=headers)\n",
    "    result = json.loads(r.text)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_from_API(data): # within compiled code\n",
    "    weights = np.array(data[\"result\"]) \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now=datetime.now()\n",
    "date_time=now.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =1 #32   #1\n",
    "IMG_SIZE = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up the directories automatically  \n",
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets to benchmark\n",
    "\n",
    "dataset_name='cats_and_dogs_filtered'  #originalTF notebook dataset\n",
    "#dataset_name='Animals_filtered'  # 50 animal dataset\n",
    "#dataset_name='fowl_data' # from https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-pytorch\n",
    "\n",
    "PATH2 = os.path.join(os.path.dirname(path_to_zip),dataset_name )    \n",
    "print(PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = os.path.join(PATH2, 'train')\n",
    "validation_dir = os.path.join(PATH2, 'validation')\n",
    "\n",
    "print(PATH2,'\\n',train_dir,'\\n',validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                                         shuffle=True,\n",
    "                                                                         batch_size=BATCH_SIZE,\n",
    "                                                                         image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
    "                                                                              shuffle=True,\n",
    "                                                                              batch_size=BATCH_SIZE,\n",
    "                                                                              image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset class names:\", train_dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "num_outputs=len(class_names)\n",
    "print('number of classes and outputs:',num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of training batches: %d' % tf.data.experimental.cardinality(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 1 #10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = IMG_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose base\n",
    "\n",
    "MODEL = 'MobileNetV2'\n",
    "#MODEL = 'ResNet50'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL=='MobileNetV2':\n",
    "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "elif MODEL=='ResNet50':\n",
    "    preprocess_input = tf.keras.applications.resnet50.preprocess_input\n",
    "    base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "else:\n",
    "    error(\"Model Not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original adds 2d global_average_pooling2d layer to make the base layer ready for classification.  \n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up learning and top layer, changed from binary to multiclass so softmax added to top\n",
    "final_output_layer=tf.keras.layers.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the multiclass equivalent to original transfer learning network  \n",
    "prediction_layer4 = tf.keras.layers.Dense(num_outputs)  # change1 from original: I make num_outputs outputs\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = prediction_layer4(x)    # adds the 49 nodes\n",
    "outputs = final_output_layer(x)  # change2: adds softmax\n",
    "orig_TF_paradigm_model = tf.keras.Model(inputs, outputs)   # this is now the traditional leanring network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 3:SparseCategoricalCrossentropy from BinaryCrossentropy because it is multiclass\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "orig_TF_paradigm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True,) # Now I compile it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_TF_paradigm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the version of the base model called model_with_av_layer that will feed inputs to OM layer \n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "outputs = global_average_layer(x)\n",
    "#outputs = prediction_layer(x)   # It is basically the same as the original notebook but no prediction layer\n",
    "model_with_av_layer = tf.keras.Model(inputs, outputs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_av_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=model_with_av_layer.output.shape[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an equivalent TF model that will recieve the OM weights\n",
    "prediction_layer2 = tf.keras.layers.Dense(num_outputs)  # top nodes\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = prediction_layer2(x)  # nodes whose weights I will be changed by om\n",
    "outputs = final_output_layer(x)  #softmax\n",
    "tf_model_tobe_trained_by_OM = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it needs compile if I do validation but I never really do any learning so most of this doesnt really matter\n",
    "base_learning_rate = 0.0001\n",
    "tf_model_tobe_trained_by_OM.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True,) # needs to be added to prevent execution optimization (not data optimiziation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model_tobe_trained_by_OM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not validate at every point for graphing (otherwise things take forever)\n",
    "validate_points=np.concatenate([np.arange(30),np.arange(30,40,2),np.arange(40,60,3),np.arange(60,100,5),np.arange(100,300,7),np.arange(300,700,10),np.arange(700,1500,15),np.arange(1500,3000,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(validate_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the number of entries in validation set\n",
    "num_data_in_validation_dataset=0\n",
    "for data, label in validation_dataset: #.as_numpy_iterator():\n",
    "    num_data_in_validation_dataset=num_data_in_validation_dataset+len(data)\n",
    "print(num_data_in_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup validation manually so I dont have to go through base for every validation (saves time because validation is the same)\n",
    "i=0\n",
    "validation_labels=np.ones((1,num_data_in_validation_dataset))*50  #crazy initial values for debug\n",
    "net_out_validation_for_OM=np.ones((num_data_in_validation_dataset,num_inputs))*50  \n",
    "\n",
    "for data, label in validation_dataset: #.as_numpy_iterator():\n",
    "    len_batch=len(label)\n",
    "    validation_labels[0,(BATCH_SIZE*i):(BATCH_SIZE*(i)+len_batch)]=label  # set up validation truth table\n",
    "    temp=model_with_av_layer.predict(data)  #run bottom transfer layers pre-rfn\n",
    "    net_out_validation_for_OM[BATCH_SIZE*i:(BATCH_SIZE*i+len_batch),:]=temp[0:len_batch,:]\n",
    "    i=i+1\n",
    "validation_labels=validation_labels.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_labels[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables to store performance of batches for both approaches\n",
    "num_batches_in_train_dataset=len(train_dataset)              # number of batches in train_dataset\n",
    "print(\"num of data of train batches in a single epoch\",num_batches_in_train_dataset,\"batch size =\",BATCH_SIZE,\"epochs=\",initial_epochs)\n",
    "OM_learn_Vacc=[] \n",
    "tf_learn_Vacc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store baselines before learn\n",
    "store_baselines_before_learning=False\n",
    "if store_baselines_before_learning:\n",
    "    loss_, accuracy_ = orig_TF_paradigm_model.evaluate(validation_dataset)\n",
    "    print('validation of TF before train on new',accuracy_)\n",
    "    tf_learn_Vacc.append(accuracy_) #store data\n",
    "\n",
    "    OMmodel.test_data_score(net_out_validation_for_OM,validation_labels[0,:])  #validate for OM\n",
    "    print('validation of OM on NEW data before train on new',OMmodel.last_score_accuracy/100)\n",
    "    OM_learn_Vacc.append(OMmodel.last_score_accuracy/100) \n",
    "    \n",
    "    # add one more zero point and increase the index of everything by 1 to test before learning\n",
    "    graph_points=list(validate_points.copy()+1)\n",
    "    graph_points.insert(0,0)\n",
    "else:\n",
    "    graph_points=list(validate_points.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=False\n",
    "labels_presented=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for j in range(initial_epochs):\n",
    "    if j>0:  #update graph points if doing more than one epoch\n",
    "        validate_points=np.concatenate([np.arange(0,3000,30)])\n",
    "        graph_points[num_stored:]=[]\n",
    "        graph_points=graph_points+list(validate_points.copy()+len(labels_presented)+1)   #need to figure out graphing stuff\n",
    "\n",
    "    for i, data in train_dataset.enumerate():   \n",
    "        \n",
    "        labels_presented.append(data[1])  # store record of labels presented\n",
    "                \n",
    "        #the original learning        \n",
    "        batch_logs = orig_TF_paradigm_model.train_step(data)\n",
    "                \n",
    "        # prepare data to send out to OM.        \n",
    "        net_out = model_with_av_layer.predict_on_batch(data[0])  # run through base layer to have data ready for OM\n",
    "        \n",
    "        \n",
    "        # call to the API\n",
    "        json_back=Send_to_API(MODEL,dataset_name, net_out, data[1].numpy(),num_outputs, verbose=verbose)\n",
    "        \n",
    "        \n",
    "        outfrom_Get_from_API=Get_from_API(json_back) #extract weights from API results\n",
    "        \n",
    "        u_model=np.array(outfrom_Get_from_API).T\n",
    "        \n",
    "        \n",
    "        if verbose: \n",
    "            print(\"input size: \",net_out.shape,\"label size: \",data[1].shape)\n",
    "            print(\"label is:\",np.array(data[1]))\n",
    "            print(\"label is:\",data[1].numpy())\n",
    "            print(\"net_out\",net_out)\n",
    "            print(\"num_outpus\",num_outputs)\n",
    "            print(\"Json back in notebook\",json_back)\n",
    "            print(\"outfrom_Get_from_API\",outfrom_Get_from_API)\n",
    "            print(\"u_model.shape\",u_model.shape)\n",
    "        \n",
    "        # put weights that come back into final OM generated transfer learning network: \n",
    "        tf_model_tobe_trained_by_OM.trainable_weights[0].assign(tf.Variable(np.float32(u_model)))     #OMmodel.u_model.T))) \n",
    "        # this is now the TF equivalent of OM learned net \n",
    "        \n",
    "        # validation code run in specified validation points\n",
    "        if i in validate_points:  # minimizing number of validations because it takes too long\n",
    "        \n",
    "            # validating TF learned top layer\n",
    "            loss_, accuracy_ = orig_TF_paradigm_model.evaluate(validation_dataset)\n",
    "\n",
    "            # adding TF accuracies to the record\n",
    "            tf_learn_Vacc.append(accuracy_)  \n",
    "            \n",
    "            # validating OM learned top layer but equivalent net to TF\n",
    "            loss0, accuracy_OMTF = tf_model_tobe_trained_by_OM.evaluate(validation_dataset)  # it is validated and run just like the original TF\n",
    "\n",
    "            OM_learn_Vacc.append(accuracy_OMTF)\n",
    "\n",
    "            print(\"Training: Batch {}, Epoch {}, {} entries accuracy of OM {} and TF {} \".format(i,j, len(data[0]), np.round(OM_learn_Vacc[-1],4),np.round(accuracy_,4)))\n",
    "            \n",
    "    \n",
    "    num_stored=len(OM_learn_Vacc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(OM_learn_Vacc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# indicate when all animals are presented at least once\n",
    "all_presented=1\n",
    "while len(np.unique(labels_presented[0:all_presented])) != len(np.unique(labels_presented)):\n",
    "    all_presented=all_presented+1\n",
    "all_presented=np.where(np.array(graph_points) >= all_presented)[0][0]   #first index where all presented\n",
    "print('first validation run after all animals were presented:',all_presented,'\\nwhich begins with training instance',graph_points[all_presented])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grange=int(graph_points[num_stored]*.02)     #90  #31 #90 #121  # choosing a nice zoom range\n",
    "if grange==0:\n",
    "    grange=num_stored\n",
    "maxpoint=max(graph_points[0:grange])\n",
    "print(maxpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoomed figure\n",
    "print('first',graph_points[grange],'plots')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(graph_points[0:(all_presented+1)],OM_learn_Vacc[0:(all_presented+1)],color='darkblue')\n",
    "plt.plot(graph_points[all_presented:grange],OM_learn_Vacc[all_presented:grange],color='cornflowerblue', label='OM Accuracy NEW')\n",
    "plt.plot(graph_points[0:grange],tf_learn_Vacc[0:grange],color='orange', label='TF Accuracy NEW')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "plt.ylim([0.4,1.0])\n",
    "plt.title(\"OM vs TF {} Accuracy within {} Batch={} Example(s) Trained\".format(dataset_name,graph_points[grange],BATCH_SIZE))\n",
    "\n",
    "plt.xlabel('Batch Number')\n",
    "plt.show()\n",
    "print('Labels  ',np.array(labels_presented)[0:grange].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grange=np.int(num_stored*.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(graph_points),num_stored,graph_points[num_stored])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#unzoomed figure\n",
    "print('first',graph_points[grange],'plots')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(graph_points[0:(all_presented+1)],OM_learn_Vacc[0:(all_presented+1)],color='darkblue')\n",
    "plt.plot(graph_points[all_presented:grange],OM_learn_Vacc[all_presented:grange],color='cornflowerblue', label='OM')\n",
    "plt.plot(graph_points[0:grange],tf_learn_Vacc[0:grange],color='orange', label='TF')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "plt.ylim([0.4,1.0])\n",
    "\n",
    "plt.title(\"OM vs TF {} {} {} trained total\".format(dataset_name,MODEL,graph_points[grange]))\n",
    "#plt.xticks(np.arange(0,graph_points[num_stored]+1,int(graph_points[num_stored]/10)))  # Set label locations.\n",
    "plt.xticks(np.arange(0,graph_points[grange]+1,int(graph_points[grange]/10)))  # Set label locations.\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Batch Number')\n",
    "plt.show()\n",
    "print('Labels  ',np.array(labels_presented)[0:grange].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
