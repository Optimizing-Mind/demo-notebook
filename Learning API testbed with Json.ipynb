{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tsvi\\AppData\\Local\\Programs\\Python\\Python36\\lib\\site-packages\\OpenSSL\\_util.py:6: CryptographyDeprecationWarning: Python 3.6 is no longer supported by the Python core team. Therefore, support for it is deprecated in cryptography and will be removed in a future release.\n",
      "  from cryptography.hazmat.bindings.openssl.binding import Binding\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Send_to_API(base_name,dataset_name, top_layer_inputs, labels):\n",
    "        try:\n",
    "            X_j = top_layer_inputs.tolist() # inputs\n",
    "            labels_j = labels.tolist() #labels\n",
    "        \n",
    "        username = os.getenv('JUPYTERHUB_USER')\n",
    "        payload = {'username' : username, 'base': base_name, 'data' : dataset_name, 'X': X_j, 'y': labels_j}\n",
    "        headers = { 'content-type': 'application/json', 'accept': 'application/json' }\n",
    "        \n",
    "        r = requests.post(url = 'https://om-learn-api.azurewebsites.net/api/train', data = json.dumps(payload), headers=headers)\n",
    "        result = json.loads(r.text)\n",
    "        \n",
    "        return(result)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return(e.msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now=datetime.now()\n",
    "date_time=now.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'05-09-2023'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =1 #32   #1\n",
    "IMG_SIZE = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I left this because it sets up the directories automatically  \n",
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tsvi\\.keras\\datasets\\cats_and_dogs_filtered\n"
     ]
    }
   ],
   "source": [
    "# Different datasets to benchmark\n",
    "#dataset_name='Animals_filtered'  # 50 animal dataset\n",
    "dataset_name='cats_and_dogs_filtered'  #originalTF notebook dataset \"full\"\n",
    "#dataset_name='fowl_data' # from https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-pytorch\n",
    "\n",
    "PATH2 = os.path.join(os.path.dirname(path_to_zip),dataset_name )    \n",
    "# also put in original\n",
    "print(PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tsvi\\.keras\\datasets\\cats_and_dogs_filtered \n",
      " C:\\Users\\Tsvi\\.keras\\datasets\\cats_and_dogs_filtered\\train \n",
      " C:\\Users\\Tsvi\\.keras\\datasets\\cats_and_dogs_filtered\\validation\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(PATH2, 'train')\n",
    "validation_dir = os.path.join(PATH2, 'validation')\n",
    "\n",
    "#print(third_animal_dataset_path,'\\n',third_animal_train_dir,'\\n',third_animal_validation_dir)\n",
    "print(PATH2,'\\n',train_dir,'\\n',validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                                         shuffle=True,\n",
    "                                                                         batch_size=BATCH_SIZE,\n",
    "                                                                         image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
    "                                                                              shuffle=True,\n",
    "                                                                              batch_size=BATCH_SIZE,\n",
    "                                                                              image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class names: ['cats', 'dogs']\n"
     ]
    }
   ],
   "source": [
    "#print(\"original dataset class names:\", train_dataset.class_names)\n",
    "print(\"Dataset class names:\", train_dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes and outputs: 2\n"
     ]
    }
   ],
   "source": [
    "class_names = train_dataset.class_names\n",
    "num_outputs=len(class_names)\n",
    "print('number of classes and outputs:',num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation batches: 800\n",
      "Number of test batches: 200\n"
     ]
    }
   ],
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 2000\n"
     ]
    }
   ],
   "source": [
    "print('Number of training batches: %d' % tf.data.experimental.cardinality(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model 10 epochs\n",
    "initial_epochs = 1 #10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "#base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = 'MobileNetV2'\n",
    "#MODEL = 'ResNet50'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL=='MobileNetV2':\n",
    "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "elif MODEL=='ResNet50':\n",
    "    preprocess_input = tf.keras.applications.resnet50.preprocess_input\n",
    "    base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "else:\n",
    "    error(\"Model Not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2_1.00_160\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "Conv1 (Conv2D)                  (None, 80, 80, 32)   864         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bn_Conv1 (BatchNormalization)   (None, 80, 80, 32)   128         Conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "Conv1_relu (ReLU)               (None, 80, 80, 32)   0           bn_Conv1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise (Depthw (None, 80, 80, 32)   288         Conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_BN (Bat (None, 80, 80, 32)   128         expanded_conv_depthwise[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_depthwise_relu (R (None, 80, 80, 32)   0           expanded_conv_depthwise_BN[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project (Conv2D)  (None, 80, 80, 16)   512         expanded_conv_depthwise_relu[0][0\n",
      "__________________________________________________________________________________________________\n",
      "expanded_conv_project_BN (Batch (None, 80, 80, 16)   64          expanded_conv_project[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand (Conv2D)         (None, 80, 80, 96)   1536        expanded_conv_project_BN[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_BN (BatchNormali (None, 80, 80, 96)   384         block_1_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_1_expand_relu (ReLU)      (None, 80, 80, 96)   0           block_1_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_pad (ZeroPadding2D)     (None, 81, 81, 96)   0           block_1_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise (DepthwiseCon (None, 40, 40, 96)   864         block_1_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_BN (BatchNorm (None, 40, 40, 96)   384         block_1_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_1_depthwise_relu (ReLU)   (None, 40, 40, 96)   0           block_1_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project (Conv2D)        (None, 40, 40, 24)   2304        block_1_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_1_project_BN (BatchNormal (None, 40, 40, 24)   96          block_1_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand (Conv2D)         (None, 40, 40, 144)  3456        block_1_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_BN (BatchNormali (None, 40, 40, 144)  576         block_2_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_2_expand_relu (ReLU)      (None, 40, 40, 144)  0           block_2_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise (DepthwiseCon (None, 40, 40, 144)  1296        block_2_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_BN (BatchNorm (None, 40, 40, 144)  576         block_2_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_2_depthwise_relu (ReLU)   (None, 40, 40, 144)  0           block_2_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project (Conv2D)        (None, 40, 40, 24)   3456        block_2_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_2_project_BN (BatchNormal (None, 40, 40, 24)   96          block_2_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_2_add (Add)               (None, 40, 40, 24)   0           block_1_project_BN[0][0]         \n",
      "                                                                 block_2_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand (Conv2D)         (None, 40, 40, 144)  3456        block_2_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_BN (BatchNormali (None, 40, 40, 144)  576         block_3_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_3_expand_relu (ReLU)      (None, 40, 40, 144)  0           block_3_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_pad (ZeroPadding2D)     (None, 41, 41, 144)  0           block_3_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise (DepthwiseCon (None, 20, 20, 144)  1296        block_3_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_BN (BatchNorm (None, 20, 20, 144)  576         block_3_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_3_depthwise_relu (ReLU)   (None, 20, 20, 144)  0           block_3_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project (Conv2D)        (None, 20, 20, 32)   4608        block_3_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_3_project_BN (BatchNormal (None, 20, 20, 32)   128         block_3_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand (Conv2D)         (None, 20, 20, 192)  6144        block_3_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_4_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_4_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_4_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise (DepthwiseCon (None, 20, 20, 192)  1728        block_4_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_BN (BatchNorm (None, 20, 20, 192)  768         block_4_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_4_depthwise_relu (ReLU)   (None, 20, 20, 192)  0           block_4_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project (Conv2D)        (None, 20, 20, 32)   6144        block_4_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_4_project_BN (BatchNormal (None, 20, 20, 32)   128         block_4_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_4_add (Add)               (None, 20, 20, 32)   0           block_3_project_BN[0][0]         \n",
      "                                                                 block_4_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand (Conv2D)         (None, 20, 20, 192)  6144        block_4_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_5_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_5_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_5_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise (DepthwiseCon (None, 20, 20, 192)  1728        block_5_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_BN (BatchNorm (None, 20, 20, 192)  768         block_5_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_5_depthwise_relu (ReLU)   (None, 20, 20, 192)  0           block_5_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project (Conv2D)        (None, 20, 20, 32)   6144        block_5_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_5_project_BN (BatchNormal (None, 20, 20, 32)   128         block_5_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_5_add (Add)               (None, 20, 20, 32)   0           block_4_add[0][0]                \n",
      "                                                                 block_5_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand (Conv2D)         (None, 20, 20, 192)  6144        block_5_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_BN (BatchNormali (None, 20, 20, 192)  768         block_6_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_6_expand_relu (ReLU)      (None, 20, 20, 192)  0           block_6_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_pad (ZeroPadding2D)     (None, 21, 21, 192)  0           block_6_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise (DepthwiseCon (None, 10, 10, 192)  1728        block_6_pad[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_BN (BatchNorm (None, 10, 10, 192)  768         block_6_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_6_depthwise_relu (ReLU)   (None, 10, 10, 192)  0           block_6_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project (Conv2D)        (None, 10, 10, 64)   12288       block_6_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_6_project_BN (BatchNormal (None, 10, 10, 64)   256         block_6_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand (Conv2D)         (None, 10, 10, 384)  24576       block_6_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_7_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_7_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_7_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_7_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_7_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_7_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_7_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project (Conv2D)        (None, 10, 10, 64)   24576       block_7_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_7_project_BN (BatchNormal (None, 10, 10, 64)   256         block_7_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_7_add (Add)               (None, 10, 10, 64)   0           block_6_project_BN[0][0]         \n",
      "                                                                 block_7_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand (Conv2D)         (None, 10, 10, 384)  24576       block_7_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_8_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_8_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_8_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_8_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_8_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_8_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_8_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project (Conv2D)        (None, 10, 10, 64)   24576       block_8_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_8_project_BN (BatchNormal (None, 10, 10, 64)   256         block_8_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_8_add (Add)               (None, 10, 10, 64)   0           block_7_add[0][0]                \n",
      "                                                                 block_8_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand (Conv2D)         (None, 10, 10, 384)  24576       block_8_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_BN (BatchNormali (None, 10, 10, 384)  1536        block_9_expand[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "block_9_expand_relu (ReLU)      (None, 10, 10, 384)  0           block_9_expand_BN[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise (DepthwiseCon (None, 10, 10, 384)  3456        block_9_expand_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_BN (BatchNorm (None, 10, 10, 384)  1536        block_9_depthwise[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "block_9_depthwise_relu (ReLU)   (None, 10, 10, 384)  0           block_9_depthwise_BN[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project (Conv2D)        (None, 10, 10, 64)   24576       block_9_depthwise_relu[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "block_9_project_BN (BatchNormal (None, 10, 10, 64)   256         block_9_project[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_9_add (Add)               (None, 10, 10, 64)   0           block_8_add[0][0]                \n",
      "                                                                 block_9_project_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand (Conv2D)        (None, 10, 10, 384)  24576       block_9_add[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_BN (BatchNormal (None, 10, 10, 384)  1536        block_10_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_10_expand_relu (ReLU)     (None, 10, 10, 384)  0           block_10_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise (DepthwiseCo (None, 10, 10, 384)  3456        block_10_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_BN (BatchNor (None, 10, 10, 384)  1536        block_10_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_10_depthwise_relu (ReLU)  (None, 10, 10, 384)  0           block_10_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project (Conv2D)       (None, 10, 10, 96)   36864       block_10_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_10_project_BN (BatchNorma (None, 10, 10, 96)   384         block_10_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand (Conv2D)        (None, 10, 10, 576)  55296       block_10_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_11_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_11_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_11_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise (DepthwiseCo (None, 10, 10, 576)  5184        block_11_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_BN (BatchNor (None, 10, 10, 576)  2304        block_11_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_11_depthwise_relu (ReLU)  (None, 10, 10, 576)  0           block_11_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project (Conv2D)       (None, 10, 10, 96)   55296       block_11_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_11_project_BN (BatchNorma (None, 10, 10, 96)   384         block_11_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_11_add (Add)              (None, 10, 10, 96)   0           block_10_project_BN[0][0]        \n",
      "                                                                 block_11_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand (Conv2D)        (None, 10, 10, 576)  55296       block_11_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_12_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_12_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_12_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise (DepthwiseCo (None, 10, 10, 576)  5184        block_12_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_BN (BatchNor (None, 10, 10, 576)  2304        block_12_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_12_depthwise_relu (ReLU)  (None, 10, 10, 576)  0           block_12_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project (Conv2D)       (None, 10, 10, 96)   55296       block_12_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_12_project_BN (BatchNorma (None, 10, 10, 96)   384         block_12_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_12_add (Add)              (None, 10, 10, 96)   0           block_11_add[0][0]               \n",
      "                                                                 block_12_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand (Conv2D)        (None, 10, 10, 576)  55296       block_12_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_BN (BatchNormal (None, 10, 10, 576)  2304        block_13_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_13_expand_relu (ReLU)     (None, 10, 10, 576)  0           block_13_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_pad (ZeroPadding2D)    (None, 11, 11, 576)  0           block_13_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise (DepthwiseCo (None, 5, 5, 576)    5184        block_13_pad[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_BN (BatchNor (None, 5, 5, 576)    2304        block_13_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_13_depthwise_relu (ReLU)  (None, 5, 5, 576)    0           block_13_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project (Conv2D)       (None, 5, 5, 160)    92160       block_13_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_13_project_BN (BatchNorma (None, 5, 5, 160)    640         block_13_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand (Conv2D)        (None, 5, 5, 960)    153600      block_13_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_14_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_14_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_14_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_14_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_14_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_14_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_14_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project (Conv2D)       (None, 5, 5, 160)    153600      block_14_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_14_project_BN (BatchNorma (None, 5, 5, 160)    640         block_14_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_14_add (Add)              (None, 5, 5, 160)    0           block_13_project_BN[0][0]        \n",
      "                                                                 block_14_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand (Conv2D)        (None, 5, 5, 960)    153600      block_14_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_15_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_15_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_15_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_15_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_15_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_15_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_15_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project (Conv2D)       (None, 5, 5, 160)    153600      block_15_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_15_project_BN (BatchNorma (None, 5, 5, 160)    640         block_15_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block_15_add (Add)              (None, 5, 5, 160)    0           block_14_add[0][0]               \n",
      "                                                                 block_15_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand (Conv2D)        (None, 5, 5, 960)    153600      block_15_add[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_BN (BatchNormal (None, 5, 5, 960)    3840        block_16_expand[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block_16_expand_relu (ReLU)     (None, 5, 5, 960)    0           block_16_expand_BN[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise (DepthwiseCo (None, 5, 5, 960)    8640        block_16_expand_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_BN (BatchNor (None, 5, 5, 960)    3840        block_16_depthwise[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block_16_depthwise_relu (ReLU)  (None, 5, 5, 960)    0           block_16_depthwise_BN[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project (Conv2D)       (None, 5, 5, 320)    307200      block_16_depthwise_relu[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "block_16_project_BN (BatchNorma (None, 5, 5, 320)    1280        block_16_project[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1 (Conv2D)                 (None, 5, 5, 1280)   409600      block_16_project_BN[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "Conv_1_bn (BatchNormalization)  (None, 5, 5, 1280)   5120        Conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "out_relu (ReLU)                 (None, 5, 5, 1280)   0           Conv_1_bn[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,257,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original adds 2d global_average_pooling2d layer to make the base layer ready for classification.  \n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I set up traditional learning and top layer, but I change from binary to multiclass so I must add softmax to top\n",
    "final_output_layer=tf.keras.layers.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the multiclass equivalent to original transfer learning network  \n",
    "prediction_layer4 = tf.keras.layers.Dense(num_outputs)  # change1 from original: I make num_outputs outputs\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = prediction_layer4(x)    # adds the 49 nodes\n",
    "outputs = final_output_layer(x)  # change2: adds softmax\n",
    "orig_TF_paradigm_model = tf.keras.Model(inputs, outputs)   # this is now the traditional leanring network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 3:SparseCategoricalCrossentropy from BinaryCrossentropy because it is multiclass\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "orig_TF_paradigm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True,) # Now I compile it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 160, 160, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.math.truediv (TFOpLambda) (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.math.subtract (TFOpLambda (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_160 (Functi (None, 5, 5, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 2562      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "orig_TF_paradigm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I create the version of the base model that will feed inputs to OM layer and I call it model_with_av_layer\n",
    "# it does not have anything after the global_average_layer\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "outputs = global_average_layer(x)\n",
    "#outputs = prediction_layer(x)   # It is basically the same as the original notebook but no prediction layer\n",
    "model_with_av_layer = tf.keras.Model(inputs, outputs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 160, 160, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.math.truediv_1 (TFOpLambd (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.math.subtract_1 (TFOpLamb (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_160 (Functi (None, 5, 5, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "=================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_with_av_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OM model that will take inputs from the outputs of model_with_av_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OM_Model_API import OM_CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: OM core 0.3 5/9/2023\n"
     ]
    }
   ],
   "source": [
    "num_inputs=model_with_av_layer.output.shape[1]  #1280\n",
    "\n",
    "OMmodel = OM_CORE(num_inputs, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in case it is run or tested un-initiated  \n",
    "OMmodel.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an equivalent TF model that will recieve the OM weights\n",
    "prediction_layer2 = tf.keras.layers.Dense(num_outputs)  # top nodes\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = prediction_layer2(x)  # nodes whose weights I will be changed by om\n",
    "outputs = final_output_layer(x)  #softmax\n",
    "tf_model_tobe_trained_by_OM = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it needs compile if I do validation but I never really do any learning so most of this doesnt really matter\n",
    "base_learning_rate = 0.0001\n",
    "tf_model_tobe_trained_by_OM.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True,) # Efi: needs to be added to prevent execution optimization (not data optimiziation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 160, 160, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.math.truediv_2 (TFOpLambd (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.math.subtract_2 (TFOpLamb (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "mobilenetv2_1.00_160 (Functi (None, 5, 5, 1280)        2257984   \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1280)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 2562      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf_model_tobe_trained_by_OM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so I dont validate at every point for graphing (otherwise things take forever)\n",
    "validate_points=np.concatenate([np.arange(30),np.arange(30,40,2),np.arange(40,60,3),np.arange(60,100,5),np.arange(100,300,7),np.arange(300,700,10),np.arange(700,1500,15),np.arange(1500,3000,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start re-run here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "# find the number of entries in thrid_animal_validation set: there doesnt seem to be an adequate internal function to do this!\n",
    "# Im sure there is a better way but whatever\n",
    "num_data_in_validation_dataset=0\n",
    "for data, label in validation_dataset: #.as_numpy_iterator():\n",
    "    num_data_in_validation_dataset=num_data_in_validation_dataset+len(data)\n",
    "print(num_data_in_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup validation manually so I dont have to go through base for every validation (saves time because validation is the same)\n",
    "i=0\n",
    "validation_labels=np.ones((1,num_data_in_validation_dataset))*50  #crazy initial values for debug\n",
    "net_out_validation_for_OM=np.ones((num_data_in_validation_dataset,num_inputs))*50  \n",
    "\n",
    "for data, label in validation_dataset: #.as_numpy_iterator():\n",
    "    #print(\"Batch\",i,\"out of\",num_data_in_validation_dataset,\"size:\",len(data),len(label))\n",
    "    len_batch=len(label)\n",
    "    #print(label)\n",
    "    validation_labels[0,(BATCH_SIZE*i):(BATCH_SIZE*(i)+len_batch)]=label  # set up validation truth table\n",
    "    temp=model_with_av_layer.predict(data)  #run bottom transfer layers pre-rfn\n",
    "    net_out_validation_for_OM[BATCH_SIZE*i:(BATCH_SIZE*i+len_batch),:]=temp[0:len_batch,:]\n",
    "    i=i+1\n",
    "validation_labels=validation_labels.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 1 0 0 1 1 1 1 1 1 0 1\n",
      " 1 1 1 0 0 1 1 0 1 0 0 0 0 1 0 1 0 1 0 1 1 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0\n",
      " 1 0 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1 1 0\n",
      " 1 1 0 1 0 0 0 0 0 0 1 0 0 1 0 1 1 1 0 0 0 0 1 0 0 0 1 1 0 0 0 0 1 0 1 1 0\n",
      " 1 1 1 1 0 1 1 1 1 1 0 1 0 1 1 0 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 0 1 0 1\n",
      " 1 0 1 1 0 0 0 0 0 1 1 0 1 1 1 0 1 1 1 1 0 0 0 1 1 0 0 0 1 0 1 1 1 1 0 1 0\n",
      " 1 0 1 1 1 0 1 0 0 1 0 1 1 1 1 1 1 0 0 0 0 0 1 1 1 1 0 0 1 1 1 1 0 1 1 0 1\n",
      " 0 0 1 0 1 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 1\n",
      " 1 1 0 0 1 0 1 1 0 0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 1 1 0 1 1\n",
      " 0 1 1 0 1 1 1 1 0 0 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 1\n",
      " 0 0 0 0 1 1 1 0 1 1 1 1 1 0 0 1 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 1 0 0\n",
      " 0 0 0 1 0 1 1 1 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 1 0 1 1 0 1 1 0 1 0 1 0 0\n",
      " 0 1 1 0 1 0 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 0 0 1 0 1 0 1 1\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 1 1 0 0 0 1 1 1 1 0 1 1 1 1 0 0 0 1 0 0\n",
      " 1 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 1 1 0 1 1 0 1 1 0 1 1 0\n",
      " 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 0 0 1 1 0 0 1 1 1 0 1 0 1 1 0 0 0\n",
      " 1 1 0 0 0 1 0 1 0 0 0 1 0 1 1 0 0 1 0 1 0 0 1 1 1 0 1 0 1 1 0 0 1 0 1 1 1\n",
      " 0 1 1 1 0 1 0 0 1 1 1 0 0 1 0 0 0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 1 0 1 0 0\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 1 1 1 0 1 1 0 1 0 1 0 1\n",
      " 0 1 0 0 1 0 1 1 0 1 1 0 1 0 0 1 1 0 1 1 1 1 1 1 1 0 0 1 1 0 1 0 1 0 0 1 0\n",
      " 0 1 0 0 0 1 1 1 0 0 0 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 0 1 0 1 0 0 0 1 1 1\n",
      " 1 1 0 1 0 0 1 0 1 0 0 1 0 0 1 1 1 0 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(validation_labels[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data of train batches in a single epoch 2000 batch size = 1 epochs= 1\n"
     ]
    }
   ],
   "source": [
    "# for batches\n",
    "num_batches_in_train_dataset=len(train_dataset)              # number of batches in train_dataset\n",
    "print(\"num of data of train batches in a single epoch\",num_batches_in_train_dataset,\"batch size =\",BATCH_SIZE,\"epochs=\",initial_epochs)\n",
    "OM_learn_Vacc=[] #0.5*np.ones(num_batches_in_train_dataset*initial_epochs)    #initializing as .5 just in case zeros gets through somehow  #np.zeros(num_batches_in_train_dataset)\n",
    "tf_learn_Vacc=[] #0.5*np.ones(num_batches_in_train_dataset*initial_epochs)    #np.zeros(num_batches_in_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_presented=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_learn_Vacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store baselines before learn\n",
    "store_baselines_before_learning=False\n",
    "if store_baselines_before_learning:\n",
    "    loss_, accuracy_ = orig_TF_paradigm_model.evaluate(validation_dataset)\n",
    "    print('validation of TF before train on new',accuracy_)\n",
    "    tf_learn_Vacc.append(accuracy_) #store data\n",
    "\n",
    "    OMmodel.test_data_score(net_out_validation_for_OM,validation_labels[0,:])  #validate for OM\n",
    "    print('validation of OM on NEW data before train on new',OMmodel.last_score_accuracy/100)\n",
    "    OM_learn_Vacc.append(OMmodel.last_score_accuracy/100) \n",
    "    \n",
    "    # I want to add one more zero point and increase the index of everything by 1 to test before learning\n",
    "    graph_points=list(validate_points.copy()+1)\n",
    "    graph_points.insert(0,0)\n",
    "else:\n",
    "    graph_points=list(validate_points.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [1] , previously [0. 0.] of each.\n",
      "800/800 [==============================] - 51s 63ms/step - loss: 0.7711 - accuracy: 0.41251s - loss: 0.7710 - accuracy:  - ETA: \n",
      " (Number correct, number incorrect, number of tests): ( 391 409 800 ) %Correct:  48.875 %\n",
      "False Positives by label: [  0. 409.]\n",
      "False Negatives by label: [409.   0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 0, Epoch 0, 1 entries accuracy of OM core 0.4888 and TF 0.4125 \n",
      " Learned [1] of label(s) [1] , previously [0. 1.] of each.\n",
      "800/800 [==============================] - 54s 67ms/step - loss: 0.7724 - accuracy: 0.4175\n",
      " (Number correct, number incorrect, number of tests): ( 391 409 800 ) %Correct:  48.875 %\n",
      "False Positives by label: [  0. 409.]\n",
      "False Negatives by label: [409.   0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 1, Epoch 0, 1 entries accuracy of OM core 0.4888 and TF 0.4175 \n",
      " Learned [1] of label(s) [1] , previously [0. 2.] of each.\n",
      "800/800 [==============================] - 56s 69ms/step - loss: 0.7730 - accuracy: 0.42120s - loss: 0.7731 - accuracy: 0.\n",
      " (Number correct, number incorrect, number of tests): ( 391 409 800 ) %Correct:  48.875 %\n",
      "False Positives by label: [  0. 409.]\n",
      "False Negatives by label: [409.   0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 2, Epoch 0, 1 entries accuracy of OM core 0.4888 and TF 0.4212 \n",
      " Learned [1] of label(s) [0] , previously [0. 3.] of each.\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.7723 - accuracy: 0.4212\n",
      " (Number correct, number incorrect, number of tests): ( 656 144 800 ) %Correct:  82.0 %\n",
      "False Positives by label: [  0. 144.]\n",
      "False Negatives by label: [144.   0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 3, Epoch 0, 1 entries accuracy of OM core 0.82 and TF 0.4212 \n",
      " Learned [1] of label(s) [0] , previously [1. 3.] of each.\n",
      "800/800 [==============================] - 61s 76ms/step - loss: 0.7709 - accuracy: 0.4250\n",
      " (Number correct, number incorrect, number of tests): ( 752 48 800 ) %Correct:  94.0 %\n",
      "False Positives by label: [ 1. 47.]\n",
      "False Negatives by label: [47.  1.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 4, Epoch 0, 1 entries accuracy of OM core 0.94 and TF 0.425 \n",
      " Learned [1] of label(s) [1] , previously [2. 3.] of each.\n",
      "800/800 [==============================] - 56s 70ms/step - loss: 0.7697 - accuracy: 0.4275\n",
      " (Number correct, number incorrect, number of tests): ( 728 72 800 ) %Correct:  91.0 %\n",
      "False Positives by label: [ 0. 72.]\n",
      "False Negatives by label: [72.  0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 5, Epoch 0, 1 entries accuracy of OM core 0.91 and TF 0.4275 \n",
      " Learned [1] of label(s) [1] , previously [2. 4.] of each.\n",
      "800/800 [==============================] - 58s 72ms/step - loss: 0.7706 - accuracy: 0.4238\n",
      " (Number correct, number incorrect, number of tests): ( 719 81 800 ) %Correct:  89.875 %\n",
      "False Positives by label: [ 0. 81.]\n",
      "False Negatives by label: [81.  0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 6, Epoch 0, 1 entries accuracy of OM core 0.8988 and TF 0.4238 \n",
      " Learned [1] of label(s) [1] , previously [2. 5.] of each.\n",
      "800/800 [==============================] - 61s 76ms/step - loss: 0.7703 - accuracy: 0.4300\n",
      " (Number correct, number incorrect, number of tests): ( 704 96 800 ) %Correct:  88.0 %\n",
      "False Positives by label: [ 0. 96.]\n",
      "False Negatives by label: [96.  0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 7, Epoch 0, 1 entries accuracy of OM core 0.88 and TF 0.43 \n",
      " Learned [1] of label(s) [0] , previously [2. 6.] of each.\n",
      "800/800 [==============================] - 73s 91ms/step - loss: 0.7682 - accuracy: 0.4325\n",
      " (Number correct, number incorrect, number of tests): ( 747 53 800 ) %Correct:  93.375 %\n",
      "False Positives by label: [ 7. 46.]\n",
      "False Negatives by label: [46.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 8, Epoch 0, 1 entries accuracy of OM core 0.9338 and TF 0.4325 \n",
      " Learned [1] of label(s) [1] , previously [3. 6.] of each.\n",
      "800/800 [==============================] - 69s 87ms/step - loss: 0.7685 - accuracy: 0.4350\n",
      " (Number correct, number incorrect, number of tests): ( 748 52 800 ) %Correct:  93.5 %\n",
      "False Positives by label: [ 4. 48.]\n",
      "False Negatives by label: [48.  4.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 9, Epoch 0, 1 entries accuracy of OM core 0.935 and TF 0.435 \n",
      " Learned [1] of label(s) [1] , previously [3. 7.] of each.\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.7670 - accuracy: 0.43870s - loss: 0.765\n",
      " (Number correct, number incorrect, number of tests): ( 741 59 800 ) %Correct:  92.625 %\n",
      "False Positives by label: [ 2. 57.]\n",
      "False Negatives by label: [57.  2.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 10, Epoch 0, 1 entries accuracy of OM core 0.9262 and TF 0.4387 \n",
      " Learned [1] of label(s) [0] , previously [3. 8.] of each.\n",
      "800/800 [==============================] - 54s 67ms/step - loss: 0.7664 - accuracy: 0.4387\n",
      " (Number correct, number incorrect, number of tests): ( 757 43 800 ) %Correct:  94.625 %\n",
      "False Positives by label: [ 3. 40.]\n",
      "False Negatives by label: [40.  3.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 11, Epoch 0, 1 entries accuracy of OM core 0.9462 and TF 0.4387 \n",
      " Learned [1] of label(s) [0] , previously [4. 8.] of each.\n",
      "800/800 [==============================] - 52s 65ms/step - loss: 0.7656 - accuracy: 0.44500s - loss: 0.764\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 6. 22.]\n",
      "False Negatives by label: [22.  6.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 12, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.445 \n",
      " Learned [1] of label(s) [1] , previously [5. 8.] of each.\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.7645 - accuracy: 0.4462\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 5. 22.]\n",
      "False Negatives by label: [22.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 13, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.4462 \n",
      " Learned [1] of label(s) [0] , previously [5. 9.] of each.\n",
      "800/800 [==============================] - 56s 70ms/step - loss: 0.7632 - accuracy: 0.4487\n",
      " (Number correct, number incorrect, number of tests): ( 775 25 800 ) %Correct:  96.875 %\n",
      "False Positives by label: [ 6. 19.]\n",
      "False Negatives by label: [19.  6.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 14, Epoch 0, 1 entries accuracy of OM core 0.9688 and TF 0.4487 \n",
      " Learned [1] of label(s) [1] , previously [6. 9.] of each.\n",
      "800/800 [==============================] - 57s 72ms/step - loss: 0.7629 - accuracy: 0.4462\n",
      " (Number correct, number incorrect, number of tests): ( 779 21 800 ) %Correct:  97.375 %\n",
      "False Positives by label: [ 6. 15.]\n",
      "False Negatives by label: [15.  6.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 15, Epoch 0, 1 entries accuracy of OM core 0.9738 and TF 0.4462 \n",
      " Learned [1] of label(s) [0] , previously [ 6. 10.] of each.\n",
      "800/800 [==============================] - 54s 67ms/step - loss: 0.7613 - accuracy: 0.4500\n",
      " (Number correct, number incorrect, number of tests): ( 776 24 800 ) %Correct:  97.0 %\n",
      "False Positives by label: [ 7. 17.]\n",
      "False Negatives by label: [17.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 16, Epoch 0, 1 entries accuracy of OM core 0.97 and TF 0.45 \n",
      " Learned [1] of label(s) [1] , previously [ 7. 10.] of each.\n",
      "800/800 [==============================] - 55s 68ms/step - loss: 0.7585 - accuracy: 0.45501s - loss: 0.7 - ETA: \n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 3. 25.]\n",
      "False Negatives by label: [25.  3.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 17, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.455 \n",
      " Learned [1] of label(s) [0] , previously [ 7. 11.] of each.\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.7586 - accuracy: 0.4525\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 4. 23.]\n",
      "False Negatives by label: [23.  4.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 18, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.4525 \n",
      " Learned [1] of label(s) [0] , previously [ 8. 11.] of each.\n",
      "800/800 [==============================] - 57s 71ms/step - loss: 0.7567 - accuracy: 0.4500\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 3. 25.]\n",
      "False Negatives by label: [25.  3.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 19, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.45 \n",
      " Learned [1] of label(s) [1] , previously [ 9. 11.] of each.\n",
      "800/800 [==============================] - 60s 75ms/step - loss: 0.7550 - accuracy: 0.44870s - loss: 0.7547 - accu\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 5. 21.]\n",
      "False Negatives by label: [21.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 20, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.4487 \n",
      " Learned [1] of label(s) [0] , previously [ 9. 12.] of each.\n",
      "800/800 [==============================] - 47s 59ms/step - loss: 0.7527 - accuracy: 0.4550\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 5. 23.]\n",
      "False Negatives by label: [23.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 21, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.455 \n",
      " Learned [1] of label(s) [1] , previously [10. 12.] of each.\n",
      "800/800 [==============================] - 47s 59ms/step - loss: 0.7514 - accuracy: 0.4538\n",
      " (Number correct, number incorrect, number of tests): ( 771 29 800 ) %Correct:  96.375 %\n",
      "False Positives by label: [ 4. 25.]\n",
      "False Negatives by label: [25.  4.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 22, Epoch 0, 1 entries accuracy of OM core 0.9638 and TF 0.4538 \n",
      " Learned [1] of label(s) [1] , previously [10. 13.] of each.\n",
      "800/800 [==============================] - 47s 59ms/step - loss: 0.7498 - accuracy: 0.4550\n",
      " (Number correct, number incorrect, number of tests): ( 770 30 800 ) %Correct:  96.25 %\n",
      "False Positives by label: [ 3. 27.]\n",
      "False Negatives by label: [27.  3.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 23, Epoch 0, 1 entries accuracy of OM core 0.9625 and TF 0.455 \n",
      " Learned [1] of label(s) [0] , previously [10. 14.] of each.\n",
      "800/800 [==============================] - 47s 59ms/step - loss: 0.7489 - accuracy: 0.4550\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 3. 24.]\n",
      "False Negatives by label: [24.  3.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 24, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.455 \n",
      " Learned [1] of label(s) [0] , previously [11. 14.] of each.\n",
      "800/800 [==============================] - 47s 58ms/step - loss: 0.7460 - accuracy: 0.4512\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 5. 23.]\n",
      "False Negatives by label: [23.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 25, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.4512 \n",
      " Learned [1] of label(s) [1] , previously [12. 14.] of each.\n",
      "800/800 [==============================] - 47s 58ms/step - loss: 0.7448 - accuracy: 0.4550\n",
      " (Number correct, number incorrect, number of tests): ( 771 29 800 ) %Correct:  96.375 %\n",
      "False Positives by label: [ 5. 24.]\n",
      "False Negatives by label: [24.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 26, Epoch 0, 1 entries accuracy of OM core 0.9638 and TF 0.455 \n",
      " Learned [1] of label(s) [1] , previously [12. 15.] of each.\n",
      "800/800 [==============================] - 47s 58ms/step - loss: 0.7426 - accuracy: 0.4600\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 5. 23.]\n",
      "False Negatives by label: [23.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 27, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.46 \n",
      " Learned [1] of label(s) [0] , previously [12. 16.] of each.\n",
      "800/800 [==============================] - 47s 59ms/step - loss: 0.7422 - accuracy: 0.4588\n",
      " (Number correct, number incorrect, number of tests): ( 771 29 800 ) %Correct:  96.375 %\n",
      "False Positives by label: [ 5. 24.]\n",
      "False Negatives by label: [24.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 28, Epoch 0, 1 entries accuracy of OM core 0.9638 and TF 0.4588 \n",
      " Learned [1] of label(s) [1] , previously [13. 16.] of each.\n",
      "800/800 [==============================] - 47s 59ms/step - loss: 0.7418 - accuracy: 0.4625\n",
      " (Number correct, number incorrect, number of tests): ( 769 31 800 ) %Correct:  96.125 %\n",
      "False Positives by label: [ 5. 26.]\n",
      "False Negatives by label: [26.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 29, Epoch 0, 1 entries accuracy of OM core 0.9612 and TF 0.4625 \n",
      " Learned [1] of label(s) [0] , previously [13. 17.] of each.\n",
      "800/800 [==============================] - 47s 58ms/step - loss: 0.7390 - accuracy: 0.4650\n",
      " (Number correct, number incorrect, number of tests): ( 770 30 800 ) %Correct:  96.25 %\n",
      "False Positives by label: [ 8. 22.]\n",
      "False Negatives by label: [22.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 30, Epoch 0, 1 entries accuracy of OM core 0.9625 and TF 0.465 \n",
      " Learned [1] of label(s) [1] , previously [14. 17.] of each.\n",
      " Learned [1] of label(s) [1] , previously [14. 18.] of each.\n",
      "800/800 [==============================] - 47s 58ms/step - loss: 0.7370 - accuracy: 0.4675\n",
      " (Number correct, number incorrect, number of tests): ( 769 31 800 ) %Correct:  96.125 %\n",
      "False Positives by label: [ 8. 23.]\n",
      "False Negatives by label: [23.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 32, Epoch 0, 1 entries accuracy of OM core 0.9612 and TF 0.4675 \n",
      " Learned [1] of label(s) [1] , previously [14. 19.] of each.\n",
      " Learned [1] of label(s) [0] , previously [14. 20.] of each.\n",
      "800/800 [==============================] - 47s 59ms/step - loss: 0.7348 - accuracy: 0.4750\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 8. 18.]\n",
      "False Negatives by label: [18.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 34, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.475 \n",
      " Learned [1] of label(s) [1] , previously [15. 20.] of each.\n",
      " Learned [1] of label(s) [0] , previously [15. 21.] of each.\n",
      "800/800 [==============================] - 47s 58ms/step - loss: 0.7332 - accuracy: 0.4762\n",
      " (Number correct, number incorrect, number of tests): ( 770 30 800 ) %Correct:  96.25 %\n",
      "False Positives by label: [ 9. 21.]\n",
      "False Negatives by label: [21.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 36, Epoch 0, 1 entries accuracy of OM core 0.9625 and TF 0.4762 \n",
      " Learned [1] of label(s) [0] , previously [16. 21.] of each.\n",
      " Learned [1] of label(s) [0] , previously [17. 21.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.7315 - accuracy: 0.4787\n",
      " (Number correct, number incorrect, number of tests): ( 771 29 800 ) %Correct:  96.375 %\n",
      "False Positives by label: [ 9. 20.]\n",
      "False Negatives by label: [20.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 38, Epoch 0, 1 entries accuracy of OM core 0.9638 and TF 0.4787 \n",
      " Learned [1] of label(s) [1] , previously [18. 21.] of each.\n",
      " Learned [1] of label(s) [0] , previously [18. 22.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.7279 - accuracy: 0.4812\n",
      " (Number correct, number incorrect, number of tests): ( 769 31 800 ) %Correct:  96.125 %\n",
      "False Positives by label: [ 9. 22.]\n",
      "False Negatives by label: [22.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 40, Epoch 0, 1 entries accuracy of OM core 0.9612 and TF 0.4812 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [0] , previously [19. 22.] of each.\n",
      " Learned [1] of label(s) [0] , previously [20. 22.] of each.\n",
      " Learned [1] of label(s) [1] , previously [21. 22.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.7248 - accuracy: 0.4825\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 9. 18.]\n",
      "False Negatives by label: [18.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 43, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.4825 \n",
      " Learned [1] of label(s) [0] , previously [21. 23.] of each.\n",
      " Learned [1] of label(s) [0] , previously [22. 23.] of each.\n",
      " Learned [1] of label(s) [0] , previously [23. 23.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.7209 - accuracy: 0.4837\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [10. 18.]\n",
      "False Negatives by label: [18. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 46, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.4837 \n",
      " Learned [1] of label(s) [0] , previously [24. 23.] of each.\n",
      " Learned [1] of label(s) [1] , previously [25. 23.] of each.\n",
      " Learned [1] of label(s) [1] , previously [25. 24.] of each.\n",
      "800/800 [==============================] - 45s 56ms/step - loss: 0.7174 - accuracy: 0.4888\n",
      " (Number correct, number incorrect, number of tests): ( 771 29 800 ) %Correct:  96.375 %\n",
      "False Positives by label: [10. 19.]\n",
      "False Negatives by label: [19. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 49, Epoch 0, 1 entries accuracy of OM core 0.9638 and TF 0.4888 \n",
      " Learned [1] of label(s) [0] , previously [25. 25.] of each.\n",
      " Learned [1] of label(s) [1] , previously [26. 25.] of each.\n",
      " Learned [1] of label(s) [0] , previously [26. 26.] of each.\n",
      "800/800 [==============================] - 45s 57ms/step - loss: 0.7141 - accuracy: 0.4938\n",
      " (Number correct, number incorrect, number of tests): ( 771 29 800 ) %Correct:  96.375 %\n",
      "False Positives by label: [12. 17.]\n",
      "False Negatives by label: [17. 12.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 52, Epoch 0, 1 entries accuracy of OM core 0.9638 and TF 0.4938 \n",
      " Learned [1] of label(s) [0] , previously [27. 26.] of each.\n",
      " Learned [1] of label(s) [1] , previously [28. 26.] of each.\n",
      " Learned [1] of label(s) [1] , previously [28. 27.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.7121 - accuracy: 0.4988\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 55, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.4988 \n",
      " Learned [1] of label(s) [0] , previously [28. 28.] of each.\n",
      " Learned [1] of label(s) [0] , previously [29. 28.] of each.\n",
      " Learned [1] of label(s) [1] , previously [30. 28.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.7086 - accuracy: 0.5013\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 7. 20.]\n",
      "False Negatives by label: [20.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 58, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.5013 \n",
      " Learned [1] of label(s) [0] , previously [30. 29.] of each.\n",
      " Learned [1] of label(s) [1] , previously [31. 29.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.7050 - accuracy: 0.5088\n",
      " (Number correct, number incorrect, number of tests): ( 775 25 800 ) %Correct:  96.875 %\n",
      "False Positives by label: [ 7. 18.]\n",
      "False Negatives by label: [18.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 60, Epoch 0, 1 entries accuracy of OM core 0.9688 and TF 0.5088 \n",
      " Learned [1] of label(s) [1] , previously [31. 30.] of each.\n",
      " Learned [1] of label(s) [1] , previously [31. 31.] of each.\n",
      " Learned [1] of label(s) [0] , previously [31. 32.] of each.\n",
      " Learned [1] of label(s) [0] , previously [32. 32.] of each.\n",
      " Learned [1] of label(s) [1] , previously [33. 32.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.7004 - accuracy: 0.5188\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 7. 20.]\n",
      "False Negatives by label: [20.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 65, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.5188 \n",
      " Learned [1] of label(s) [1] , previously [33. 33.] of each.\n",
      " Learned [1] of label(s) [1] , previously [33. 34.] of each.\n",
      " Learned [1] of label(s) [1] , previously [33. 35.] of each.\n",
      " Learned [1] of label(s) [0] , previously [33. 36.] of each.\n",
      " Learned [1] of label(s) [1] , previously [34. 36.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6959 - accuracy: 0.5325\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 70, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.5325 \n",
      " Learned [1] of label(s) [1] , previously [34. 37.] of each.\n",
      " Learned [1] of label(s) [1] , previously [34. 38.] of each.\n",
      " Learned [1] of label(s) [0] , previously [34. 39.] of each.\n",
      " Learned [1] of label(s) [1] , previously [35. 39.] of each.\n",
      " Learned [1] of label(s) [1] , previously [35. 40.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6926 - accuracy: 0.5375\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 7. 21.]\n",
      "False Negatives by label: [21.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 75, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.5375 \n",
      " Learned [1] of label(s) [0] , previously [35. 41.] of each.\n",
      " Learned [1] of label(s) [0] , previously [36. 41.] of each.\n",
      " Learned [1] of label(s) [0] , previously [37. 41.] of each.\n",
      " Learned [1] of label(s) [1] , previously [38. 41.] of each.\n",
      " Learned [1] of label(s) [1] , previously [38. 42.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6857 - accuracy: 0.5450\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [10. 18.]\n",
      "False Negatives by label: [18. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 80, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.545 \n",
      " Learned [1] of label(s) [1] , previously [38. 43.] of each.\n",
      " Learned [1] of label(s) [1] , previously [38. 44.] of each.\n",
      " Learned [1] of label(s) [0] , previously [38. 45.] of each.\n",
      " Learned [1] of label(s) [1] , previously [39. 45.] of each.\n",
      " Learned [1] of label(s) [0] , previously [39. 46.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6815 - accuracy: 0.5512\n",
      " (Number correct, number incorrect, number of tests): ( 770 30 800 ) %Correct:  96.25 %\n",
      "False Positives by label: [12. 18.]\n",
      "False Negatives by label: [18. 12.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 85, Epoch 0, 1 entries accuracy of OM core 0.9625 and TF 0.5512 \n",
      " Learned [1] of label(s) [0] , previously [40. 46.] of each.\n",
      " Learned [1] of label(s) [1] , previously [41. 46.] of each.\n",
      " Learned [1] of label(s) [0] , previously [41. 47.] of each.\n",
      " Learned [1] of label(s) [1] , previously [42. 47.] of each.\n",
      " Learned [1] of label(s) [0] , previously [42. 48.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6758 - accuracy: 0.5612\n",
      " (Number correct, number incorrect, number of tests): ( 771 29 800 ) %Correct:  96.375 %\n",
      "False Positives by label: [12. 17.]\n",
      "False Negatives by label: [17. 12.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 90, Epoch 0, 1 entries accuracy of OM core 0.9638 and TF 0.5612 \n",
      " Learned [1] of label(s) [0] , previously [43. 48.] of each.\n",
      " Learned [1] of label(s) [0] , previously [44. 48.] of each.\n",
      " Learned [1] of label(s) [1] , previously [45. 48.] of each.\n",
      " Learned [1] of label(s) [0] , previously [45. 49.] of each.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [0] , previously [46. 49.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6681 - accuracy: 0.5775\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 9. 18.]\n",
      "False Negatives by label: [18.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 95, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.5775 \n",
      " Learned [1] of label(s) [0] , previously [47. 49.] of each.\n",
      " Learned [1] of label(s) [0] , previously [48. 49.] of each.\n",
      " Learned [1] of label(s) [0] , previously [49. 49.] of each.\n",
      " Learned [1] of label(s) [1] , previously [50. 49.] of each.\n",
      " Learned [1] of label(s) [1] , previously [50. 50.] of each.\n",
      "800/800 [==============================] - 45s 57ms/step - loss: 0.6616 - accuracy: 0.5913\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [11. 17.]\n",
      "False Negatives by label: [17. 11.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 100, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.5913 \n",
      " Learned [1] of label(s) [0] , previously [50. 51.] of each.\n",
      " Learned [1] of label(s) [1] , previously [51. 51.] of each.\n",
      " Learned [1] of label(s) [0] , previously [51. 52.] of each.\n",
      " Learned [1] of label(s) [0] , previously [52. 52.] of each.\n",
      " Learned [1] of label(s) [0] , previously [53. 52.] of each.\n",
      " Learned [1] of label(s) [1] , previously [54. 52.] of each.\n",
      " Learned [1] of label(s) [1] , previously [54. 53.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6519 - accuracy: 0.6037\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [11. 17.]\n",
      "False Negatives by label: [17. 11.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 107, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.6037 \n",
      " Learned [1] of label(s) [1] , previously [54. 54.] of each.\n",
      " Learned [1] of label(s) [0] , previously [54. 55.] of each.\n",
      " Learned [1] of label(s) [0] , previously [55. 55.] of each.\n",
      " Learned [1] of label(s) [1] , previously [56. 55.] of each.\n",
      " Learned [1] of label(s) [0] , previously [56. 56.] of each.\n",
      " Learned [1] of label(s) [1] , previously [57. 56.] of each.\n",
      " Learned [1] of label(s) [0] , previously [57. 57.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6428 - accuracy: 0.6237\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 9. 17.]\n",
      "False Negatives by label: [17.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 114, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.6237 \n",
      " Learned [1] of label(s) [0] , previously [58. 57.] of each.\n",
      " Learned [1] of label(s) [1] , previously [59. 57.] of each.\n",
      " Learned [1] of label(s) [1] , previously [59. 58.] of each.\n",
      " Learned [1] of label(s) [1] , previously [59. 59.] of each.\n",
      " Learned [1] of label(s) [1] , previously [59. 60.] of each.\n",
      " Learned [1] of label(s) [1] , previously [59. 61.] of each.\n",
      " Learned [1] of label(s) [0] , previously [59. 62.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6371 - accuracy: 0.6350\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 8. 18.]\n",
      "False Negatives by label: [18.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 121, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.635 \n",
      " Learned [1] of label(s) [1] , previously [60. 62.] of each.\n",
      " Learned [1] of label(s) [0] , previously [60. 63.] of each.\n",
      " Learned [1] of label(s) [0] , previously [61. 63.] of each.\n",
      " Learned [1] of label(s) [1] , previously [62. 63.] of each.\n",
      " Learned [1] of label(s) [0] , previously [62. 64.] of each.\n",
      " Learned [1] of label(s) [1] , previously [63. 64.] of each.\n",
      " Learned [1] of label(s) [0] , previously [63. 65.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6310 - accuracy: 0.6500\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 9. 18.]\n",
      "False Negatives by label: [18.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 128, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.65 \n",
      " Learned [1] of label(s) [0] , previously [64. 65.] of each.\n",
      " Learned [1] of label(s) [1] , previously [65. 65.] of each.\n",
      " Learned [1] of label(s) [0] , previously [65. 66.] of each.\n",
      " Learned [1] of label(s) [1] , previously [66. 66.] of each.\n",
      " Learned [1] of label(s) [1] , previously [66. 67.] of each.\n",
      " Learned [1] of label(s) [0] , previously [66. 68.] of each.\n",
      " Learned [1] of label(s) [1] , previously [67. 68.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6251 - accuracy: 0.6612\n",
      " (Number correct, number incorrect, number of tests): ( 776 24 800 ) %Correct:  97.0 %\n",
      "False Positives by label: [ 5. 19.]\n",
      "False Negatives by label: [19.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 135, Epoch 0, 1 entries accuracy of OM core 0.97 and TF 0.6612 \n",
      " Learned [1] of label(s) [1] , previously [67. 69.] of each.\n",
      " Learned [1] of label(s) [1] , previously [67. 70.] of each.\n",
      " Learned [1] of label(s) [1] , previously [67. 71.] of each.\n",
      " Learned [1] of label(s) [0] , previously [67. 72.] of each.\n",
      " Learned [1] of label(s) [1] , previously [68. 72.] of each.\n",
      " Learned [1] of label(s) [0] , previously [68. 73.] of each.\n",
      " Learned [1] of label(s) [0] , previously [69. 73.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6197 - accuracy: 0.6675\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 142, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.6675 \n",
      " Learned [1] of label(s) [1] , previously [70. 73.] of each.\n",
      " Learned [1] of label(s) [0] , previously [70. 74.] of each.\n",
      " Learned [1] of label(s) [0] , previously [71. 74.] of each.\n",
      " Learned [1] of label(s) [0] , previously [72. 74.] of each.\n",
      " Learned [1] of label(s) [0] , previously [73. 74.] of each.\n",
      " Learned [1] of label(s) [1] , previously [74. 74.] of each.\n",
      " Learned [1] of label(s) [0] , previously [74. 75.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6147 - accuracy: 0.6812\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 149, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.6812 \n",
      " Learned [1] of label(s) [1] , previously [75. 75.] of each.\n",
      " Learned [1] of label(s) [1] , previously [75. 76.] of each.\n",
      " Learned [1] of label(s) [0] , previously [75. 77.] of each.\n",
      " Learned [1] of label(s) [0] , previously [76. 77.] of each.\n",
      " Learned [1] of label(s) [1] , previously [77. 77.] of each.\n",
      " Learned [1] of label(s) [0] , previously [77. 78.] of each.\n",
      " Learned [1] of label(s) [1] , previously [78. 78.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6064 - accuracy: 0.7000\n",
      " (Number correct, number incorrect, number of tests): ( 775 25 800 ) %Correct:  96.875 %\n",
      "False Positives by label: [ 5. 20.]\n",
      "False Negatives by label: [20.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 156, Epoch 0, 1 entries accuracy of OM core 0.9688 and TF 0.7 \n",
      " Learned [1] of label(s) [1] , previously [78. 79.] of each.\n",
      " Learned [1] of label(s) [0] , previously [78. 80.] of each.\n",
      " Learned [1] of label(s) [1] , previously [79. 80.] of each.\n",
      " Learned [1] of label(s) [1] , previously [79. 81.] of each.\n",
      " Learned [1] of label(s) [0] , previously [79. 82.] of each.\n",
      " Learned [1] of label(s) [0] , previously [80. 82.] of each.\n",
      " Learned [1] of label(s) [0] , previously [81. 82.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.6021 - accuracy: 0.7075\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 7. 20.]\n",
      "False Negatives by label: [20.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 163, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.7075 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [0] , previously [82. 82.] of each.\n",
      " Learned [1] of label(s) [1] , previously [83. 82.] of each.\n",
      " Learned [1] of label(s) [1] , previously [83. 83.] of each.\n",
      " Learned [1] of label(s) [1] , previously [83. 84.] of each.\n",
      " Learned [1] of label(s) [0] , previously [83. 85.] of each.\n",
      " Learned [1] of label(s) [1] , previously [84. 85.] of each.\n",
      " Learned [1] of label(s) [0] , previously [84. 86.] of each.\n",
      "800/800 [==============================] - 45s 57ms/step - loss: 0.5961 - accuracy: 0.7225\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 170, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.7225 \n",
      " Learned [1] of label(s) [0] , previously [85. 86.] of each.\n",
      " Learned [1] of label(s) [0] , previously [86. 86.] of each.\n",
      " Learned [1] of label(s) [0] , previously [87. 86.] of each.\n",
      " Learned [1] of label(s) [0] , previously [88. 86.] of each.\n",
      " Learned [1] of label(s) [1] , previously [89. 86.] of each.\n",
      " Learned [1] of label(s) [0] , previously [89. 87.] of each.\n",
      " Learned [1] of label(s) [1] , previously [90. 87.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5888 - accuracy: 0.7237\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 177, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.7237 \n",
      " Learned [1] of label(s) [0] , previously [90. 88.] of each.\n",
      " Learned [1] of label(s) [1] , previously [91. 88.] of each.\n",
      " Learned [1] of label(s) [0] , previously [91. 89.] of each.\n",
      " Learned [1] of label(s) [1] , previously [92. 89.] of each.\n",
      " Learned [1] of label(s) [1] , previously [92. 90.] of each.\n",
      " Learned [1] of label(s) [1] , previously [92. 91.] of each.\n",
      " Learned [1] of label(s) [1] , previously [92. 92.] of each.\n",
      "800/800 [==============================] - 45s 57ms/step - loss: 0.5847 - accuracy: 0.7262\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 7. 20.]\n",
      "False Negatives by label: [20.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 184, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.7262 \n",
      " Learned [1] of label(s) [1] , previously [92. 93.] of each.\n",
      " Learned [1] of label(s) [1] , previously [92. 94.] of each.\n",
      " Learned [1] of label(s) [0] , previously [92. 95.] of each.\n",
      " Learned [1] of label(s) [1] , previously [93. 95.] of each.\n",
      " Learned [1] of label(s) [0] , previously [93. 96.] of each.\n",
      " Learned [1] of label(s) [1] , previously [94. 96.] of each.\n",
      " Learned [1] of label(s) [0] , previously [94. 97.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5815 - accuracy: 0.7337\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 191, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.7337 \n",
      " Learned [1] of label(s) [1] , previously [95. 97.] of each.\n",
      " Learned [1] of label(s) [1] , previously [95. 98.] of each.\n",
      " Learned [1] of label(s) [1] , previously [95. 99.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 95. 100.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 95. 101.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 95. 102.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 96. 102.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5792 - accuracy: 0.7412\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 9. 19.]\n",
      "False Negatives by label: [19.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 198, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.7412 \n",
      " Learned [1] of label(s) [1] , previously [ 97. 102.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 97. 103.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 98. 103.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 99. 103.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 99. 104.] of each.\n",
      " Learned [1] of label(s) [0] , previously [100. 104.] of each.\n",
      " Learned [1] of label(s) [0] , previously [101. 104.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5769 - accuracy: 0.7450\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 9. 18.]\n",
      "False Negatives by label: [18.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 205, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.745 \n",
      " Learned [1] of label(s) [0] , previously [102. 104.] of each.\n",
      " Learned [1] of label(s) [1] , previously [103. 104.] of each.\n",
      " Learned [1] of label(s) [1] , previously [103. 105.] of each.\n",
      " Learned [1] of label(s) [1] , previously [103. 106.] of each.\n",
      " Learned [1] of label(s) [0] , previously [103. 107.] of each.\n",
      " Learned [1] of label(s) [1] , previously [104. 107.] of each.\n",
      " Learned [1] of label(s) [0] , previously [104. 108.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5731 - accuracy: 0.7550\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 9. 17.]\n",
      "False Negatives by label: [17.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 212, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.755 \n",
      " Learned [1] of label(s) [0] , previously [105. 108.] of each.\n",
      " Learned [1] of label(s) [0] , previously [106. 108.] of each.\n",
      " Learned [1] of label(s) [0] , previously [107. 108.] of each.\n",
      " Learned [1] of label(s) [1] , previously [108. 108.] of each.\n",
      " Learned [1] of label(s) [1] , previously [108. 109.] of each.\n",
      " Learned [1] of label(s) [0] , previously [108. 110.] of each.\n",
      " Learned [1] of label(s) [0] , previously [109. 110.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5675 - accuracy: 0.7650\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [10. 18.]\n",
      "False Negatives by label: [18. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 219, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.765 \n",
      " Learned [1] of label(s) [0] , previously [110. 110.] of each.\n",
      " Learned [1] of label(s) [0] , previously [111. 110.] of each.\n",
      " Learned [1] of label(s) [0] , previously [112. 110.] of each.\n",
      " Learned [1] of label(s) [1] , previously [113. 110.] of each.\n",
      " Learned [1] of label(s) [0] , previously [113. 111.] of each.\n",
      " Learned [1] of label(s) [1] , previously [114. 111.] of each.\n",
      " Learned [1] of label(s) [0] , previously [114. 112.] of each.\n",
      "800/800 [==============================] - 45s 57ms/step - loss: 0.5616 - accuracy: 0.7650\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [10. 16.]\n",
      "False Negatives by label: [16. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 226, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.765 \n",
      " Learned [1] of label(s) [1] , previously [115. 112.] of each.\n",
      " Learned [1] of label(s) [1] , previously [115. 113.] of each.\n",
      " Learned [1] of label(s) [1] , previously [115. 114.] of each.\n",
      " Learned [1] of label(s) [1] , previously [115. 115.] of each.\n",
      " Learned [1] of label(s) [0] , previously [115. 116.] of each.\n",
      " Learned [1] of label(s) [0] , previously [116. 116.] of each.\n",
      " Learned [1] of label(s) [1] , previously [117. 116.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5563 - accuracy: 0.7800\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [10. 18.]\n",
      "False Negatives by label: [18. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 233, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.78 \n",
      " Learned [1] of label(s) [0] , previously [117. 117.] of each.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [1] , previously [118. 117.] of each.\n",
      " Learned [1] of label(s) [0] , previously [118. 118.] of each.\n",
      " Learned [1] of label(s) [1] , previously [119. 118.] of each.\n",
      " Learned [1] of label(s) [0] , previously [119. 119.] of each.\n",
      " Learned [1] of label(s) [0] , previously [120. 119.] of each.\n",
      " Learned [1] of label(s) [1] , previously [121. 119.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5511 - accuracy: 0.7925\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 9. 18.]\n",
      "False Negatives by label: [18.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 240, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.7925 \n",
      " Learned [1] of label(s) [1] , previously [121. 120.] of each.\n",
      " Learned [1] of label(s) [1] , previously [121. 121.] of each.\n",
      " Learned [1] of label(s) [1] , previously [121. 122.] of each.\n",
      " Learned [1] of label(s) [1] , previously [121. 123.] of each.\n",
      " Learned [1] of label(s) [1] , previously [121. 124.] of each.\n",
      " Learned [1] of label(s) [0] , previously [121. 125.] of each.\n",
      " Learned [1] of label(s) [0] , previously [122. 125.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5481 - accuracy: 0.8075\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 9. 18.]\n",
      "False Negatives by label: [18.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 247, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.8075 \n",
      " Learned [1] of label(s) [1] , previously [123. 125.] of each.\n",
      " Learned [1] of label(s) [1] , previously [123. 126.] of each.\n",
      " Learned [1] of label(s) [0] , previously [123. 127.] of each.\n",
      " Learned [1] of label(s) [0] , previously [124. 127.] of each.\n",
      " Learned [1] of label(s) [0] , previously [125. 127.] of each.\n",
      " Learned [1] of label(s) [1] , previously [126. 127.] of each.\n",
      " Learned [1] of label(s) [0] , previously [126. 128.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5447 - accuracy: 0.8087\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 9. 19.]\n",
      "False Negatives by label: [19.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 254, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.8087 \n",
      " Learned [1] of label(s) [0] , previously [127. 128.] of each.\n",
      " Learned [1] of label(s) [1] , previously [128. 128.] of each.\n",
      " Learned [1] of label(s) [0] , previously [128. 129.] of each.\n",
      " Learned [1] of label(s) [0] , previously [129. 129.] of each.\n",
      " Learned [1] of label(s) [1] , previously [130. 129.] of each.\n",
      " Learned [1] of label(s) [0] , previously [130. 130.] of each.\n",
      " Learned [1] of label(s) [1] , previously [131. 130.] of each.\n",
      "800/800 [==============================] - 45s 57ms/step - loss: 0.5395 - accuracy: 0.8175\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 8. 19.]\n",
      "False Negatives by label: [19.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 261, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.8175 \n",
      " Learned [1] of label(s) [1] , previously [131. 131.] of each.\n",
      " Learned [1] of label(s) [1] , previously [131. 132.] of each.\n",
      " Learned [1] of label(s) [1] , previously [131. 133.] of each.\n",
      " Learned [1] of label(s) [0] , previously [131. 134.] of each.\n",
      " Learned [1] of label(s) [0] , previously [132. 134.] of each.\n",
      " Learned [1] of label(s) [1] , previously [133. 134.] of each.\n",
      " Learned [1] of label(s) [1] , previously [133. 135.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5359 - accuracy: 0.8225\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 268, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.8225 \n",
      " Learned [1] of label(s) [0] , previously [133. 136.] of each.\n",
      " Learned [1] of label(s) [0] , previously [134. 136.] of each.\n",
      " Learned [1] of label(s) [0] , previously [135. 136.] of each.\n",
      " Learned [1] of label(s) [1] , previously [136. 136.] of each.\n",
      " Learned [1] of label(s) [1] , previously [136. 137.] of each.\n",
      " Learned [1] of label(s) [0] , previously [136. 138.] of each.\n",
      " Learned [1] of label(s) [1] , previously [137. 138.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5283 - accuracy: 0.8338\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 275, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.8338 \n",
      " Learned [1] of label(s) [1] , previously [137. 139.] of each.\n",
      " Learned [1] of label(s) [1] , previously [137. 140.] of each.\n",
      " Learned [1] of label(s) [0] , previously [137. 141.] of each.\n",
      " Learned [1] of label(s) [0] , previously [138. 141.] of each.\n",
      " Learned [1] of label(s) [1] , previously [139. 141.] of each.\n",
      " Learned [1] of label(s) [1] , previously [139. 142.] of each.\n",
      " Learned [1] of label(s) [0] , previously [139. 143.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5255 - accuracy: 0.8425\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 6. 20.]\n",
      "False Negatives by label: [20.  6.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 282, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.8425 \n",
      " Learned [1] of label(s) [1] , previously [140. 143.] of each.\n",
      " Learned [1] of label(s) [1] , previously [140. 144.] of each.\n",
      " Learned [1] of label(s) [0] , previously [140. 145.] of each.\n",
      " Learned [1] of label(s) [0] , previously [141. 145.] of each.\n",
      " Learned [1] of label(s) [1] , previously [142. 145.] of each.\n",
      " Learned [1] of label(s) [0] , previously [142. 146.] of each.\n",
      " Learned [1] of label(s) [0] , previously [143. 146.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5210 - accuracy: 0.8475\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 9. 18.]\n",
      "False Negatives by label: [18.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 289, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.8475 \n",
      " Learned [1] of label(s) [1] , previously [144. 146.] of each.\n",
      " Learned [1] of label(s) [1] , previously [144. 147.] of each.\n",
      " Learned [1] of label(s) [1] , previously [144. 148.] of each.\n",
      " Learned [1] of label(s) [1] , previously [144. 149.] of each.\n",
      " Learned [1] of label(s) [1] , previously [144. 150.] of each.\n",
      " Learned [1] of label(s) [1] , previously [144. 151.] of each.\n",
      " Learned [1] of label(s) [0] , previously [144. 152.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5174 - accuracy: 0.8562\n",
      " (Number correct, number incorrect, number of tests): ( 775 25 800 ) %Correct:  96.875 %\n",
      "False Positives by label: [ 6. 19.]\n",
      "False Negatives by label: [19.  6.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 296, Epoch 0, 1 entries accuracy of OM core 0.9688 and TF 0.8562 \n",
      " Learned [1] of label(s) [0] , previously [145. 152.] of each.\n",
      " Learned [1] of label(s) [0] , previously [146. 152.] of each.\n",
      " Learned [1] of label(s) [1] , previously [147. 152.] of each.\n",
      " Learned [1] of label(s) [1] , previously [147. 153.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5163 - accuracy: 0.8562\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 7. 20.]\n",
      "False Negatives by label: [20.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 300, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.8562 \n",
      " Learned [1] of label(s) [0] , previously [147. 154.] of each.\n",
      " Learned [1] of label(s) [0] , previously [148. 154.] of each.\n",
      " Learned [1] of label(s) [0] , previously [149. 154.] of each.\n",
      " Learned [1] of label(s) [0] , previously [150. 154.] of each.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [0] , previously [151. 154.] of each.\n",
      " Learned [1] of label(s) [0] , previously [152. 154.] of each.\n",
      " Learned [1] of label(s) [1] , previously [153. 154.] of each.\n",
      " Learned [1] of label(s) [0] , previously [153. 155.] of each.\n",
      " Learned [1] of label(s) [0] , previously [154. 155.] of each.\n",
      " Learned [1] of label(s) [0] , previously [155. 155.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5094 - accuracy: 0.8562\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 8. 19.]\n",
      "False Negatives by label: [19.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 310, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.8562 \n",
      " Learned [1] of label(s) [0] , previously [156. 155.] of each.\n",
      " Learned [1] of label(s) [1] , previously [157. 155.] of each.\n",
      " Learned [1] of label(s) [0] , previously [157. 156.] of each.\n",
      " Learned [1] of label(s) [0] , previously [158. 156.] of each.\n",
      " Learned [1] of label(s) [0] , previously [159. 156.] of each.\n",
      " Learned [1] of label(s) [0] , previously [160. 156.] of each.\n",
      " Learned [1] of label(s) [1] , previously [161. 156.] of each.\n",
      " Learned [1] of label(s) [0] , previously [161. 157.] of each.\n",
      " Learned [1] of label(s) [0] , previously [162. 157.] of each.\n",
      " Learned [1] of label(s) [0] , previously [163. 157.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5039 - accuracy: 0.8650\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 320, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.865 \n",
      " Learned [1] of label(s) [0] , previously [164. 157.] of each.\n",
      " Learned [1] of label(s) [0] , previously [165. 157.] of each.\n",
      " Learned [1] of label(s) [1] , previously [166. 157.] of each.\n",
      " Learned [1] of label(s) [0] , previously [166. 158.] of each.\n",
      " Learned [1] of label(s) [0] , previously [167. 158.] of each.\n",
      " Learned [1] of label(s) [0] , previously [168. 158.] of each.\n",
      " Learned [1] of label(s) [1] , previously [169. 158.] of each.\n",
      " Learned [1] of label(s) [1] , previously [169. 159.] of each.\n",
      " Learned [1] of label(s) [0] , previously [169. 160.] of each.\n",
      " Learned [1] of label(s) [1] , previously [170. 160.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5027 - accuracy: 0.8625\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 330, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.8625 \n",
      " Learned [1] of label(s) [1] , previously [170. 161.] of each.\n",
      " Learned [1] of label(s) [1] , previously [170. 162.] of each.\n",
      " Learned [1] of label(s) [0] , previously [170. 163.] of each.\n",
      " Learned [1] of label(s) [1] , previously [171. 163.] of each.\n",
      " Learned [1] of label(s) [1] , previously [171. 164.] of each.\n",
      " Learned [1] of label(s) [0] , previously [171. 165.] of each.\n",
      " Learned [1] of label(s) [1] , previously [172. 165.] of each.\n",
      " Learned [1] of label(s) [1] , previously [172. 166.] of each.\n",
      " Learned [1] of label(s) [0] , previously [172. 167.] of each.\n",
      " Learned [1] of label(s) [1] , previously [173. 167.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.5004 - accuracy: 0.8637\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 340, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.8637 \n",
      " Learned [1] of label(s) [1] , previously [173. 168.] of each.\n",
      " Learned [1] of label(s) [0] , previously [173. 169.] of each.\n",
      " Learned [1] of label(s) [1] , previously [174. 169.] of each.\n",
      " Learned [1] of label(s) [1] , previously [174. 170.] of each.\n",
      " Learned [1] of label(s) [1] , previously [174. 171.] of each.\n",
      " Learned [1] of label(s) [1] , previously [174. 172.] of each.\n",
      " Learned [1] of label(s) [0] , previously [174. 173.] of each.\n",
      " Learned [1] of label(s) [1] , previously [175. 173.] of each.\n",
      " Learned [1] of label(s) [1] , previously [175. 174.] of each.\n",
      " Learned [1] of label(s) [1] , previously [175. 175.] of each.\n",
      "800/800 [==============================] - 46s 57ms/step - loss: 0.4964 - accuracy: 0.8675\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 8. 19.]\n",
      "False Negatives by label: [19.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 350, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.8675 \n",
      " Learned [1] of label(s) [0] , previously [175. 176.] of each.\n",
      " Learned [1] of label(s) [0] , previously [176. 176.] of each.\n",
      " Learned [1] of label(s) [0] , previously [177. 176.] of each.\n",
      " Learned [1] of label(s) [0] , previously [178. 176.] of each.\n",
      " Learned [1] of label(s) [0] , previously [179. 176.] of each.\n",
      " Learned [1] of label(s) [0] , previously [180. 176.] of each.\n",
      " Learned [1] of label(s) [1] , previously [181. 176.] of each.\n",
      " Learned [1] of label(s) [1] , previously [181. 177.] of each.\n",
      " Learned [1] of label(s) [0] , previously [181. 178.] of each.\n",
      " Learned [1] of label(s) [1] , previously [182. 178.] of each.\n",
      "800/800 [==============================] - 51s 63ms/step - loss: 0.4914 - accuracy: 0.8763\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 360, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.8763 \n",
      " Learned [1] of label(s) [0] , previously [182. 179.] of each.\n",
      " Learned [1] of label(s) [0] , previously [183. 179.] of each.\n",
      " Learned [1] of label(s) [1] , previously [184. 179.] of each.\n",
      " Learned [1] of label(s) [1] , previously [184. 180.] of each.\n",
      " Learned [1] of label(s) [0] , previously [184. 181.] of each.\n",
      " Learned [1] of label(s) [0] , previously [185. 181.] of each.\n",
      " Learned [1] of label(s) [0] , previously [186. 181.] of each.\n",
      " Learned [1] of label(s) [0] , previously [187. 181.] of each.\n",
      " Learned [1] of label(s) [0] , previously [188. 181.] of each.\n",
      " Learned [1] of label(s) [1] , previously [189. 181.] of each.\n",
      "800/800 [==============================] - 50s 62ms/step - loss: 0.4902 - accuracy: 0.8725\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 370, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.8725 \n",
      " Learned [1] of label(s) [0] , previously [189. 182.] of each.\n",
      " Learned [1] of label(s) [0] , previously [190. 182.] of each.\n",
      " Learned [1] of label(s) [1] , previously [191. 182.] of each.\n",
      " Learned [1] of label(s) [1] , previously [191. 183.] of each.\n",
      " Learned [1] of label(s) [1] , previously [191. 184.] of each.\n",
      " Learned [1] of label(s) [1] , previously [191. 185.] of each.\n",
      " Learned [1] of label(s) [1] , previously [191. 186.] of each.\n",
      " Learned [1] of label(s) [0] , previously [191. 187.] of each.\n",
      " Learned [1] of label(s) [0] , previously [192. 187.] of each.\n",
      " Learned [1] of label(s) [0] , previously [193. 187.] of each.\n",
      "800/800 [==============================] - 53s 66ms/step - loss: 0.4878 - accuracy: 0.8712\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 380, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.8712 \n",
      " Learned [1] of label(s) [0] , previously [194. 187.] of each.\n",
      " Learned [1] of label(s) [1] , previously [195. 187.] of each.\n",
      " Learned [1] of label(s) [0] , previously [195. 188.] of each.\n",
      " Learned [1] of label(s) [0] , previously [196. 188.] of each.\n",
      " Learned [1] of label(s) [1] , previously [197. 188.] of each.\n",
      " Learned [1] of label(s) [0] , previously [197. 189.] of each.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [0] , previously [198. 189.] of each.\n",
      " Learned [1] of label(s) [1] , previously [199. 189.] of each.\n",
      " Learned [1] of label(s) [1] , previously [199. 190.] of each.\n",
      " Learned [1] of label(s) [1] , previously [199. 191.] of each.\n",
      "800/800 [==============================] - 60s 74ms/step - loss: 0.4842 - accuracy: 0.8750\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 8. 19.]\n",
      "False Negatives by label: [19.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 390, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.875 \n",
      " Learned [1] of label(s) [1] , previously [199. 192.] of each.\n",
      " Learned [1] of label(s) [1] , previously [199. 193.] of each.\n",
      " Learned [1] of label(s) [0] , previously [199. 194.] of each.\n",
      " Learned [1] of label(s) [0] , previously [200. 194.] of each.\n",
      " Learned [1] of label(s) [0] , previously [201. 194.] of each.\n",
      " Learned [1] of label(s) [0] , previously [202. 194.] of each.\n",
      " Learned [1] of label(s) [1] , previously [203. 194.] of each.\n",
      " Learned [1] of label(s) [1] , previously [203. 195.] of each.\n",
      " Learned [1] of label(s) [1] , previously [203. 196.] of each.\n",
      " Learned [1] of label(s) [0] , previously [203. 197.] of each.\n",
      "800/800 [==============================] - 49s 61ms/step - loss: 0.4802 - accuracy: 0.8850\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 8. 18.]\n",
      "False Negatives by label: [18.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 400, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.885 \n",
      " Learned [1] of label(s) [0] , previously [204. 197.] of each.\n",
      " Learned [1] of label(s) [0] , previously [205. 197.] of each.\n",
      " Learned [1] of label(s) [1] , previously [206. 197.] of each.\n",
      " Learned [1] of label(s) [0] , previously [206. 198.] of each.\n",
      " Learned [1] of label(s) [1] , previously [207. 198.] of each.\n",
      " Learned [1] of label(s) [0] , previously [207. 199.] of each.\n",
      " Learned [1] of label(s) [0] , previously [208. 199.] of each.\n",
      " Learned [1] of label(s) [0] , previously [209. 199.] of each.\n",
      " Learned [1] of label(s) [0] , previously [210. 199.] of each.\n",
      " Learned [1] of label(s) [0] , previously [211. 199.] of each.\n",
      "800/800 [==============================] - 50s 62ms/step - loss: 0.4787 - accuracy: 0.8775\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [10. 18.]\n",
      "False Negatives by label: [18. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 410, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.8775 \n",
      " Learned [1] of label(s) [1] , previously [212. 199.] of each.\n",
      " Learned [1] of label(s) [1] , previously [212. 200.] of each.\n",
      " Learned [1] of label(s) [0] , previously [212. 201.] of each.\n",
      " Learned [1] of label(s) [0] , previously [213. 201.] of each.\n",
      " Learned [1] of label(s) [0] , previously [214. 201.] of each.\n",
      " Learned [1] of label(s) [1] , previously [215. 201.] of each.\n",
      " Learned [1] of label(s) [0] , previously [215. 202.] of each.\n",
      " Learned [1] of label(s) [1] , previously [216. 202.] of each.\n",
      " Learned [1] of label(s) [0] , previously [216. 203.] of each.\n",
      " Learned [1] of label(s) [0] , previously [217. 203.] of each.\n",
      "800/800 [==============================] - 47s 59ms/step - loss: 0.4793 - accuracy: 0.8737\n",
      " (Number correct, number incorrect, number of tests): ( 775 25 800 ) %Correct:  96.875 %\n",
      "False Positives by label: [ 9. 16.]\n",
      "False Negatives by label: [16.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 420, Epoch 0, 1 entries accuracy of OM core 0.9688 and TF 0.8737 \n",
      " Learned [1] of label(s) [1] , previously [218. 203.] of each.\n",
      " Learned [1] of label(s) [1] , previously [218. 204.] of each.\n",
      " Learned [1] of label(s) [1] , previously [218. 205.] of each.\n",
      " Learned [1] of label(s) [0] , previously [218. 206.] of each.\n",
      " Learned [1] of label(s) [1] , previously [219. 206.] of each.\n",
      " Learned [1] of label(s) [1] , previously [219. 207.] of each.\n",
      " Learned [1] of label(s) [0] , previously [219. 208.] of each.\n",
      " Learned [1] of label(s) [0] , previously [220. 208.] of each.\n",
      " Learned [1] of label(s) [0] , previously [221. 208.] of each.\n",
      " Learned [1] of label(s) [0] , previously [222. 208.] of each.\n",
      "800/800 [==============================] - 48s 60ms/step - loss: 0.4778 - accuracy: 0.87370s - loss: 0.4774 \n",
      " (Number correct, number incorrect, number of tests): ( 775 25 800 ) %Correct:  96.875 %\n",
      "False Positives by label: [ 9. 16.]\n",
      "False Negatives by label: [16.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 430, Epoch 0, 1 entries accuracy of OM core 0.9688 and TF 0.8737 \n",
      " Learned [1] of label(s) [0] , previously [223. 208.] of each.\n",
      " Learned [1] of label(s) [0] , previously [224. 208.] of each.\n",
      " Learned [1] of label(s) [1] , previously [225. 208.] of each.\n",
      " Learned [1] of label(s) [1] , previously [225. 209.] of each.\n",
      " Learned [1] of label(s) [0] , previously [225. 210.] of each.\n",
      " Learned [1] of label(s) [1] , previously [226. 210.] of each.\n",
      " Learned [1] of label(s) [1] , previously [226. 211.] of each.\n",
      " Learned [1] of label(s) [0] , previously [226. 212.] of each.\n",
      " Learned [1] of label(s) [0] , previously [227. 212.] of each.\n",
      " Learned [1] of label(s) [0] , previously [228. 212.] of each.\n",
      "800/800 [==============================] - 54s 67ms/step - loss: 0.4778 - accuracy: 0.8725\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [10. 16.]\n",
      "False Negatives by label: [16. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 440, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.8725 \n",
      " Learned [1] of label(s) [1] , previously [229. 212.] of each.\n",
      " Learned [1] of label(s) [1] , previously [229. 213.] of each.\n",
      " Learned [1] of label(s) [1] , previously [229. 214.] of each.\n",
      " Learned [1] of label(s) [1] , previously [229. 215.] of each.\n",
      " Learned [1] of label(s) [0] , previously [229. 216.] of each.\n",
      " Learned [1] of label(s) [0] , previously [230. 216.] of each.\n",
      " Learned [1] of label(s) [0] , previously [231. 216.] of each.\n",
      " Learned [1] of label(s) [1] , previously [232. 216.] of each.\n",
      " Learned [1] of label(s) [1] , previously [232. 217.] of each.\n",
      " Learned [1] of label(s) [1] , previously [232. 218.] of each.\n",
      "800/800 [==============================] - 59s 73ms/step - loss: 0.4726 - accuracy: 0.8800\n",
      " (Number correct, number incorrect, number of tests): ( 775 25 800 ) %Correct:  96.875 %\n",
      "False Positives by label: [ 9. 16.]\n",
      "False Negatives by label: [16.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 450, Epoch 0, 1 entries accuracy of OM core 0.9688 and TF 0.88 \n",
      " Learned [1] of label(s) [1] , previously [232. 219.] of each.\n",
      " Learned [1] of label(s) [1] , previously [232. 220.] of each.\n",
      " Learned [1] of label(s) [0] , previously [232. 221.] of each.\n",
      " Learned [1] of label(s) [0] , previously [233. 221.] of each.\n",
      " Learned [1] of label(s) [1] , previously [234. 221.] of each.\n",
      " Learned [1] of label(s) [0] , previously [234. 222.] of each.\n",
      " Learned [1] of label(s) [1] , previously [235. 222.] of each.\n",
      " Learned [1] of label(s) [1] , previously [235. 223.] of each.\n",
      " Learned [1] of label(s) [0] , previously [235. 224.] of each.\n",
      " Learned [1] of label(s) [1] , previously [236. 224.] of each.\n",
      "800/800 [==============================] - 78s 97ms/step - loss: 0.4649 - accuracy: 0.8963\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [10. 17.]\n",
      "False Negatives by label: [17. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 460, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.8963 \n",
      " Learned [1] of label(s) [1] , previously [236. 225.] of each.\n",
      " Learned [1] of label(s) [0] , previously [236. 226.] of each.\n",
      " Learned [1] of label(s) [0] , previously [237. 226.] of each.\n",
      " Learned [1] of label(s) [1] , previously [238. 226.] of each.\n",
      " Learned [1] of label(s) [1] , previously [238. 227.] of each.\n",
      " Learned [1] of label(s) [1] , previously [238. 228.] of each.\n",
      " Learned [1] of label(s) [1] , previously [238. 229.] of each.\n",
      " Learned [1] of label(s) [0] , previously [238. 230.] of each.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [0] , previously [239. 230.] of each.\n",
      " Learned [1] of label(s) [1] , previously [240. 230.] of each.\n",
      "800/800 [==============================] - 155s 194ms/step - loss: 0.4606 - accuracy: 0.9050\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 9. 19.]\n",
      "False Negatives by label: [19.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 470, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.905 \n",
      " Learned [1] of label(s) [1] , previously [240. 231.] of each.\n",
      " Learned [1] of label(s) [1] , previously [240. 232.] of each.\n",
      " Learned [1] of label(s) [0] , previously [240. 233.] of each.\n",
      " Learned [1] of label(s) [1] , previously [241. 233.] of each.\n",
      " Learned [1] of label(s) [0] , previously [241. 234.] of each.\n",
      " Learned [1] of label(s) [0] , previously [242. 234.] of each.\n",
      " Learned [1] of label(s) [0] , previously [243. 234.] of each.\n",
      " Learned [1] of label(s) [1] , previously [244. 234.] of each.\n",
      " Learned [1] of label(s) [1] , previously [244. 235.] of each.\n",
      " Learned [1] of label(s) [1] , previously [244. 236.] of each.\n",
      "800/800 [==============================] - 153s 191ms/step - loss: 0.4553 - accuracy: 0.9125\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 9. 18.]\n",
      "False Negatives by label: [18.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 480, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.9125 \n",
      " Learned [1] of label(s) [1] , previously [244. 237.] of each.\n",
      " Learned [1] of label(s) [0] , previously [244. 238.] of each.\n",
      " Learned [1] of label(s) [0] , previously [245. 238.] of each.\n",
      " Learned [1] of label(s) [0] , previously [246. 238.] of each.\n",
      " Learned [1] of label(s) [0] , previously [247. 238.] of each.\n",
      " Learned [1] of label(s) [1] , previously [248. 238.] of each.\n",
      " Learned [1] of label(s) [1] , previously [248. 239.] of each.\n",
      " Learned [1] of label(s) [1] , previously [248. 240.] of each.\n",
      " Learned [1] of label(s) [1] , previously [248. 241.] of each.\n",
      " Learned [1] of label(s) [0] , previously [248. 242.] of each.\n",
      "800/800 [==============================] - 148s 184ms/step - loss: 0.4526 - accuracy: 0.9187\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 9. 19.]\n",
      "False Negatives by label: [19.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 490, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.9187 \n",
      " Learned [1] of label(s) [0] , previously [249. 242.] of each.\n",
      " Learned [1] of label(s) [1] , previously [250. 242.] of each.\n",
      " Learned [1] of label(s) [1] , previously [250. 243.] of each.\n",
      " Learned [1] of label(s) [0] , previously [250. 244.] of each.\n",
      " Learned [1] of label(s) [0] , previously [251. 244.] of each.\n",
      " Learned [1] of label(s) [0] , previously [252. 244.] of each.\n",
      " Learned [1] of label(s) [1] , previously [253. 244.] of each.\n",
      " Learned [1] of label(s) [1] , previously [253. 245.] of each.\n",
      " Learned [1] of label(s) [0] , previously [253. 246.] of each.\n",
      " Learned [1] of label(s) [1] , previously [254. 246.] of each.\n",
      "800/800 [==============================] - 150s 187ms/step - loss: 0.4504 - accuracy: 0.9200\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 8. 19.]\n",
      "False Negatives by label: [19.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 500, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.92 \n",
      " Learned [1] of label(s) [0] , previously [254. 247.] of each.\n",
      " Learned [1] of label(s) [1] , previously [255. 247.] of each.\n",
      " Learned [1] of label(s) [0] , previously [255. 248.] of each.\n",
      " Learned [1] of label(s) [1] , previously [256. 248.] of each.\n",
      " Learned [1] of label(s) [0] , previously [256. 249.] of each.\n",
      " Learned [1] of label(s) [0] , previously [257. 249.] of each.\n",
      " Learned [1] of label(s) [1] , previously [258. 249.] of each.\n",
      " Learned [1] of label(s) [0] , previously [258. 250.] of each.\n",
      " Learned [1] of label(s) [1] , previously [259. 250.] of each.\n",
      " Learned [1] of label(s) [1] , previously [259. 251.] of each.\n",
      "800/800 [==============================] - 149s 186ms/step - loss: 0.4486 - accuracy: 0.9212\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 8. 19.]\n",
      "False Negatives by label: [19.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 510, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.9212 \n",
      " Learned [1] of label(s) [1] , previously [259. 252.] of each.\n",
      " Learned [1] of label(s) [0] , previously [259. 253.] of each.\n",
      " Learned [1] of label(s) [1] , previously [260. 253.] of each.\n",
      " Learned [1] of label(s) [0] , previously [260. 254.] of each.\n",
      " Learned [1] of label(s) [0] , previously [261. 254.] of each.\n",
      " Learned [1] of label(s) [1] , previously [262. 254.] of each.\n",
      " Learned [1] of label(s) [1] , previously [262. 255.] of each.\n",
      " Learned [1] of label(s) [0] , previously [262. 256.] of each.\n",
      " Learned [1] of label(s) [1] , previously [263. 256.] of each.\n",
      " Learned [1] of label(s) [0] , previously [263. 257.] of each.\n",
      "800/800 [==============================] - 151s 189ms/step - loss: 0.4458 - accuracy: 0.9225\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 8. 19.]\n",
      "False Negatives by label: [19.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 520, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.9225 \n",
      " Learned [1] of label(s) [0] , previously [264. 257.] of each.\n",
      " Learned [1] of label(s) [1] , previously [265. 257.] of each.\n",
      " Learned [1] of label(s) [1] , previously [265. 258.] of each.\n",
      " Learned [1] of label(s) [0] , previously [265. 259.] of each.\n",
      " Learned [1] of label(s) [1] , previously [266. 259.] of each.\n",
      " Learned [1] of label(s) [1] , previously [266. 260.] of each.\n",
      " Learned [1] of label(s) [0] , previously [266. 261.] of each.\n",
      " Learned [1] of label(s) [1] , previously [267. 261.] of each.\n",
      " Learned [1] of label(s) [0] , previously [267. 262.] of each.\n",
      " Learned [1] of label(s) [1] , previously [268. 262.] of each.\n",
      "800/800 [==============================] - 182s 227ms/step - loss: 0.4456 - accuracy: 0.9212\n",
      " (Number correct, number incorrect, number of tests): ( 772 28 800 ) %Correct:  96.5 %\n",
      "False Positives by label: [ 9. 19.]\n",
      "False Negatives by label: [19.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 530, Epoch 0, 1 entries accuracy of OM core 0.965 and TF 0.9212 \n",
      " Learned [1] of label(s) [1] , previously [268. 263.] of each.\n",
      " Learned [1] of label(s) [1] , previously [268. 264.] of each.\n",
      " Learned [1] of label(s) [0] , previously [268. 265.] of each.\n",
      " Learned [1] of label(s) [1] , previously [269. 265.] of each.\n",
      " Learned [1] of label(s) [1] , previously [269. 266.] of each.\n",
      " Learned [1] of label(s) [1] , previously [269. 267.] of each.\n",
      " Learned [1] of label(s) [1] , previously [269. 268.] of each.\n",
      " Learned [1] of label(s) [0] , previously [269. 269.] of each.\n",
      " Learned [1] of label(s) [0] , previously [270. 269.] of each.\n",
      " Learned [1] of label(s) [0] , previously [271. 269.] of each.\n",
      "800/800 [==============================] - 206s 257ms/step - loss: 0.4421 - accuracy: 0.9275\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 8. 19.]\n",
      "False Negatives by label: [19.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 540, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.9275 \n",
      " Learned [1] of label(s) [0] , previously [272. 269.] of each.\n",
      " Learned [1] of label(s) [1] , previously [273. 269.] of each.\n",
      " Learned [1] of label(s) [0] , previously [273. 270.] of each.\n",
      " Learned [1] of label(s) [1] , previously [274. 270.] of each.\n",
      " Learned [1] of label(s) [0] , previously [274. 271.] of each.\n",
      " Learned [1] of label(s) [1] , previously [275. 271.] of each.\n",
      " Learned [1] of label(s) [1] , previously [275. 272.] of each.\n",
      " Learned [1] of label(s) [1] , previously [275. 273.] of each.\n",
      " Learned [1] of label(s) [1] , previously [275. 274.] of each.\n",
      " Learned [1] of label(s) [1] , previously [275. 275.] of each.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 194s 243ms/step - loss: 0.4415 - accuracy: 0.9275\n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 8. 19.]\n",
      "False Negatives by label: [19.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 550, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.9275 \n",
      " Learned [1] of label(s) [0] , previously [275. 276.] of each.\n",
      " Learned [1] of label(s) [1] , previously [276. 276.] of each.\n",
      " Learned [1] of label(s) [1] , previously [276. 277.] of each.\n",
      " Learned [1] of label(s) [1] , previously [276. 278.] of each.\n",
      " Learned [1] of label(s) [1] , previously [276. 279.] of each.\n",
      " Learned [1] of label(s) [0] , previously [276. 280.] of each.\n",
      " Learned [1] of label(s) [0] , previously [277. 280.] of each.\n",
      " Learned [1] of label(s) [0] , previously [278. 280.] of each.\n",
      " Learned [1] of label(s) [1] , previously [279. 280.] of each.\n",
      " Learned [1] of label(s) [1] , previously [279. 281.] of each.\n",
      "800/800 [==============================] - 154s 192ms/step - loss: 0.4430 - accuracy: 0.9250\n",
      " (Number correct, number incorrect, number of tests): ( 774 26 800 ) %Correct:  96.75 %\n",
      "False Positives by label: [ 7. 19.]\n",
      "False Negatives by label: [19.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 560, Epoch 0, 1 entries accuracy of OM core 0.9675 and TF 0.925 \n",
      " Learned [1] of label(s) [0] , previously [279. 282.] of each.\n",
      " Learned [1] of label(s) [1] , previously [280. 282.] of each.\n",
      " Learned [1] of label(s) [0] , previously [280. 283.] of each.\n",
      " Learned [1] of label(s) [1] , previously [281. 283.] of each.\n",
      " Learned [1] of label(s) [0] , previously [281. 284.] of each.\n",
      " Learned [1] of label(s) [1] , previously [282. 284.] of each.\n",
      " Learned [1] of label(s) [0] , previously [282. 285.] of each.\n",
      " Learned [1] of label(s) [0] , previously [283. 285.] of each.\n",
      " Learned [1] of label(s) [0] , previously [284. 285.] of each.\n",
      " Learned [1] of label(s) [1] , previously [285. 285.] of each.\n",
      "800/800 [==============================] - 150s 187ms/step - loss: 0.4438 - accuracy: 0.9237 - loss: 0.4441 - accuracy: \n",
      " (Number correct, number incorrect, number of tests): ( 773 27 800 ) %Correct:  96.625 %\n",
      "False Positives by label: [ 7. 20.]\n",
      "False Negatives by label: [20.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 570, Epoch 0, 1 entries accuracy of OM core 0.9662 and TF 0.9237 \n",
      " Learned [1] of label(s) [1] , previously [285. 286.] of each.\n",
      " Learned [1] of label(s) [1] , previously [285. 287.] of each.\n",
      " Learned [1] of label(s) [1] , previously [285. 288.] of each.\n",
      " Learned [1] of label(s) [1] , previously [285. 289.] of each.\n",
      " Learned [1] of label(s) [0] , previously [285. 290.] of each.\n",
      " Learned [1] of label(s) [0] , previously [286. 290.] of each.\n",
      " Learned [1] of label(s) [1] , previously [287. 290.] of each.\n",
      " Learned [1] of label(s) [1] , previously [287. 291.] of each.\n",
      " Learned [1] of label(s) [0] , previously [287. 292.] of each.\n",
      " Learned [1] of label(s) [0] , previously [288. 292.] of each.\n",
      "617/800 [======================>.......] - ETA: 34s - loss: 0.4450 - accuracy: 0.9190- ETA: 36s - loss: 0.4442 - accuracy: "
     ]
    }
   ],
   "source": [
    "for j in range(initial_epochs):\n",
    "    if j>0:  #update graph points if doing more than one epoch\n",
    "        validate_points=np.concatenate([np.arange(0,3000,30)])\n",
    "        graph_points[num_stored:]=[]\n",
    "        graph_points=graph_points+list(validate_points.copy()+len(labels_presented)+1)   #need to figure out graphing stuff\n",
    "\n",
    "    for i, data in train_dataset.enumerate():  #seems similar to fit \n",
    "        \n",
    "        labels_presented.append(data[1])  # store record of labels presented\n",
    "        \n",
    "        #t = time.time()\n",
    "        \n",
    "        #the original learning        \n",
    "        batch_logs = orig_TF_paradigm_model.train_step(data)\n",
    "        \n",
    "        #TF_time=time.time() - t\n",
    "\n",
    "        \n",
    "        #t = time.time()      # I include time to percolate data through base.  \n",
    "        \n",
    "        # prepare data to send out to OM.        \n",
    "        net_out = model_with_av_layer.predict_on_batch(data[0])  # run through base layer to have data ready for OM\n",
    "        \n",
    "        \n",
    "        OMmodel.learn(net_out,data[1])  # needs data post base and label\n",
    "\n",
    "        # call to the API\n",
    "        \n",
    "        Send_to_API(MODEL,dataset_name, top_layer_inputs, labels)\n",
    "        \n",
    "        json_back=Get_from_API(MODEL,dataset_name)\n",
    "        \n",
    "        u_model=json_back['model']\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #OM_time=time.time() - t\n",
    "        \n",
    "        #print('Learning Time  OM:',np.round(OM_time,4),'  TF:',np.round(TF_time,4),'  TF/OM=',np.round(TF_time/OM_time,4),'X')\n",
    "        \n",
    "        # this would be equivalent to a callback from the API getting back the weights from the API\n",
    "        # the TF structure recieves the weights that come back put into the trainable layer: OMmodel.u_model\n",
    "        tf_model_tobe_trained_by_OM.trainable_weights[0].assign(tf.Variable(np.float32(OMmodel.u_model.T))) \n",
    "        # this is now the TF equivalent of OM learned net \n",
    "        \n",
    "        #this is code to validate but it is repeated in the validated_points\n",
    "        #loss0, accuracy0 = tf_model_tobe_trained_by_OM.evaluate(validation_dataset)  # it can be validated and run just like the original\n",
    "        \n",
    "        \n",
    "        if i in validate_points:  # minimizing number of validations because it takes too long\n",
    "        \n",
    "            # validating TF learned top layer\n",
    "            loss_, accuracy_ = orig_TF_paradigm_model.evaluate(validation_dataset)\n",
    "\n",
    "            # validating directly with OM, This function is not needed in the API, just the tf_model_tobe_trained_by_OM code (a few down)\n",
    "            OMmodel.test_data_score(net_out_validation_for_OM,validation_labels[0,:])  #,verbose=True) \n",
    "                \n",
    "            # adding TF accuracies to the record\n",
    "            tf_learn_Vacc.append(accuracy_)  \n",
    "            \n",
    "            # adding OM  accuracies to the records\n",
    "            OM_learn_Vacc.append(OMmodel.last_score_accuracy/100) # using direct validation\n",
    "\n",
    "            if False:  # extra sanity validation to show OMTF == OM core but extra tests take too much time if you are training the net\n",
    "            \n",
    "                # validating OM learned top layer but equivalent net to TF\n",
    "                loss0, accuracy_OMTF = tf_model_tobe_trained_by_OM.evaluate(validation_dataset)  # it is validated and run just like the original TF\n",
    "                #OM_learn_Vacc.append(accuracy_OMTF)\n",
    "\n",
    "                print(\"Training: Batch {}, Epoch {}, {} entries accuracy of OM core {} OMTF {} (should be roughly equivalent to OM) and TF {} \".format(i,j, len(data[0]), OM_learn_Vacc[-1],accuracy_OMTF,accuracy_))\n",
    "            \n",
    "            else:\n",
    "                print(\"Training: Batch {}, Epoch {}, {} entries accuracy of OM core {} and TF {} \".format(i,j, len(data[0]), np.round(OM_learn_Vacc[-1],4),np.round(accuracy_,4)))\n",
    "\n",
    "        \n",
    "    \n",
    "    num_stored=len(OM_learn_Vacc)\n",
    "    \n",
    "    if False:  # epoch plot not implemented in this version\n",
    "        OM_epoch_Vacc_old[j]=sum(OM_learn_Vacc_old[(j*num_batches_in_train_dataset):((j+1)*num_batches_in_train_dataset)])/num_batches_in_train_dataset\n",
    "        tf_epoch_Vacc_old[j]=sum(tf_learn_Vacc_old[(j*num_batches_in_train_dataset):((j+1)*num_batches_in_train_dataset)])/num_batches_in_train_dataset\n",
    "        print(\"Epoch {} summing {} batches to get epoch accuracy of OM on new {} old {} and TF on new {} old {} data\".format(j,i,OM_epoch_Vacc[j],OM_epoch_Vacc_old[j],tf_learn_Vacc[j],tf_epoch_Vacc_old[j]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(OM_learn_Vacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# indicate when all animals are presented at least once\n",
    "all_presented=1\n",
    "while len(np.unique(labels_presented[0:all_presented])) != len(np.unique(labels_presented)):\n",
    "    all_presented=all_presented+1\n",
    "all_presented=np.where(np.array(graph_points) >= all_presented)[0][0]   #first index where all presented\n",
    "print('first validation run after all animals were presented:',all_presented,'\\nwhich begins with training instance',graph_points[all_presented])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grange=int(graph_points[num_stored]*.02)     #90  #31 #90 #121  # choosing a nice zoom range\n",
    "if grange==0:\n",
    "    grange=num_stored\n",
    "maxpoint=max(graph_points[0:grange])\n",
    "print(maxpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoomed figure\n",
    "print('first',graph_points[grange],'plots')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(graph_points[0:(all_presented+1)],OM_learn_Vacc[0:(all_presented+1)],color='darkblue')\n",
    "plt.plot(graph_points[all_presented:grange],OM_learn_Vacc[all_presented:grange],color='cornflowerblue', label='OM Accuracy NEW')\n",
    "plt.plot(graph_points[0:grange],tf_learn_Vacc[0:grange],color='orange', label='TF Accuracy NEW')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "plt.ylim([0.4,1.0])\n",
    "plt.title(\"OM vs TF {} Accuracy within {} Batch={} Example(s) Trained\".format(dataset_name,graph_points[grange],BATCH_SIZE))\n",
    "\n",
    "plt.xlabel('Batch Number')\n",
    "plt.show()\n",
    "print('Labels  ',np.array(labels_presented)[0:grange].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grange=np.int(num_stored*.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(graph_points),num_stored,graph_points[num_stored])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#unzoomed figure\n",
    "print('first',graph_points[grange],'plots')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "#plt.plot(graph_points[0:grange],third_animal_OM_learn_Vacc[0:grange], label='OM Accuracy NEW')\n",
    "#plt.plot(graph_points[0:grange],third_animal_tf_learn_Vacc[0:grange], label='TF Accuracy NEW')\n",
    "\n",
    "plt.plot(graph_points[0:(all_presented+1)],OM_learn_Vacc[0:(all_presented+1)],color='darkblue')\n",
    "plt.plot(graph_points[all_presented:grange],OM_learn_Vacc[all_presented:grange],color='cornflowerblue', label='OM')\n",
    "plt.plot(graph_points[0:grange],tf_learn_Vacc[0:grange],color='orange', label='TF')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "plt.ylim([0.4,1.0])\n",
    "\n",
    "plt.title(\"OM vs TF {} {} {} trained total\".format(dataset_name,MODEL,graph_points[grange]))\n",
    "#plt.xticks(np.arange(0,graph_points[num_stored]+1,int(graph_points[num_stored]/10)))  # Set label locations.\n",
    "plt.xticks(np.arange(0,graph_points[grange]+1,int(graph_points[grange]/10)))  # Set label locations.\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Batch Number')\n",
    "plt.show()\n",
    "print('Labels  ',np.array(labels_presented)[0:grange].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
