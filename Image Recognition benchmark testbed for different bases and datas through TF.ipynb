{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow import keras\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now=datetime.now()\n",
    "date_time=now.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01-22-2023'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.2\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE =1 #32   #1\n",
    "IMG_SIZE = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I left this because it sets up the directories automatically  \n",
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tsvi\\.keras\\datasets\\fowl_data\n"
     ]
    }
   ],
   "source": [
    "# Different datasets to benchmark\n",
    "#dataset_name='Animals_filtered'  # 50 animal dataset\n",
    "dataset_name='cats_and_dogs_filtered'  #originalTF notebook dataset \"full\"\n",
    "#dataset_name='fowl_data' # from https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-pytorch\n",
    "\n",
    "PATH2 = os.path.join(os.path.dirname(path_to_zip),dataset_name )    \n",
    "# also put in original\n",
    "print(PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tsvi\\.keras\\datasets\\fowl_data \n",
      " C:\\Users\\Tsvi\\.keras\\datasets\\fowl_data\\train \n",
      " C:\\Users\\Tsvi\\.keras\\datasets\\fowl_data\\validation\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(PATH2, 'train')\n",
    "validation_dir = os.path.join(PATH2, 'validation')\n",
    "\n",
    "#print(third_animal_dataset_path,'\\n',third_animal_train_dir,'\\n',third_animal_validation_dir)\n",
    "print(PATH2,'\\n',train_dir,'\\n',validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 230 files belonging to 2 classes.\n",
      "Found 250 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                                         shuffle=True,\n",
    "                                                                         batch_size=BATCH_SIZE,\n",
    "                                                                         image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
    "                                                                              shuffle=True,\n",
    "                                                                              batch_size=BATCH_SIZE,\n",
    "                                                                              image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class names: ['chickens', 'turkeys']\n"
     ]
    }
   ],
   "source": [
    "#print(\"original dataset class names:\", train_dataset.class_names)\n",
    "print(\"Dataset class names:\", train_dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes and outputs: 2\n"
     ]
    }
   ],
   "source": [
    "class_names = train_dataset.class_names\n",
    "num_outputs=len(class_names)\n",
    "print('number of classes and outputs:',num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation batches: 200\n",
      "Number of test batches: 50\n"
     ]
    }
   ],
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 230\n"
     ]
    }
   ],
   "source": [
    "print('Number of training batches: %d' % tf.data.experimental.cardinality(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training the model 10 epochs\n",
    "initial_epochs = 1 #10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "#base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE, include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ResNEt=True #False\n",
    "# MODEL = 'MobileNetV2'\n",
    "MODEL = 'ResNet50'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL=='MobileNetV2':\n",
    "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "elif MODEL=='ResNet50':\n",
    "    preprocess_input = tf.keras.applications.resnet50.preprocess_input\n",
    "    base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "else:\n",
    "    error(\"Model Not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 160, 160, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 166, 166, 3)  0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 80, 80, 64)   9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 80, 80, 64)   256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 80, 80, 64)   0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 82, 82, 64)   0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 40, 40, 64)   0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 40, 40, 64)   4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 40, 40, 64)   256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 40, 40, 64)   0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 40, 40, 64)   36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 40, 40, 64)   256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 40, 40, 64)   0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 40, 40, 256)  16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 40, 40, 256)  16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 40, 40, 256)  1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 40, 40, 256)  1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 40, 40, 256)  0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 40, 40, 256)  0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 40, 40, 64)   16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 40, 40, 64)   256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 40, 40, 64)   0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 40, 40, 64)   36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 40, 40, 64)   256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 40, 40, 64)   0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 40, 40, 256)  16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 40, 40, 256)  1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 40, 40, 256)  0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 40, 40, 256)  0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 40, 40, 64)   16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 40, 40, 64)   256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 40, 40, 64)   0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 40, 40, 64)   36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 40, 40, 64)   256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 40, 40, 64)   0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 40, 40, 256)  16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 40, 40, 256)  1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 40, 40, 256)  0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 40, 40, 256)  0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 20, 20, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 20, 20, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 20, 20, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 20, 20, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 20, 20, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 20, 20, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 20, 20, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 20, 20, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 20, 20, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 20, 20, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 20, 20, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 20, 20, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 20, 20, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 20, 20, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 20, 20, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 20, 20, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 20, 20, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 20, 20, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 20, 20, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 20, 20, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 20, 20, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 20, 20, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 20, 20, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 20, 20, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 20, 20, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 20, 20, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 20, 20, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 20, 20, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 20, 20, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 20, 20, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 20, 20, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 20, 20, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 20, 20, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 20, 20, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 20, 20, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 20, 20, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 20, 20, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 20, 20, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 20, 20, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 20, 20, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 20, 20, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 20, 20, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 10, 10, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 10, 10, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 10, 10, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 10, 10, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 10, 10, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 10, 10, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 10, 10, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 10, 10, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 10, 10, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 10, 10, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 10, 10, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 10, 10, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 10, 10, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 10, 10, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 10, 10, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 10, 10, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 10, 10, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 10, 10, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 10, 10, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 10, 10, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 10, 10, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 10, 10, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 10, 10, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 10, 10, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 10, 10, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 10, 10, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 10, 10, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 10, 10, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 10, 10, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 10, 10, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 10, 10, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 5, 5, 512)    524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 5, 5, 512)    0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 5, 5, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 5, 5, 512)    0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 5, 5, 2048)   2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 5, 5, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 5, 5, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 5, 5, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 5, 5, 2048)   0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 5, 5, 2048)   0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 5, 5, 512)    1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 5, 5, 512)    0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 5, 5, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 5, 5, 512)    0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 5, 5, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 5, 5, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 5, 5, 2048)   0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 5, 5, 2048)   0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 5, 5, 512)    1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 5, 5, 512)    0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 5, 5, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 5, 5, 512)    2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 5, 5, 512)    0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 5, 5, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 5, 5, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 5, 5, 2048)   0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 5, 5, 2048)   0           conv5_block3_add[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,587,712\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original adds 2d global_average_pooling2d layer to make the base layer ready for classification.  \n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now I set up traditional learning and top layer, but I change from binary to multiclass so I must add softmax to top\n",
    "final_output_layer=tf.keras.layers.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the multiclass equivalent to original transfer learning network  \n",
    "prediction_layer4 = tf.keras.layers.Dense(num_outputs)  # change1 from original: I make num_outputs outputs\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = prediction_layer4(x)    # adds the 49 nodes\n",
    "outputs = final_output_layer(x)  # change2: adds softmax\n",
    "orig_TF_paradigm_model = tf.keras.Model(inputs, outputs)   # this is now the traditional leanring network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 3:SparseCategoricalCrossentropy from BinaryCrossentropy because it is multiclass\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "orig_TF_paradigm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True,) # Now I compile it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 160, 160, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.__operators__.getitem (Sl (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.nn.bias_add (TFOpLambda)  (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, 5, 5, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 4098      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "orig_TF_paradigm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I create the version of the base model that will feed inputs to OM layer and I call it model_with_av_layer\n",
    "# it does not have anything after the global_average_layer\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "outputs = global_average_layer(x)\n",
    "#outputs = prediction_layer(x)   # It is basically the same as the original notebook but no prediction layer\n",
    "model_with_av_layer = tf.keras.Model(inputs, outputs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 160, 160, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.__operators__.getitem_1 ( (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.nn.bias_add_1 (TFOpLambda (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, 5, 5, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "=================================================================\n",
      "Total params: 23,587,712\n",
      "Trainable params: 0\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_with_av_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up OM model that will take inputs from the 1280 outputs of model_with_av_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OM_Model_learn import OM_CORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version: OM core 0.27 minimzie from 5/12 3/21/2022 brought back stuff from 14 RFN conv\n"
     ]
    }
   ],
   "source": [
    "num_inputs=model_with_av_layer.output.shape[1]  #1280\n",
    "\n",
    "OMmodel = OM_CORE(num_inputs, num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OMmodel.update()   #in case it is run or tested un-initiated  \n",
    "OMmodel.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an equivalent TF model that will recieve the OM weights\n",
    "prediction_layer2 = tf.keras.layers.Dense(num_outputs)  # top nodes\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = prediction_layer2(x)  # nodes whose weights I will be changed by om\n",
    "outputs = final_output_layer(x)  #softmax\n",
    "tf_model_tobe_trained_by_OM = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it needs compile if I do validation but I never really do any learning so most of this doesnt really matter\n",
    "base_learning_rate = 0.0001\n",
    "tf_model_tobe_trained_by_OM.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True,) # Efi: needs to be added to prevent optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 160, 160, 3)]     0         \n",
      "_________________________________________________________________\n",
      "sequential (Sequential)      (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.__operators__.getitem_2 ( (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "tf.nn.bias_add_2 (TFOpLambda (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "resnet50 (Functional)        (None, 5, 5, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d (Gl (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 4098      \n",
      "_________________________________________________________________\n",
      "softmax (Softmax)            (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 23,591,810\n",
      "Trainable params: 4,098\n",
      "Non-trainable params: 23,587,712\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf_model_tobe_trained_by_OM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so I dont validate at every point for graphing (otherwise things take forever)\n",
    "validate_points=np.concatenate([np.arange(30),np.arange(30,40,2),np.arange(40,60,3),np.arange(60,100,5),np.arange(100,300,7),np.arange(300,700,10),np.arange(700,1500,15),np.arange(1500,3000,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "248"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(validate_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start re-run here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "# find the number of entries in thrid_animal_validation set: there doesnt seem to be an adequate internal function to do this!\n",
    "# Im sure there is a better way but whatever\n",
    "num_data_in_validation_dataset=0\n",
    "for data, label in validation_dataset: #.as_numpy_iterator():\n",
    "    num_data_in_validation_dataset=num_data_in_validation_dataset+len(data)\n",
    "print(num_data_in_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup validation manually so I dont have to go through base for every validation (saves time because validation is the same)\n",
    "i=0\n",
    "validation_labels=np.ones((1,num_data_in_validation_dataset))*50  #crazy initial values for debug\n",
    "net_out_validation_for_OM=np.ones((num_data_in_validation_dataset,num_inputs))*50  \n",
    "\n",
    "for data, label in validation_dataset: #.as_numpy_iterator():\n",
    "    #print(\"Batch\",i,\"out of\",num_data_in_validation_dataset,\"size:\",len(data),len(label))\n",
    "    len_batch=len(label)\n",
    "    #print(label)\n",
    "    validation_labels[0,(BATCH_SIZE*i):(BATCH_SIZE*(i)+len_batch)]=label  # set up validation truth table\n",
    "    temp=model_with_av_layer.predict(data)  #run bottom transfer layers pre-rfn\n",
    "    net_out_validation_for_OM[BATCH_SIZE*i:(BATCH_SIZE*i+len_batch),:]=temp[0:len_batch,:]\n",
    "    i=i+1\n",
    "validation_labels=validation_labels.astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 1 0 1 1\n",
      " 0 1 0 0 0 0 0 1 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 0 1 1 1 1 0 0 1 1 1 1 0 1\n",
      " 0 1 1 1 1 0 1 1 0 0 0 0 1 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 0 1 0 0 0 1 0 1 0 0 1 0 1 0 0 0 0 1 0 1 0 0 1 0 0 0 1 1 0 1 1 0 1 1\n",
      " 1 0 0 1 0 0 1 0 1 1 1 1 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 1 1\n",
      " 1 0 0 0 0 0 1 1 0 1 0 1 1 1 0]\n"
     ]
    }
   ],
   "source": [
    "print(validation_labels[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data of train batches in a single epoch 230 batch size = 1 epochs= 1\n"
     ]
    }
   ],
   "source": [
    "# for batches\n",
    "num_batches_in_train_dataset=len(train_dataset)              # number of batches in train_dataset\n",
    "print(\"num of data of train batches in a single epoch\",num_batches_in_train_dataset,\"batch size =\",BATCH_SIZE,\"epochs=\",initial_epochs)\n",
    "OM_learn_Vacc=[] #0.5*np.ones(num_batches_in_train_dataset*initial_epochs)    #initializing as .5 just in case zeros gets through somehow  #np.zeros(num_batches_in_train_dataset)\n",
    "tf_learn_Vacc=[] #0.5*np.ones(num_batches_in_train_dataset*initial_epochs)    #np.zeros(num_batches_in_train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_presented=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_learn_Vacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store baselines before learn\n",
    "store_baselines_before_learning=False\n",
    "if store_baselines_before_learning:\n",
    "    loss_, accuracy_ = orig_TF_paradigm_model.evaluate(validation_dataset)\n",
    "    print('validation of TF before train on new',accuracy_)\n",
    "    tf_learn_Vacc.append(accuracy_) #store data\n",
    "\n",
    "    OMmodel.test_data_score(net_out_validation_for_OM,validation_labels[0,:])  #validate for OM\n",
    "    print('validation of OM on NEW data before train on new',OMmodel.last_score_accuracy/100)\n",
    "    OM_learn_Vacc.append(OMmodel.last_score_accuracy/100) \n",
    "    \n",
    "    # I want to add one more zero point and increase the index of everything by 1 to test before learning\n",
    "    graph_points=list(validate_points.copy()+1)\n",
    "    graph_points.insert(0,0)\n",
    "else:\n",
    "    graph_points=list(validate_points.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [0] , previously [0. 0.] of each.\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 0.7687 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 96 104 200 ) %Correct:  48.0 %\n",
      "False Positives by label: [104.   0.]\n",
      "False Negatives by label: [  0. 104.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 0, Epoch 0, 1 entries accuracy of OM core 0.48 and TF 0.52 \n",
      " Learned [1] of label(s) [1] , previously [1. 0.] of each.\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.7698 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 127 73 200 ) %Correct:  63.5 %\n",
      "False Positives by label: [13. 60.]\n",
      "False Negatives by label: [60. 13.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 1, Epoch 0, 1 entries accuracy of OM core 0.635 and TF 0.52 \n",
      " Learned [1] of label(s) [1] , previously [1. 1.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7744 - accuracy: 0.5150\n",
      " (Number correct, number incorrect, number of tests): ( 106 94 200 ) %Correct:  53.0 %\n",
      "False Positives by label: [ 0. 94.]\n",
      "False Negatives by label: [94.  0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 2, Epoch 0, 1 entries accuracy of OM core 0.53 and TF 0.515 \n",
      " Learned [1] of label(s) [1] , previously [1. 2.] of each.\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.7801 - accuracy: 0.5050\n",
      " (Number correct, number incorrect, number of tests): ( 104 96 200 ) %Correct:  52.0 %\n",
      "False Positives by label: [ 0. 96.]\n",
      "False Negatives by label: [96.  0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 3, Epoch 0, 1 entries accuracy of OM core 0.52 and TF 0.505 \n",
      " Learned [1] of label(s) [1] , previously [1. 3.] of each.\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.7678 - accuracy: 0.5300\n",
      " (Number correct, number incorrect, number of tests): ( 104 96 200 ) %Correct:  52.0 %\n",
      "False Positives by label: [ 0. 96.]\n",
      "False Negatives by label: [96.  0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 4, Epoch 0, 1 entries accuracy of OM core 0.52 and TF 0.53 \n",
      " Learned [1] of label(s) [1] , previously [1. 4.] of each.\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 0.7774 - accuracy: 0.5050\n",
      " (Number correct, number incorrect, number of tests): ( 104 96 200 ) %Correct:  52.0 %\n",
      "False Positives by label: [ 0. 96.]\n",
      "False Negatives by label: [96.  0.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 5, Epoch 0, 1 entries accuracy of OM core 0.52 and TF 0.505 \n",
      " Learned [1] of label(s) [0] , previously [1. 5.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.7781 - accuracy: 0.5050\n",
      " (Number correct, number incorrect, number of tests): ( 114 86 200 ) %Correct:  57.0 %\n",
      "False Positives by label: [ 1. 85.]\n",
      "False Negatives by label: [85.  1.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 6, Epoch 0, 1 entries accuracy of OM core 0.57 and TF 0.505 \n",
      " Learned [1] of label(s) [0] , previously [2. 5.] of each.\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 0.7767 - accuracy: 0.5050\n",
      " (Number correct, number incorrect, number of tests): ( 140 60 200 ) %Correct:  70.0 %\n",
      "False Positives by label: [ 4. 56.]\n",
      "False Negatives by label: [56.  4.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 7, Epoch 0, 1 entries accuracy of OM core 0.7 and TF 0.505 \n",
      " Learned [1] of label(s) [1] , previously [3. 5.] of each.\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.7768 - accuracy: 0.5050\n",
      " (Number correct, number incorrect, number of tests): ( 136 64 200 ) %Correct:  68.0 %\n",
      "False Positives by label: [ 3. 61.]\n",
      "False Negatives by label: [61.  3.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 8, Epoch 0, 1 entries accuracy of OM core 0.68 and TF 0.505 \n",
      " Learned [1] of label(s) [1] , previously [3. 6.] of each.\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 0.7807 - accuracy: 0.5000\n",
      " (Number correct, number incorrect, number of tests): ( 127 73 200 ) %Correct:  63.5 %\n",
      "False Positives by label: [ 3. 70.]\n",
      "False Negatives by label: [70.  3.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 9, Epoch 0, 1 entries accuracy of OM core 0.635 and TF 0.5 \n",
      " Learned [1] of label(s) [0] , previously [3. 7.] of each.\n",
      "200/200 [==============================] - 20s 97ms/step - loss: 0.7670 - accuracy: 0.5150\n",
      " (Number correct, number incorrect, number of tests): ( 143 57 200 ) %Correct:  71.5 %\n",
      "False Positives by label: [ 5. 52.]\n",
      "False Negatives by label: [52.  5.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 10, Epoch 0, 1 entries accuracy of OM core 0.715 and TF 0.515 \n",
      " Learned [1] of label(s) [1] , previously [4. 7.] of each.\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 0.7841 - accuracy: 0.4950\n",
      " (Number correct, number incorrect, number of tests): ( 143 57 200 ) %Correct:  71.5 %\n",
      "False Positives by label: [ 4. 53.]\n",
      "False Negatives by label: [53.  4.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 11, Epoch 0, 1 entries accuracy of OM core 0.715 and TF 0.495 \n",
      " Learned [1] of label(s) [1] , previously [4. 8.] of each.\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.7756 - accuracy: 0.5050\n",
      " (Number correct, number incorrect, number of tests): ( 134 66 200 ) %Correct:  67.0 %\n",
      "False Positives by label: [ 4. 62.]\n",
      "False Negatives by label: [62.  4.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 12, Epoch 0, 1 entries accuracy of OM core 0.67 and TF 0.505 \n",
      " Learned [1] of label(s) [0] , previously [4. 9.] of each.\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 0.7746 - accuracy: 0.5050\n",
      " (Number correct, number incorrect, number of tests): ( 148 52 200 ) %Correct:  74.0 %\n",
      "False Positives by label: [ 8. 44.]\n",
      "False Negatives by label: [44.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 13, Epoch 0, 1 entries accuracy of OM core 0.74 and TF 0.505 \n",
      " Learned [1] of label(s) [0] , previously [5. 9.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7691 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 162 38 200 ) %Correct:  81.0 %\n",
      "False Positives by label: [ 9. 29.]\n",
      "False Negatives by label: [29.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 14, Epoch 0, 1 entries accuracy of OM core 0.81 and TF 0.52 \n",
      " Learned [1] of label(s) [1] , previously [6. 9.] of each.\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.7686 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 163 37 200 ) %Correct:  81.5 %\n",
      "False Positives by label: [ 9. 28.]\n",
      "False Negatives by label: [28.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 15, Epoch 0, 1 entries accuracy of OM core 0.815 and TF 0.52 \n",
      " Learned [1] of label(s) [1] , previously [ 6. 10.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7679 - accuracy: 0.52000s - loss: 0.7679 - accuracy: 0.52\n",
      " (Number correct, number incorrect, number of tests): ( 161 39 200 ) %Correct:  80.5 %\n",
      "False Positives by label: [ 9. 30.]\n",
      "False Negatives by label: [30.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 16, Epoch 0, 1 entries accuracy of OM core 0.805 and TF 0.52 \n",
      " Learned [1] of label(s) [0] , previously [ 6. 11.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.7702 - accuracy: 0.5150\n",
      " (Number correct, number incorrect, number of tests): ( 165 35 200 ) %Correct:  82.5 %\n",
      "False Positives by label: [10. 25.]\n",
      "False Negatives by label: [25. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 17, Epoch 0, 1 entries accuracy of OM core 0.825 and TF 0.515 \n",
      " Learned [1] of label(s) [0] , previously [ 7. 11.] of each.\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 0.7672 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 169 31 200 ) %Correct:  84.5 %\n",
      "False Positives by label: [13. 18.]\n",
      "False Negatives by label: [18. 13.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 18, Epoch 0, 1 entries accuracy of OM core 0.845 and TF 0.52 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [1] , previously [ 8. 11.] of each.\n",
      "200/200 [==============================] - 19s 97ms/step - loss: 0.7770 - accuracy: 0.5100\n",
      " (Number correct, number incorrect, number of tests): ( 168 32 200 ) %Correct:  84.0 %\n",
      "False Positives by label: [13. 19.]\n",
      "False Negatives by label: [19. 13.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 19, Epoch 0, 1 entries accuracy of OM core 0.84 and TF 0.51 \n",
      " Learned [1] of label(s) [1] , previously [ 8. 12.] of each.\n",
      "200/200 [==============================] - 23s 114ms/step - loss: 0.7716 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 167 33 200 ) %Correct:  83.5 %\n",
      "False Positives by label: [15. 18.]\n",
      "False Negatives by label: [18. 15.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 20, Epoch 0, 1 entries accuracy of OM core 0.835 and TF 0.52 \n",
      " Learned [1] of label(s) [1] , previously [ 8. 13.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.7713 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 167 33 200 ) %Correct:  83.5 %\n",
      "False Positives by label: [15. 18.]\n",
      "False Negatives by label: [18. 15.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 21, Epoch 0, 1 entries accuracy of OM core 0.835 and TF 0.52 \n",
      " Learned [1] of label(s) [1] , previously [ 8. 14.] of each.\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.7713 - accuracy: 0.5150\n",
      " (Number correct, number incorrect, number of tests): ( 170 30 200 ) %Correct:  85.0 %\n",
      "False Positives by label: [11. 19.]\n",
      "False Negatives by label: [19. 11.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 22, Epoch 0, 1 entries accuracy of OM core 0.85 and TF 0.515 \n",
      " Learned [1] of label(s) [1] , previously [ 8. 15.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7745 - accuracy: 0.5100\n",
      " (Number correct, number incorrect, number of tests): ( 170 30 200 ) %Correct:  85.0 %\n",
      "False Positives by label: [12. 18.]\n",
      "False Negatives by label: [18. 12.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 23, Epoch 0, 1 entries accuracy of OM core 0.85 and TF 0.51 \n",
      " Learned [1] of label(s) [1] , previously [ 8. 16.] of each.\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.7662 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 169 31 200 ) %Correct:  84.5 %\n",
      "False Positives by label: [12. 19.]\n",
      "False Negatives by label: [19. 12.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 24, Epoch 0, 1 entries accuracy of OM core 0.845 and TF 0.52 \n",
      " Learned [1] of label(s) [0] , previously [ 8. 17.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7751 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 173 27 200 ) %Correct:  86.5 %\n",
      "False Positives by label: [ 9. 18.]\n",
      "False Negatives by label: [18.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 25, Epoch 0, 1 entries accuracy of OM core 0.865 and TF 0.52 \n",
      " Learned [1] of label(s) [0] , previously [ 9. 17.] of each.\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.7648 - accuracy: 0.5300\n",
      " (Number correct, number incorrect, number of tests): ( 172 28 200 ) %Correct:  86.0 %\n",
      "False Positives by label: [11. 17.]\n",
      "False Negatives by label: [17. 11.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 26, Epoch 0, 1 entries accuracy of OM core 0.86 and TF 0.53 \n",
      " Learned [1] of label(s) [1] , previously [10. 17.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.7738 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 172 28 200 ) %Correct:  86.0 %\n",
      "False Positives by label: [10. 18.]\n",
      "False Negatives by label: [18. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 27, Epoch 0, 1 entries accuracy of OM core 0.86 and TF 0.52 \n",
      " Learned [1] of label(s) [1] , previously [10. 18.] of each.\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 0.7684 - accuracy: 0.5250\n",
      " (Number correct, number incorrect, number of tests): ( 173 27 200 ) %Correct:  86.5 %\n",
      "False Positives by label: [ 9. 18.]\n",
      "False Negatives by label: [18.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 28, Epoch 0, 1 entries accuracy of OM core 0.865 and TF 0.525 \n",
      " Learned [1] of label(s) [0] , previously [10. 19.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7630 - accuracy: 0.5300\n",
      " (Number correct, number incorrect, number of tests): ( 175 25 200 ) %Correct:  87.5 %\n",
      "False Positives by label: [ 9. 16.]\n",
      "False Negatives by label: [16.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 29, Epoch 0, 1 entries accuracy of OM core 0.875 and TF 0.53 \n",
      " Learned [1] of label(s) [1] , previously [11. 19.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7631 - accuracy: 0.5300\n",
      " (Number correct, number incorrect, number of tests): ( 173 27 200 ) %Correct:  86.5 %\n",
      "False Positives by label: [10. 17.]\n",
      "False Negatives by label: [17. 10.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 30, Epoch 0, 1 entries accuracy of OM core 0.865 and TF 0.53 \n",
      " Learned [1] of label(s) [1] , previously [11. 20.] of each.\n",
      " Learned [1] of label(s) [0] , previously [11. 21.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7533 - accuracy: 0.5300\n",
      " (Number correct, number incorrect, number of tests): ( 174 26 200 ) %Correct:  87.0 %\n",
      "False Positives by label: [12. 14.]\n",
      "False Negatives by label: [14. 12.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 32, Epoch 0, 1 entries accuracy of OM core 0.87 and TF 0.53 \n",
      " Learned [1] of label(s) [1] , previously [12. 21.] of each.\n",
      " Learned [1] of label(s) [0] , previously [12. 22.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7593 - accuracy: 0.5250\n",
      " (Number correct, number incorrect, number of tests): ( 173 27 200 ) %Correct:  86.5 %\n",
      "False Positives by label: [14. 13.]\n",
      "False Negatives by label: [13. 14.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 34, Epoch 0, 1 entries accuracy of OM core 0.865 and TF 0.525 \n",
      " Learned [1] of label(s) [1] , previously [13. 22.] of each.\n",
      " Learned [1] of label(s) [1] , previously [13. 23.] of each.\n",
      "200/200 [==============================] - 22s 109ms/step - loss: 0.7584 - accuracy: 0.5250\n",
      " (Number correct, number incorrect, number of tests): ( 170 30 200 ) %Correct:  85.0 %\n",
      "False Positives by label: [13. 17.]\n",
      "False Negatives by label: [17. 13.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 36, Epoch 0, 1 entries accuracy of OM core 0.85 and TF 0.525 \n",
      " Learned [1] of label(s) [1] , previously [13. 24.] of each.\n",
      " Learned [1] of label(s) [1] , previously [13. 25.] of each.\n",
      "200/200 [==============================] - 22s 111ms/step - loss: 0.7625 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 171 29 200 ) %Correct:  85.5 %\n",
      "False Positives by label: [12. 17.]\n",
      "False Negatives by label: [17. 12.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 38, Epoch 0, 1 entries accuracy of OM core 0.855 and TF 0.52 \n",
      " Learned [1] of label(s) [0] , previously [13. 26.] of each.\n",
      " Learned [1] of label(s) [0] , previously [14. 26.] of each.\n",
      "200/200 [==============================] - 21s 105ms/step - loss: 0.7594 - accuracy: 0.5300\n",
      " (Number correct, number incorrect, number of tests): ( 170 30 200 ) %Correct:  85.0 %\n",
      "False Positives by label: [13. 17.]\n",
      "False Negatives by label: [17. 13.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 40, Epoch 0, 1 entries accuracy of OM core 0.85 and TF 0.53 \n",
      " Learned [1] of label(s) [1] , previously [15. 26.] of each.\n",
      " Learned [1] of label(s) [1] , previously [15. 27.] of each.\n",
      " Learned [1] of label(s) [1] , previously [15. 28.] of each.\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.7651 - accuracy: 0.5250\n",
      " (Number correct, number incorrect, number of tests): ( 169 31 200 ) %Correct:  84.5 %\n",
      "False Positives by label: [13. 18.]\n",
      "False Negatives by label: [18. 13.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 43, Epoch 0, 1 entries accuracy of OM core 0.845 and TF 0.525 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [0] , previously [15. 29.] of each.\n",
      " Learned [1] of label(s) [1] , previously [16. 29.] of each.\n",
      " Learned [1] of label(s) [1] , previously [16. 30.] of each.\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.7641 - accuracy: 0.5250\n",
      " (Number correct, number incorrect, number of tests): ( 169 31 200 ) %Correct:  84.5 %\n",
      "False Positives by label: [14. 17.]\n",
      "False Negatives by label: [17. 14.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 46, Epoch 0, 1 entries accuracy of OM core 0.845 and TF 0.525 \n",
      " Learned [1] of label(s) [1] , previously [16. 31.] of each.\n",
      " Learned [1] of label(s) [0] , previously [16. 32.] of each.\n",
      " Learned [1] of label(s) [0] , previously [17. 32.] of each.\n",
      "200/200 [==============================] - 20s 98ms/step - loss: 0.7593 - accuracy: 0.5300\n",
      " (Number correct, number incorrect, number of tests): ( 169 31 200 ) %Correct:  84.5 %\n",
      "False Positives by label: [15. 16.]\n",
      "False Negatives by label: [16. 15.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 49, Epoch 0, 1 entries accuracy of OM core 0.845 and TF 0.53 \n",
      " Learned [1] of label(s) [1] , previously [18. 32.] of each.\n",
      " Learned [1] of label(s) [1] , previously [18. 33.] of each.\n",
      " Learned [1] of label(s) [1] , previously [18. 34.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7612 - accuracy: 0.5250\n",
      " (Number correct, number incorrect, number of tests): ( 171 29 200 ) %Correct:  85.5 %\n",
      "False Positives by label: [14. 15.]\n",
      "False Negatives by label: [15. 14.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 52, Epoch 0, 1 entries accuracy of OM core 0.855 and TF 0.525 \n",
      " Learned [1] of label(s) [1] , previously [18. 35.] of each.\n",
      " Learned [1] of label(s) [0] , previously [18. 36.] of each.\n",
      " Learned [1] of label(s) [0] , previously [19. 36.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.7555 - accuracy: 0.5250\n",
      " (Number correct, number incorrect, number of tests): ( 170 30 200 ) %Correct:  85.0 %\n",
      "False Positives by label: [15. 15.]\n",
      "False Negatives by label: [15. 15.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 55, Epoch 0, 1 entries accuracy of OM core 0.85 and TF 0.525 \n",
      " Learned [1] of label(s) [1] , previously [20. 36.] of each.\n",
      " Learned [1] of label(s) [1] , previously [20. 37.] of each.\n",
      " Learned [1] of label(s) [0] , previously [20. 38.] of each.\n",
      "200/200 [==============================] - 21s 106ms/step - loss: 0.7569 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 171 29 200 ) %Correct:  85.5 %\n",
      "False Positives by label: [15. 14.]\n",
      "False Negatives by label: [14. 15.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 58, Epoch 0, 1 entries accuracy of OM core 0.855 and TF 0.52 \n",
      " Learned [1] of label(s) [1] , previously [21. 38.] of each.\n",
      " Learned [1] of label(s) [1] , previously [21. 39.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.7581 - accuracy: 0.5200\n",
      " (Number correct, number incorrect, number of tests): ( 172 28 200 ) %Correct:  86.0 %\n",
      "False Positives by label: [13. 15.]\n",
      "False Negatives by label: [15. 13.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 60, Epoch 0, 1 entries accuracy of OM core 0.86 and TF 0.52 \n",
      " Learned [1] of label(s) [0] , previously [21. 40.] of each.\n",
      " Learned [1] of label(s) [0] , previously [22. 40.] of each.\n",
      " Learned [1] of label(s) [0] , previously [23. 40.] of each.\n",
      " Learned [1] of label(s) [1] , previously [24. 40.] of each.\n",
      " Learned [1] of label(s) [0] , previously [24. 41.] of each.\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.7447 - accuracy: 0.5350\n",
      " (Number correct, number incorrect, number of tests): ( 171 29 200 ) %Correct:  85.5 %\n",
      "False Positives by label: [14. 15.]\n",
      "False Negatives by label: [15. 14.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 65, Epoch 0, 1 entries accuracy of OM core 0.855 and TF 0.535 \n",
      " Learned [1] of label(s) [0] , previously [25. 41.] of each.\n",
      " Learned [1] of label(s) [0] , previously [26. 41.] of each.\n",
      " Learned [1] of label(s) [1] , previously [27. 41.] of each.\n",
      " Learned [1] of label(s) [0] , previously [27. 42.] of each.\n",
      " Learned [1] of label(s) [1] , previously [28. 42.] of each.\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 0.7432 - accuracy: 0.5250\n",
      " (Number correct, number incorrect, number of tests): ( 174 26 200 ) %Correct:  87.0 %\n",
      "False Positives by label: [11. 15.]\n",
      "False Negatives by label: [15. 11.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 70, Epoch 0, 1 entries accuracy of OM core 0.87 and TF 0.525 \n",
      " Learned [1] of label(s) [1] , previously [28. 43.] of each.\n",
      " Learned [1] of label(s) [1] , previously [28. 44.] of each.\n",
      " Learned [1] of label(s) [0] , previously [28. 45.] of each.\n",
      " Learned [1] of label(s) [1] , previously [29. 45.] of each.\n",
      " Learned [1] of label(s) [0] , previously [29. 46.] of each.\n",
      "200/200 [==============================] - 21s 102ms/step - loss: 0.7394 - accuracy: 0.5250\n",
      " (Number correct, number incorrect, number of tests): ( 172 28 200 ) %Correct:  86.0 %\n",
      "False Positives by label: [12. 16.]\n",
      "False Negatives by label: [16. 12.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 75, Epoch 0, 1 entries accuracy of OM core 0.86 and TF 0.525 \n",
      " Learned [1] of label(s) [1] , previously [30. 46.] of each.\n",
      " Learned [1] of label(s) [0] , previously [30. 47.] of each.\n",
      " Learned [1] of label(s) [0] , previously [31. 47.] of each.\n",
      " Learned [1] of label(s) [0] , previously [32. 47.] of each.\n",
      " Learned [1] of label(s) [1] , previously [33. 47.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.7365 - accuracy: 0.5250\n",
      " (Number correct, number incorrect, number of tests): ( 172 28 200 ) %Correct:  86.0 %\n",
      "False Positives by label: [12. 16.]\n",
      "False Negatives by label: [16. 12.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 80, Epoch 0, 1 entries accuracy of OM core 0.86 and TF 0.525 \n",
      " Learned [1] of label(s) [1] , previously [33. 48.] of each.\n",
      " Learned [1] of label(s) [1] , previously [33. 49.] of each.\n",
      " Learned [1] of label(s) [1] , previously [33. 50.] of each.\n",
      " Learned [1] of label(s) [1] , previously [33. 51.] of each.\n",
      " Learned [1] of label(s) [1] , previously [33. 52.] of each.\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.7243 - accuracy: 0.5350\n",
      " (Number correct, number incorrect, number of tests): ( 174 26 200 ) %Correct:  87.0 %\n",
      "False Positives by label: [11. 15.]\n",
      "False Negatives by label: [15. 11.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 85, Epoch 0, 1 entries accuracy of OM core 0.87 and TF 0.535 \n",
      " Learned [1] of label(s) [1] , previously [33. 53.] of each.\n",
      " Learned [1] of label(s) [1] , previously [33. 54.] of each.\n",
      " Learned [1] of label(s) [0] , previously [33. 55.] of each.\n",
      " Learned [1] of label(s) [0] , previously [34. 55.] of each.\n",
      " Learned [1] of label(s) [0] , previously [35. 55.] of each.\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.7243 - accuracy: 0.5350\n",
      " (Number correct, number incorrect, number of tests): ( 174 26 200 ) %Correct:  87.0 %\n",
      "False Positives by label: [ 9. 17.]\n",
      "False Negatives by label: [17.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 90, Epoch 0, 1 entries accuracy of OM core 0.87 and TF 0.535 \n",
      " Learned [1] of label(s) [0] , previously [36. 55.] of each.\n",
      " Learned [1] of label(s) [0] , previously [37. 55.] of each.\n",
      " Learned [1] of label(s) [0] , previously [38. 55.] of each.\n",
      " Learned [1] of label(s) [0] , previously [39. 55.] of each.\n",
      " Learned [1] of label(s) [1] , previously [40. 55.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.7215 - accuracy: 0.5450\n",
      " (Number correct, number incorrect, number of tests): ( 177 23 200 ) %Correct:  88.5 %\n",
      "False Positives by label: [ 7. 16.]\n",
      "False Negatives by label: [16.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 95, Epoch 0, 1 entries accuracy of OM core 0.885 and TF 0.545 \n",
      " Learned [1] of label(s) [1] , previously [40. 56.] of each.\n",
      " Learned [1] of label(s) [1] , previously [40. 57.] of each.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Learned [1] of label(s) [1] , previously [40. 58.] of each.\n",
      " Learned [1] of label(s) [0] , previously [40. 59.] of each.\n",
      " Learned [1] of label(s) [1] , previously [41. 59.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.7161 - accuracy: 0.5500\n",
      " (Number correct, number incorrect, number of tests): ( 176 24 200 ) %Correct:  88.0 %\n",
      "False Positives by label: [ 8. 16.]\n",
      "False Negatives by label: [16.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 100, Epoch 0, 1 entries accuracy of OM core 0.88 and TF 0.55 \n",
      " Learned [1] of label(s) [1] , previously [41. 60.] of each.\n",
      " Learned [1] of label(s) [1] , previously [41. 61.] of each.\n",
      " Learned [1] of label(s) [0] , previously [41. 62.] of each.\n",
      " Learned [1] of label(s) [0] , previously [42. 62.] of each.\n",
      " Learned [1] of label(s) [0] , previously [43. 62.] of each.\n",
      " Learned [1] of label(s) [0] , previously [44. 62.] of each.\n",
      " Learned [1] of label(s) [1] , previously [45. 62.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.7085 - accuracy: 0.5650\n",
      " (Number correct, number incorrect, number of tests): ( 178 22 200 ) %Correct:  89.0 %\n",
      "False Positives by label: [ 7. 15.]\n",
      "False Negatives by label: [15.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 107, Epoch 0, 1 entries accuracy of OM core 0.89 and TF 0.565 \n",
      " Learned [1] of label(s) [0] , previously [45. 63.] of each.\n",
      " Learned [1] of label(s) [0] , previously [46. 63.] of each.\n",
      " Learned [1] of label(s) [1] , previously [47. 63.] of each.\n",
      " Learned [1] of label(s) [1] , previously [47. 64.] of each.\n",
      " Learned [1] of label(s) [1] , previously [47. 65.] of each.\n",
      " Learned [1] of label(s) [1] , previously [47. 66.] of each.\n",
      " Learned [1] of label(s) [1] , previously [47. 67.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.6956 - accuracy: 0.5700\n",
      " (Number correct, number incorrect, number of tests): ( 178 22 200 ) %Correct:  89.0 %\n",
      "False Positives by label: [ 7. 15.]\n",
      "False Negatives by label: [15.  7.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 114, Epoch 0, 1 entries accuracy of OM core 0.89 and TF 0.57 \n",
      " Learned [1] of label(s) [1] , previously [47. 68.] of each.\n",
      " Learned [1] of label(s) [0] , previously [47. 69.] of each.\n",
      " Learned [1] of label(s) [0] , previously [48. 69.] of each.\n",
      " Learned [1] of label(s) [1] , previously [49. 69.] of each.\n",
      " Learned [1] of label(s) [1] , previously [49. 70.] of each.\n",
      " Learned [1] of label(s) [1] , previously [49. 71.] of each.\n",
      " Learned [1] of label(s) [0] , previously [49. 72.] of each.\n",
      "200/200 [==============================] - 21s 104ms/step - loss: 0.6860 - accuracy: 0.6000\n",
      " (Number correct, number incorrect, number of tests): ( 176 24 200 ) %Correct:  88.0 %\n",
      "False Positives by label: [ 8. 16.]\n",
      "False Negatives by label: [16.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 121, Epoch 0, 1 entries accuracy of OM core 0.88 and TF 0.6 \n",
      " Learned [1] of label(s) [1] , previously [50. 72.] of each.\n",
      " Learned [1] of label(s) [0] , previously [50. 73.] of each.\n",
      " Learned [1] of label(s) [1] , previously [51. 73.] of each.\n",
      " Learned [1] of label(s) [1] , previously [51. 74.] of each.\n",
      " Learned [1] of label(s) [1] , previously [51. 75.] of each.\n",
      " Learned [1] of label(s) [0] , previously [51. 76.] of each.\n",
      " Learned [1] of label(s) [0] , previously [52. 76.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.6875 - accuracy: 0.5950\n",
      " (Number correct, number incorrect, number of tests): ( 174 26 200 ) %Correct:  87.0 %\n",
      "False Positives by label: [ 9. 17.]\n",
      "False Negatives by label: [17.  9.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 128, Epoch 0, 1 entries accuracy of OM core 0.87 and TF 0.595 \n",
      " Learned [1] of label(s) [0] , previously [53. 76.] of each.\n",
      " Learned [1] of label(s) [0] , previously [54. 76.] of each.\n",
      " Learned [1] of label(s) [0] , previously [55. 76.] of each.\n",
      " Learned [1] of label(s) [0] , previously [56. 76.] of each.\n",
      " Learned [1] of label(s) [0] , previously [57. 76.] of each.\n",
      " Learned [1] of label(s) [1] , previously [58. 76.] of each.\n",
      " Learned [1] of label(s) [1] , previously [58. 77.] of each.\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.6805 - accuracy: 0.6050\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 6. 15.]\n",
      "False Negatives by label: [15.  6.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 135, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.605 \n",
      " Learned [1] of label(s) [0] , previously [58. 78.] of each.\n",
      " Learned [1] of label(s) [1] , previously [59. 78.] of each.\n",
      " Learned [1] of label(s) [0] , previously [59. 79.] of each.\n",
      " Learned [1] of label(s) [0] , previously [60. 79.] of each.\n",
      " Learned [1] of label(s) [0] , previously [61. 79.] of each.\n",
      " Learned [1] of label(s) [1] , previously [62. 79.] of each.\n",
      " Learned [1] of label(s) [0] , previously [62. 80.] of each.\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.6626 - accuracy: 0.6100\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 6. 15.]\n",
      "False Negatives by label: [15.  6.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 142, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.61 \n",
      " Learned [1] of label(s) [1] , previously [63. 80.] of each.\n",
      " Learned [1] of label(s) [1] , previously [63. 81.] of each.\n",
      " Learned [1] of label(s) [0] , previously [63. 82.] of each.\n",
      " Learned [1] of label(s) [1] , previously [64. 82.] of each.\n",
      " Learned [1] of label(s) [0] , previously [64. 83.] of each.\n",
      " Learned [1] of label(s) [1] , previously [65. 83.] of each.\n",
      " Learned [1] of label(s) [1] , previously [65. 84.] of each.\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 0.6692 - accuracy: 0.6050\n",
      " (Number correct, number incorrect, number of tests): ( 180 20 200 ) %Correct:  90.0 %\n",
      "False Positives by label: [ 6. 14.]\n",
      "False Negatives by label: [14.  6.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 149, Epoch 0, 1 entries accuracy of OM core 0.9 and TF 0.605 \n",
      " Learned [1] of label(s) [1] , previously [65. 85.] of each.\n",
      " Learned [1] of label(s) [0] , previously [65. 86.] of each.\n",
      " Learned [1] of label(s) [0] , previously [66. 86.] of each.\n",
      " Learned [1] of label(s) [1] , previously [67. 86.] of each.\n",
      " Learned [1] of label(s) [1] , previously [67. 87.] of each.\n",
      " Learned [1] of label(s) [0] , previously [67. 88.] of each.\n",
      " Learned [1] of label(s) [1] , previously [68. 88.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.6621 - accuracy: 0.6050\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 8. 13.]\n",
      "False Negatives by label: [13.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 156, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.605 \n",
      " Learned [1] of label(s) [0] , previously [68. 89.] of each.\n",
      " Learned [1] of label(s) [1] , previously [69. 89.] of each.\n",
      " Learned [1] of label(s) [0] , previously [69. 90.] of each.\n",
      " Learned [1] of label(s) [1] , previously [70. 90.] of each.\n",
      " Learned [1] of label(s) [1] , previously [70. 91.] of each.\n",
      " Learned [1] of label(s) [1] , previously [70. 92.] of each.\n",
      " Learned [1] of label(s) [0] , previously [70. 93.] of each.\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.6516 - accuracy: 0.6200\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 8. 13.]\n",
      "False Negatives by label: [13.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 163, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.62 \n",
      " Learned [1] of label(s) [0] , previously [71. 93.] of each.\n",
      " Learned [1] of label(s) [0] , previously [72. 93.] of each.\n",
      " Learned [1] of label(s) [0] , previously [73. 93.] of each.\n",
      " Learned [1] of label(s) [1] , previously [74. 93.] of each.\n",
      " Learned [1] of label(s) [0] , previously [74. 94.] of each.\n",
      " Learned [1] of label(s) [0] , previously [75. 94.] of each.\n",
      " Learned [1] of label(s) [1] , previously [76. 94.] of each.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 20s 98ms/step - loss: 0.6520 - accuracy: 0.6350\n",
      " (Number correct, number incorrect, number of tests): ( 178 22 200 ) %Correct:  89.0 %\n",
      "False Positives by label: [ 8. 14.]\n",
      "False Negatives by label: [14.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 170, Epoch 0, 1 entries accuracy of OM core 0.89 and TF 0.635 \n",
      " Learned [1] of label(s) [0] , previously [76. 95.] of each.\n",
      " Learned [1] of label(s) [1] , previously [77. 95.] of each.\n",
      " Learned [1] of label(s) [0] , previously [77. 96.] of each.\n",
      " Learned [1] of label(s) [0] , previously [78. 96.] of each.\n",
      " Learned [1] of label(s) [0] , previously [79. 96.] of each.\n",
      " Learned [1] of label(s) [1] , previously [80. 96.] of each.\n",
      " Learned [1] of label(s) [0] , previously [80. 97.] of each.\n",
      "200/200 [==============================] - 20s 101ms/step - loss: 0.6513 - accuracy: 0.6200\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 8. 13.]\n",
      "False Negatives by label: [13.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 177, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.62 \n",
      " Learned [1] of label(s) [0] , previously [81. 97.] of each.\n",
      " Learned [1] of label(s) [1] , previously [82. 97.] of each.\n",
      " Learned [1] of label(s) [1] , previously [82. 98.] of each.\n",
      " Learned [1] of label(s) [1] , previously [82. 99.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 82. 100.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 82. 101.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 83. 101.] of each.\n",
      "200/200 [==============================] - 21s 102ms/step - loss: 0.6493 - accuracy: 0.6150\n",
      " (Number correct, number incorrect, number of tests): ( 180 20 200 ) %Correct:  90.0 %\n",
      "False Positives by label: [ 8. 12.]\n",
      "False Negatives by label: [12.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 184, Epoch 0, 1 entries accuracy of OM core 0.9 and TF 0.615 \n",
      " Learned [1] of label(s) [0] , previously [ 84. 101.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 85. 101.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 86. 101.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 86. 102.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 86. 103.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 87. 103.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 87. 104.] of each.\n",
      "200/200 [==============================] - 20s 100ms/step - loss: 0.6494 - accuracy: 0.6250\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 8. 13.]\n",
      "False Negatives by label: [13.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 191, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.625 \n",
      " Learned [1] of label(s) [1] , previously [ 87. 105.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 87. 106.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 88. 106.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 88. 107.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 88. 108.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 88. 109.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 88. 110.] of each.\n",
      "200/200 [==============================] - 20s 102ms/step - loss: 0.6418 - accuracy: 0.6250\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 8. 13.]\n",
      "False Negatives by label: [13.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 198, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.625 \n",
      " Learned [1] of label(s) [1] , previously [ 88. 111.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 88. 112.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 88. 113.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 89. 113.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 90. 113.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 91. 113.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 92. 113.] of each.\n",
      "200/200 [==============================] - 20s 99ms/step - loss: 0.6280 - accuracy: 0.6550\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 8. 13.]\n",
      "False Negatives by label: [13.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 205, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.655 \n",
      " Learned [1] of label(s) [1] , previously [ 93. 113.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 93. 114.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 94. 114.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 95. 114.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 96. 114.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 97. 114.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 97. 115.] of each.\n",
      "200/200 [==============================] - 21s 107ms/step - loss: 0.6258 - accuracy: 0.6500\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 8. 13.]\n",
      "False Negatives by label: [13.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 212, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.65 \n",
      " Learned [1] of label(s) [1] , previously [ 97. 116.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 97. 117.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 98. 117.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 98. 118.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 98. 119.] of each.\n",
      " Learned [1] of label(s) [1] , previously [ 99. 119.] of each.\n",
      " Learned [1] of label(s) [0] , previously [ 99. 120.] of each.\n",
      "200/200 [==============================] - 21s 103ms/step - loss: 0.6240 - accuracy: 0.6550\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 8. 13.]\n",
      "False Negatives by label: [13.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 219, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.655 \n",
      " Learned [1] of label(s) [0] , previously [100. 120.] of each.\n",
      " Learned [1] of label(s) [0] , previously [101. 120.] of each.\n",
      " Learned [1] of label(s) [1] , previously [102. 120.] of each.\n",
      " Learned [1] of label(s) [1] , previously [102. 121.] of each.\n",
      " Learned [1] of label(s) [1] , previously [102. 122.] of each.\n",
      " Learned [1] of label(s) [1] , previously [102. 123.] of each.\n",
      " Learned [1] of label(s) [0] , previously [102. 124.] of each.\n",
      "200/200 [==============================] - 22s 110ms/step - loss: 0.6252 - accuracy: 0.6550\n",
      " (Number correct, number incorrect, number of tests): ( 179 21 200 ) %Correct:  89.5 %\n",
      "False Positives by label: [ 8. 13.]\n",
      "False Negatives by label: [13.  8.]\n",
      "See   .last_score_accuracy for % accuracy value\n",
      "Training: Batch 226, Epoch 0, 1 entries accuracy of OM core 0.895 and TF 0.655 \n",
      " Learned [1] of label(s) [1] , previously [103. 124.] of each.\n",
      " Learned [1] of label(s) [0] , previously [103. 125.] of each.\n",
      " Learned [1] of label(s) [1] , previously [104. 125.] of each.\n"
     ]
    }
   ],
   "source": [
    "for j in range(initial_epochs):\n",
    "    if j>0:  #update graph points if doing more than one epoch\n",
    "        validate_points=np.concatenate([np.arange(0,3000,30)])\n",
    "        graph_points[num_stored:]=[]\n",
    "        graph_points=graph_points+list(validate_points.copy()+len(labels_presented)+1)   #need to figure out graphing stuff\n",
    "\n",
    "    for i, data in train_dataset.enumerate():  #seems similar to fit \n",
    "        \n",
    "        labels_presented.append(data[1])  # store record of labels presented\n",
    "        \n",
    "        #t = time.time()\n",
    "        \n",
    "        #the original learning        \n",
    "        batch_logs = orig_TF_paradigm_model.train_step(data)\n",
    "        \n",
    "        #TF_time=time.time() - t\n",
    "\n",
    "        \n",
    "        #t = time.time()      # I include time to percolate data through base.  \n",
    "        \n",
    "        # prepare data to send out to OM.        \n",
    "        net_out = model_with_av_layer.predict_on_batch(data[0])  # run through base layer to have data ready for OM\n",
    "        \n",
    "        # call to the API\n",
    "        OMmodel.learn(net_out,data[1])  # needs data post base and label\n",
    "        \n",
    "        #OM_time=time.time() - t\n",
    "        \n",
    "        #print('Learning Time  OM:',np.round(OM_time,4),'  TF:',np.round(TF_time,4),'  TF/OM=',np.round(TF_time/OM_time,4),'X')\n",
    "        \n",
    "        # this would be equivalent to a callback from the API getting back the weights from the API\n",
    "        # the TF structure recieves the weights that come back put into the trainable layer: OMmodel.ff_model\n",
    "        tf_model_tobe_trained_by_OM.trainable_weights[0].assign(tf.Variable(np.float32(OMmodel.ff_model.T))) \n",
    "        # this is now the TF equivalent of OM learned net \n",
    "        \n",
    "        #this is code to validate but it is repeated in the validated_points\n",
    "        #loss0, accuracy0 = tf_model_tobe_trained_by_OM.evaluate(validation_dataset)  # it can be validated and run just like the original\n",
    "        \n",
    "        \n",
    "        if i in validate_points:  # minimizing number of validations because it takes too long\n",
    "        \n",
    "            # validating TF learned top layer\n",
    "            loss_, accuracy_ = orig_TF_paradigm_model.evaluate(validation_dataset)\n",
    "\n",
    "            # validating directly with OM, This function is not needed in the API, just the tf_model_tobe_trained_by_OM code (a few down)\n",
    "            OMmodel.test_data_score(net_out_validation_for_OM,validation_labels[0,:])  #,verbose=True) \n",
    "                \n",
    "            # adding TF accuracies to the record\n",
    "            tf_learn_Vacc.append(accuracy_)  \n",
    "            \n",
    "            # adding OM  accuracies to the records\n",
    "            OM_learn_Vacc.append(OMmodel.last_score_accuracy/100) # using direct validation\n",
    "\n",
    "            if False:  # extra sanity validation to show OMTF == OM core but extra tests take too much time if you are training the net\n",
    "            \n",
    "                # validating OM learned top layer but equivalent net to TF\n",
    "                loss0, accuracy_OMTF = tf_model_tobe_trained_by_OM.evaluate(validation_dataset)  # it is validated and run just like the original TF\n",
    "                #OM_learn_Vacc.append(accuracy_OMTF)\n",
    "\n",
    "                print(\"Training: Batch {}, Epoch {}, {} entries accuracy of OM core {} OMTF {} (should be roughly equivalent to OM) and TF {} \".format(i,j, len(data[0]), OM_learn_Vacc[-1],accuracy_OMTF,accuracy_))\n",
    "            \n",
    "            else:\n",
    "                print(\"Training: Batch {}, Epoch {}, {} entries accuracy of OM core {} and TF {} \".format(i,j, len(data[0]), np.round(OM_learn_Vacc[-1],4),np.round(accuracy_,4)))\n",
    "\n",
    "        \n",
    "    \n",
    "    num_stored=len(OM_learn_Vacc)\n",
    "    \n",
    "    if False:  # epoch plot not implemented in this version\n",
    "        OM_epoch_Vacc_old[j]=sum(OM_learn_Vacc_old[(j*num_batches_in_train_dataset):((j+1)*num_batches_in_train_dataset)])/num_batches_in_train_dataset\n",
    "        tf_epoch_Vacc_old[j]=sum(tf_learn_Vacc_old[(j*num_batches_in_train_dataset):((j+1)*num_batches_in_train_dataset)])/num_batches_in_train_dataset\n",
    "        print(\"Epoch {} summing {} batches to get epoch accuracy of OM on new {} old {} and TF on new {} old {} data\".format(j,i,OM_epoch_Vacc[j],OM_epoch_Vacc_old[j],tf_learn_Vacc[j],tf_epoch_Vacc_old[j]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.48,\n",
       " 0.635,\n",
       " 0.53,\n",
       " 0.52,\n",
       " 0.52,\n",
       " 0.52,\n",
       " 0.57,\n",
       " 0.7,\n",
       " 0.68,\n",
       " 0.635,\n",
       " 0.715,\n",
       " 0.715,\n",
       " 0.67,\n",
       " 0.74,\n",
       " 0.81,\n",
       " 0.815,\n",
       " 0.805,\n",
       " 0.825,\n",
       " 0.845,\n",
       " 0.84,\n",
       " 0.835,\n",
       " 0.835,\n",
       " 0.85,\n",
       " 0.85,\n",
       " 0.845,\n",
       " 0.865,\n",
       " 0.86,\n",
       " 0.86,\n",
       " 0.865,\n",
       " 0.875,\n",
       " 0.865,\n",
       " 0.87,\n",
       " 0.865,\n",
       " 0.85,\n",
       " 0.855,\n",
       " 0.85,\n",
       " 0.845,\n",
       " 0.845,\n",
       " 0.845,\n",
       " 0.855,\n",
       " 0.85,\n",
       " 0.855,\n",
       " 0.86,\n",
       " 0.855,\n",
       " 0.87,\n",
       " 0.86,\n",
       " 0.86,\n",
       " 0.87,\n",
       " 0.87,\n",
       " 0.885,\n",
       " 0.88,\n",
       " 0.89,\n",
       " 0.89,\n",
       " 0.88,\n",
       " 0.87,\n",
       " 0.895,\n",
       " 0.895,\n",
       " 0.9,\n",
       " 0.895,\n",
       " 0.895,\n",
       " 0.89,\n",
       " 0.895,\n",
       " 0.9,\n",
       " 0.895,\n",
       " 0.895,\n",
       " 0.895,\n",
       " 0.895,\n",
       " 0.895,\n",
       " 0.895]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OM_learn_Vacc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first validation run after all animals were presented: 2 \n",
      "which begins with training instance 2\n"
     ]
    }
   ],
   "source": [
    "# indicate when all animals are presented at least once\n",
    "all_presented=1\n",
    "while len(np.unique(labels_presented[0:all_presented])) != len(np.unique(labels_presented)):\n",
    "    all_presented=all_presented+1\n",
    "all_presented=np.where(np.array(graph_points) >= all_presented)[0][0]   #first index where all presented\n",
    "print('first validation run after all animals were presented:',all_presented,'\\nwhich begins with training instance',graph_points[all_presented])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "grange=int(graph_points[num_stored]*.02)     #90  #31 #90 #121  # choosing a nice zoom range\n",
    "if grange==0:\n",
    "    grange=num_stored\n",
    "maxpoint=max(graph_points[0:grange])\n",
    "print(maxpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 4 plots\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels   [[0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# zoomed figure\n",
    "print('first',graph_points[grange],'plots')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(graph_points[0:(all_presented+1)],OM_learn_Vacc[0:(all_presented+1)],color='darkblue')\n",
    "plt.plot(graph_points[all_presented:grange],OM_learn_Vacc[all_presented:grange],color='cornflowerblue', label='OM Accuracy NEW')\n",
    "plt.plot(graph_points[0:grange],tf_learn_Vacc[0:grange],color='orange', label='TF Accuracy NEW')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "plt.ylim([0.0,1.0])\n",
    "plt.title(\"OM vs TF {} Accuracy within {} Batch={} Example(s) Trained\".format(dataset_name,graph_points[grange],BATCH_SIZE))\n",
    "\n",
    "plt.xlabel('Batch Number')\n",
    "plt.show()\n",
    "print('Labels  ',np.array(labels_presented)[0:grange].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "grange=num_stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2980 69 233\n"
     ]
    }
   ],
   "source": [
    "print(max(graph_points),num_stored,graph_points[num_stored])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 233 plots\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels   [[0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 1 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 0 1 0 1\n",
      "  1 1 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 1 0 0 1 1 0 1 1 0 0 0 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "#unzoomed figure\n",
    "print('first',graph_points[grange],'plots')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "#plt.plot(graph_points[0:grange],third_animal_OM_learn_Vacc[0:grange], label='OM Accuracy NEW')\n",
    "#plt.plot(graph_points[0:grange],third_animal_tf_learn_Vacc[0:grange], label='TF Accuracy NEW')\n",
    "\n",
    "plt.plot(graph_points[0:(all_presented+1)],OM_learn_Vacc[0:(all_presented+1)],color='darkblue')\n",
    "plt.plot(graph_points[all_presented:grange],OM_learn_Vacc[all_presented:grange],color='cornflowerblue', label='OM')\n",
    "plt.plot(graph_points[0:grange],tf_learn_Vacc[0:grange],color='orange', label='TF')\n",
    "\n",
    "\n",
    "\n",
    "plt.legend(loc='lower right')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "#plt.ylim([min(plt.ylim()),1])\n",
    "plt.ylim([0.0,1.0])\n",
    "#if ResNEt:\n",
    "#    fname=\"ResNet50\"\n",
    "#else:\n",
    "#    fname=\"MobileNetV2\"\n",
    "\n",
    "plt.title(\"OM vs TF {} {} {} trained total\".format(dataset_name,MODEL,graph_points[grange]))\n",
    "plt.xticks(np.arange(0,graph_points[num_stored]+1,int(graph_points[num_stored]/10)))  # Set label locations.\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Batch Number')\n",
    "plt.show()\n",
    "print('Labels  ',np.array(labels_presented)[0:grange].T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2048)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OMmodel.rfn_weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if ResNEt:\n",
    "#    fname=\"ResNet50_\"+dataset_name\n",
    "#else:\n",
    "#    fname=\"MobileNetV2_\"+dataset_name\n",
    "\n",
    "fname=MODEL+\"_\"+dataset_name\n",
    "np.save(fname, OMmodel.ff_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'dense/kernel:0' shape=(2048, 2) dtype=float32, numpy=\n",
       "array([[ 0.04259898,  0.01962377],\n",
       "       [ 0.0391567 , -0.05411763],\n",
       "       [ 0.000989  ,  0.00372347],\n",
       "       ...,\n",
       "       [-0.0380608 ,  0.03631028],\n",
       "       [-0.0396256 , -0.03165526],\n",
       "       [-0.0215494 , -0.04053424]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_TF_paradigm_model.trainable_weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(fname+'_TF',orig_TF_paradigm_model.trainable_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ResNet50_fowl_data'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.47937422e-05,  2.76027945e-04],\n",
       "       [ 1.96002288e-04,  2.27025607e-04],\n",
       "       [ 7.43085384e-04, -3.08711919e-04],\n",
       "       ...,\n",
       "       [-1.07867107e-04,  3.31566142e-04],\n",
       "       [ 1.43827987e-03, -8.69746551e-04],\n",
       "       [-6.13972461e-05,  2.62284258e-04]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.load(fname+'.npy').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-65-a308a35d4a30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstop\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mhere\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop-here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=np.where(OMmodel.rfn_weights[0]==OMmodel.rfn_weights[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import isclose\n",
    "num_feat=len(OMmodel.rfn_weights[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "close_array=np.zeros(num_feat)\n",
    "for k in range(num_feat):\n",
    "    close_array[k]=isclose(OMmodel.rfn_weights[0,k],OMmodel.rfn_weights[1,k], abs_tol=1e-3)\n",
    "np.sum(close_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.cond(OMmodel.rfn_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.layers[1].get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save accuracies and TF model for future analysis if needed\n",
    "np.save(\"TF trained using {} batch={} on {}.npy\".format(dataset_name,BATCH_SIZE,date_time),tf_learn_Vacc)\n",
    "np.save(\"OM trained using {} batch={} on {}.npy\".format(dataset_name,BATCH_SIZE,date_time),OM_learn_Vacc)\n",
    "orig_TF_paradigm_model.save(\"orig_TF_paradigm_model trained using {} on {}\".format(dataset_name,date_time))  # automate date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import logging\n",
    "#import json\n",
    "#from keras_sequential_ascii import sequential_model_to_ascii_printout\n",
    "#import scipy.linalg as scialg\n",
    "#import sympy as sy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathfilename = 'wsfile/' + 'explain_for_DeGirum.' + 'lg'  # temp, have much betterin in file omnnbu_no_json\n",
    "#logging.info('pathfilename <> ' + pathfilename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#explain_ff_components(np.array([]),base_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathfilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanity_report_brief(W,verbose=False):\n",
    "    #print(\"Report on filters without changing anything.\")\n",
    "    #logging.info(\"Report on filters without changing anything.\".format())\n",
    "\n",
    "    Wsum_min=1e-4  #cancels flter\n",
    "    Wsingle_branch_min=1e-6  #cancels branch\n",
    "    \n",
    "\n",
    "    h_filter, w_filter, numim,  n_filters = W.shape  #n_filters is the same as outputs, prints one way shapes another WTF\n",
    "    #numim, n_filters, h_filter, w_filter = W.shape  #wtf figuring order by trial and error\n",
    "    #num_inputs=layer_weights[0].shape[2]  #I believe if I recall correctly... previously: prev.shape[3]  #inputs\n",
    "    #num_outputs=layer_weights[0].shape[3]   #outs\n",
    "    #filter_h=layer_weights[0].shape[0]   #\n",
    "    #filter_w=layer_weights[0].shape[1]   #\n",
    "    \n",
    "    #print('hilter size:',h_filter*w_filter,\"W shape\",W.shape,'WTF why switch n_filters:',n_filters,' numim:',numim)\n",
    "    \n",
    "\n",
    "\n",
    "    #BWW=np.zeros_like(W)  #dont need for report\n",
    "    status=np.ones((n_filters,numim))*h_filter*w_filter   #ishape+2*pad,ishape+2*pad))\n",
    "    remove_input=0\n",
    "    remove_output=0\n",
    "    cancel=0\n",
    "    modify=0\n",
    "    for n in range(n_filters):\n",
    "        for nxt in range(numim):\n",
    "            abs=np.abs(W[:,:,nxt,n])\n",
    "            #abs=np.abs(W[n,nxt,:,:]) also need modify\n",
    "            if np.sum(abs)<1e-4: \n",
    "                #print(\"total sum of filter \",n,nxt,\"is less than 1e-4. Prone to error. Zeroing filter\")\n",
    "                #W[n,nxt,:,:]=0\n",
    "                #BWW[n,nxt,:,:]=0\n",
    "                status[n,nxt]=0\n",
    "                cancel=cancel+1\n",
    "            else:   # everything is zero dont need to go on\n",
    "                if np.min(abs)<Wsingle_branch_min:\n",
    "                    modify=modify+1\n",
    "                    count=0\n",
    "                    for i in range(h_filter):\n",
    "                        for j in range(w_filter):\n",
    "                            if W[i,j,nxt,n]<Wsingle_branch_min:\n",
    "                                #W[n,nxt,:,:]=0\n",
    "                                count=count+1  # number unused in a filter\n",
    "                    if verbose: print(\"Only\",h_filter*w_filter-count,\"/\",h_filter*w_filter,\"weights from Inp\",nxt,\"to Out\",n)\n",
    "                    status[n,nxt]=status[n,nxt]-count  # record number of unused in filter\n",
    "                    #BWW[n,nxt,:,:]=np.linalg.inv(W[n,nxt,:,:]).T\n",
    "                    #print(\"BWW:\") # not for report\n",
    "                    #print(BWW[n,nxt,:,:])\n",
    "                    #print(\"filter:\")\n",
    "                    #print(W[n,nxt,:,:])\n",
    "                #else:\n",
    "                    #BWW[n,nxt,:,:]=np.linalg.inv(W[n,nxt,:,:]).T   #pinv or inv doesnt seem to matter via eyeball comparison\n",
    "        if np.sum(status[n,:])==0 :\n",
    "            if verbose: print(n,\"Output(s) have no input connections at all!\")  #No I to O connections:\n",
    "            remove_output=remove_output+1\n",
    "        else:\n",
    "            if np.sum(status[n,:])<numim :\n",
    "                if verbose: print(\"Used: O\",n,\" by I=\",int(np.sum(status[n,:])),\" out of \",numim)#,status[n,:])\n",
    "    for nxt in range(numim):\n",
    "        if np.sum(status[:,nxt])==0 :\n",
    "            if verbose: print(\"Input field \",nxt,\" no Outputs\")#, status[:,nxt])\n",
    "            remove_input=remove_input+1\n",
    "        else:\n",
    "            if np.sum(status[:,nxt])<n_filters :\n",
    "                if verbose: print(\"I\",nxt,\"O=\",int(np.sum(status[:,nxt])),\"/\",n_filters) #,status[:,nxt])  #logging.info(\"I {} O= {} / {} \".format(nxt,int(np.sum(status[:,nxt])),n_filters))  #\n",
    "    #logging.info\n",
    "    print(\"{} inputs and {} outputs are completely unused: no part of filter gives information.\".format(remove_input,remove_output))\n",
    "    #logging.info\n",
    "    print(\"\\\"unused\\\": {} filters out of {} and {} leftover individual connections.\".format(cancel,n_filters*numim,modify))\n",
    "    #print(\"Consider removing\",cancel,\"connections out of \",n_filters*numim,\"total. Partially modifying\",modify,\"connections\")\n",
    "    #return BWW/h_filter #,W,status\n",
    "    \n",
    "    #print('layer filter connection matrix between inpus and outputs: ',status)\n",
    "\n",
    "    return [remove_input,remove_output,cancel,modify]  #unused: inputs, outputs, filters, individual connections (leftover after filters)\n",
    "\n",
    "def sanity_base(model,verbose=False,start=0,convert=False,Linalg_anal=False):  #method='FFT'\n",
    "\n",
    "    for count,layer in enumerate(model.layers,start=start):\n",
    "        layer_weights=layer.get_weights()\n",
    "        if layer_weights==[]:\n",
    "            config_info=layer.get_config()\n",
    "            #logging.info(\"\\nLayer {} {} has no weights input {} output {}\".format(count,layer.name,prev.shape,output.shape))\n",
    "\n",
    "            print('\\nLayer',count,layer.name,'has no weights') #,'input',prev.shape,'output',output.shape)\n",
    "\n",
    "        else:\n",
    "            print('\\nLayer',count,layer.name,'weights',layer_weights[0].shape)\n",
    "            #logging.info(\"\\nLayer {} {} weights {} input {} output {}\".format(count,layer.name,layer_weights[0].shape,prev.shape,output.shape))  #bias {} #,layer_weights[1].shape\n",
    "\n",
    "            lnl=\"L{}\".format(count)\n",
    "\n",
    "            if len(layer_weights[0].shape)==4:    # 4D         4D               4D        4D          4D\n",
    "                #print('Convolution layer')\n",
    "                #for this application: last two entries are inputs and outputs\n",
    "                num_inputs=layer_weights[0].shape[2]  #I believe if I recall correctly... previously: prev.shape[3]  #inputs\n",
    "                num_outputs=layer_weights[0].shape[3]   #outs\n",
    "                filter_h=layer_weights[0].shape[0]   #\n",
    "                filter_w=layer_weights[0].shape[1]   #\n",
    "\n",
    "                commonname=\"L{}.{}\".format(count,layer.name)\n",
    "                #params[\"dense\"]=False\n",
    "                if Linalg_anal: report_algebra_state(num_inputs*filter_h*filter_w,num_outputs,lnl)\n",
    "                if convert:\n",
    "                    print('converting')\n",
    "                    use_report=sanity_report_brief(np.linalg.pinv(layer_weights[0]))\n",
    "                else:\n",
    "                    use_report=sanity_report_brief(layer_weights[0])\n",
    "                #unused: inputs, outputs, filters, individual connections (leftover after filters)\n",
    "                #print(commonname,layer_weights[0].shape,\n",
    "                print('Unsed: ',use_report)\n",
    "\n",
    "\n",
    "\n",
    "                #print('layer shape',layer_weights[0].shape,'transpose shape',np.transpose(layer_weights[0],(3,2,0,1)).shape)\n",
    "                #logging.info(\"layer shape {} transpose shape {}\".format(layer_weights[0].shape,np.transpose(layer_weights[0],(3,2,0,1)).shape))\n",
    "\n",
    "\n",
    "\n",
    "                #print('layer dilation',layer.dilation_rate[0],layer.strides[0],layer.padding)\n",
    "                # used to be 4d graph_ignored_percent here\n",
    "                #logging.info(\"layer dilation {} {} {}\".format(layer.dilation_rate[0],layer.strides[0],layer.padding))\n",
    "\n",
    "                #index_for_baselines+=1\n",
    "\n",
    "            if len(layer_weights[0].shape)==2:  # Fully Connected    2D    2D    2D   2D\n",
    "                #print('dense layer')\n",
    "                commonname=\"L{}.{}\".format(count,layer.name)\n",
    "                #params[\"dense\"]=True\n",
    "                \n",
    "                if convert:\n",
    "                    W=np.linalg.pinv(layer_weights[0]).T\n",
    "                else:\n",
    "                    W=layer_weights[0]\n",
    "                #params['w']=W\n",
    "                num_inputs,num_outputs=W.shape\n",
    "                if Linalg_anal: report_algebra_state(num_inputs,num_outputs,lnl)\n",
    "\n",
    "                num_feat=len(OMmodel.rfn_weights[0,:])\n",
    "                close_array=np.zeros(num_inputs)\n",
    "                for k in range(num_inputs):\n",
    "                    close_array[k]=isclose(OMmodel.rfn_weights[0,k],OMmodel.rfn_weights[1,k], abs_tol=1e-3)\n",
    "                np.sum(close_array)\n",
    "                \n",
    "                \n",
    "                # convert weights\n",
    "                try:\n",
    "                    cond=np.linalg.cond(W.T)   # this catch execption doesnt work... for example I put nans in matrix there is error but does not except\n",
    "                except:\n",
    "                    #print('Analysis not available')\n",
    "                    logging.info(\"Analysis not available\".format())\n",
    "\n",
    "                if cond>10: logging.info(\"Network has neurons that are poorly distinguished from each other\\nTwo or more outputs may have the same responses {}\".format(np.round(np.linalg.cond(W),2)))\n",
    "                print('Network has neurons that are poorly distinguished from each other\\nTwo or more outputs may have the same responses',np.round(np.linalg.cond(W),2))\n",
    "                mrank=np.linalg.matrix_rank(W.T)\n",
    "                #print('Rank',mrank,'cond',np.round(cond,2))\n",
    "                #logging.info(\"Rank {} cond {}\".format(mrank,np.round(cond,2)))\n",
    "\n",
    "                #params[\"layer_weights\"]=(W)\n",
    "    print(\"Complete\")\n",
    "\n",
    "                \n",
    "                \n",
    "def report_algebra_state(num_outputs,num_inputs,lnl):\n",
    "    commonname=lnl\n",
    "    if num_outputs<num_inputs:\n",
    "        #params[\"description\"]=('Multiple inputs may give the same solution') #'Under-Determined'\n",
    "        commonname=commonname+'U'\n",
    "        explicit=np.arange(0,num_outputs)\n",
    "        implicit=num_outputs-num_inputs #np.arange(num_outputs,num_inputs)\n",
    "        #logging.info(\"{} ignores certain inputs and may have the same output for many inputs \".format(lnl))\n",
    "\n",
    "        print(lnl,'ignores certain inputs and may have the same output for many inputs')\n",
    "\n",
    "    elif num_outputs>num_inputs:   # less likely in convolution\n",
    "        #params[\"description\"]=('Multiple outputs may depend on the same inputs') #'Over-Determined'\n",
    "        commonname=commonname+'O'\n",
    "\n",
    "        print(lnl,'is Over-determined, inputs are 100% analyzed but the outputs must follow the following rules')\n",
    "        #logging.info(\"{}: Inputs are 100% analyzed but the outputs must follow the following rules\".format(lnl))\n",
    "\n",
    "\n",
    "        explicit=np.arange(0,num_outputs)\n",
    "        implicit=False  #np.arange(num_outputs,num_inputs)\n",
    "\n",
    "    elif num_outputs==num_inputs:\n",
    "        #params[\"description\"]=('Each input leads to a single output') #'Determined'\n",
    "        print(lnl,'is completely determined: one input X possible for each output where inputs are 100% analyzed')\n",
    "        #logging.info(\"{}: one input X leads to one output and inputs are 100% analyzed\".format(lnl))\n",
    "\n",
    "        explicit=np.arange(0,num_outputs)\n",
    "        implicit=False  #np.arange(num_outputs,num_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanity_base(base_model,convert=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1: 7 outputs unused out of 32\n",
    "# layer 7: 7 inputs out of 32\n",
    "# 98: 1 out of 576\n",
    "# layer after also 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
