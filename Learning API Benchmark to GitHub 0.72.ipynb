{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright Optimizing Mind 2023\n",
    "Optimizing Mind Jupyter notebook for benchmarking transfer learning.\n",
    "This code compares transfer learning for cats and dogs and is based on the tesor flow tutorial and code.\n",
    "The code: \n",
    "1) initiates the Optimizing Mind API \n",
    "2) downloads cats dogs data \n",
    "3) runs the tensor flow example learning code\n",
    "4) runs OM API learning on the exact same data samples as in (3)\n",
    "5) plots verification and learning curves\n",
    "\n",
    "Optimizing Mind User Guide for API https://docs.google.com/document/d/1gW8HNtol__A8LChqSs5Q_3wJsFv6A10QCgW0FUH5ZxE\n",
    "Steps to run Optimizing Mind API\n",
    "1) Register and obtain API token https://om-learn-api.azurewebsites.net/\n",
    "2) copy your token and place in \\<token\\>\n",
    "\n",
    "Changes from original TF tutorial code:\n",
    "1) top layer changed from binary to multiclass to allow for multiple labels (no penalty in performance)\n",
    "2) batch size is changed from the original 32 to 1 in order to be able to better observe OM learning curve (no penalty in performance)\n",
    "3) TF training loop code changed to a manual one (for loop instead of fit)\n",
    "4) options added for different datasets\n",
    "5) options added for different bases\n",
    "original code: https://storage.googleapis.com/tensorflow_docs/docs/site/en/tutorials/images/transfer_learning.ipynb\n",
    "\n",
    "Note code is slow because lots of validation is run on both models, but it produces detailed learning curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow import keras\n",
    "import time\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_inference_shortcut=True\n",
    "# BATCH_SIZE=1 is best to see OM progress because it learns by 20 samples\n",
    "BATCH_SIZE =1 #32 was original, TF performance is same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to call API\n",
    "# user must place valid license token in header authorization\n",
    "def Send_to_API(top_layer_inputs, labels,num_outputs,model_name=\"test_model023\"):\n",
    "\n",
    "    top_layer_inputsL = top_layer_inputs.tolist() # inputs\n",
    "    labelsL = labels.tolist() #labels\n",
    "    num_outputsL = str(num_outputs) \n",
    "\n",
    "    payload = {\"model_name\":  model_name,'inputs': top_layer_inputsL, 'labels': labelsL,'num_outputs':num_outputsL}\n",
    "    headers = { 'content-type': 'application/json', 'accept': 'application/json','Authorization':'Bearer <token> }  # <token>\n",
    "\n",
    "    r = requests.post(url = 'https://om-learn-api.azurewebsites.net/api/train/', data = json.dumps(payload), headers=headers)\n",
    "    result = json.loads(r.text)\n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_from_API(data):\n",
    "    weights = np.array(data[\"result\"]) \n",
    "    return(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "now=datetime.now()\n",
    "date_time=now.strftime(\"%m-%d-%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11-20-2023'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (160, 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets up the directories automatically  \n",
    "_URL = 'https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('cats_and_dogs.zip', origin=_URL, extract=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tsvi\\.keras\\datasets\\cats_and_dogs_filtered\n"
     ]
    }
   ],
   "source": [
    "# Datasets to benchmark\n",
    "\n",
    "dataset_name='cats_and_dogs_filtered'  #originalTF notebook dataset\n",
    "#dataset_name='Animals_filtered'  # 50 animal dataset\n",
    "#dataset_name='fowl_data' # from https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-pytorch\n",
    "\n",
    "PATH2 = os.path.join(os.path.dirname(path_to_zip),dataset_name )    \n",
    "print(PATH2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tsvi\\.keras\\datasets\\cats_and_dogs_filtered \n",
      " C:\\Users\\Tsvi\\.keras\\datasets\\cats_and_dogs_filtered\\train \n",
      " C:\\Users\\Tsvi\\.keras\\datasets\\cats_and_dogs_filtered\\validation\n"
     ]
    }
   ],
   "source": [
    "train_dir = os.path.join(PATH2, 'train')\n",
    "validation_dir = os.path.join(PATH2, 'validation')\n",
    "\n",
    "print(PATH2,'\\n',train_dir,'\\n',validation_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = tf.keras.utils.image_dataset_from_directory(train_dir,\n",
    "                                                                         shuffle=True,\n",
    "                                                                         batch_size=BATCH_SIZE,\n",
    "                                                                         image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(validation_dir,\n",
    "                                                                              shuffle=True,\n",
    "                                                                              batch_size=BATCH_SIZE,\n",
    "                                                                              image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset class names: ['cats', 'dogs']\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset class names:\", train_dataset.class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of classes and outputs: 2\n"
     ]
    }
   ],
   "source": [
    "class_names = train_dataset.class_names\n",
    "num_outputs=len(class_names)\n",
    "print('number of classes and outputs:',num_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of validation batches: 800\n",
      "Number of test batches: 200\n"
     ]
    }
   ],
   "source": [
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 2000\n"
     ]
    }
   ],
   "source": [
    "print('Number of training batches: %d' % tf.data.experimental.cardinality(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1./127.5, offset= -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs = 1 #10  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = IMG_SIZE + (3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose base\n",
    "\n",
    "MODEL = 'MobileNetV2'\n",
    "#MODEL = 'ResNet50'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODEL=='MobileNetV2':\n",
    "    preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input\n",
    "    base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "elif MODEL=='ResNet50':\n",
    "    preprocess_input = tf.keras.applications.resnet50.preprocess_input\n",
    "    base_model = tf.keras.applications.ResNet50(input_shape=IMG_SHAPE,\n",
    "                                                include_top=False,\n",
    "                                                weights='imagenet')\n",
    "else:\n",
    "    error(\"Model Not supported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"mobilenetv2_1.00_160\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 160, 160, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 80, 80, 32)   864         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 80, 80, 32)   128         ['Conv1[0][0]']                  \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 80, 80, 32)   0           ['bn_Conv1[0][0]']               \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 80, 80, 32)  288         ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                                                                                      \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 80, 80, 32)  128         ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)                                                                                \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 80, 80, 32)  0           ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                                                           ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 80, 80, 16)  512         ['expanded_conv_depthwise_relu[0]\n",
      "                                                                 [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 80, 80, 16)  64          ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                                                                                  \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 80, 80, 96)   1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                                                 ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 80, 80, 96)  384         ['block_1_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 80, 80, 96)   0           ['block_1_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 81, 81, 96)   0           ['block_1_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 40, 40, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 40, 40, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 40, 40, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 40, 40, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 40, 40, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 40, 40, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 40, 40, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 40, 40, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 40, 40, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 40, 40, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 40, 40, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 40, 40, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 40, 40, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 40, 40, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 40, 40, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 40, 40, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 40, 40, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 41, 41, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 20, 20, 144)  1296       ['block_3_pad[0][0]']            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 20, 20, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 20, 20, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 20, 20, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 20, 20, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 20, 20, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 20, 20, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 20, 20, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 20, 20, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 20, 20, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 20, 20, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 20, 20, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 20, 20, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 20, 20, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 20, 20, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 20, 20, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 20, 20, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 20, 20, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 20, 20, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 20, 20, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 20, 20, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 20, 20, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 20, 20, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 20, 20, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 20, 20, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 20, 20, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 21, 21, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 10, 10, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 10, 10, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 10, 10, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 10, 10, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 10, 10, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 10, 10, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 10, 10, 384)  1536       ['block_7_expand[0][0]']         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 10, 10, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 10, 10, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 10, 10, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 10, 10, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 10, 10, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 10, 10, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 10, 10, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 10, 10, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 10, 10, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 10, 10, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 10, 10, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 10, 10, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 10, 10, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 10, 10, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 10, 10, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 10, 10, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 10, 10, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 10, 10, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 10, 10, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 10, 10, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 10, 10, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 10, 10, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 10, 10, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 10, 10, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 10, 10, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 10, 10, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 10, 10, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 10, 10, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 10, 10, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 10, 10, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 10, 10, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 10, 10, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " block_10_project_BN (BatchNorm  (None, 10, 10, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 10, 10, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 10, 10, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 10, 10, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 10, 10, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 10, 10, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 10, 10, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 10, 10, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 10, 10, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 10, 10, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 10, 10, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 10, 10, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 10, 10, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 10, 10, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 10, 10, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 10, 10, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 10, 10, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 10, 10, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 10, 10, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 10, 10, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 10, 10, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 10, 10, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 11, 11, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 5, 5, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 5, 5, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 5, 5, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 5, 5, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 5, 5, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 5, 5, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 5, 5, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 5, 5, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 5, 5, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 5, 5, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 5, 5, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 5, 5, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 5, 5, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 5, 5, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 5, 5, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 5, 5, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 5, 5, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 5, 5, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 5, 5, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 5, 5, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 5, 5, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 5, 5, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 5, 5, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 5, 5, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 5, 5, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 5, 5, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 5, 5, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 5, 5, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 5, 5, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 5, 5, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 5, 5, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 5, 5, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 5, 5, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 5, 5, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,257,984\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the base model architecture\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the original adds 2d global_average_pooling2d layer to make the base layer ready for classification.  \n",
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up learning and top layer, changed from binary to multiclass so softmax added to top\n",
    "final_output_layer=tf.keras.layers.Softmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the multiclass equivalent to original transfer learning network  \n",
    "prediction_layer4 = tf.keras.layers.Dense(num_outputs)  # change1 from original: I make num_outputs outputs\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = prediction_layer4(x)    # adds the 49 nodes\n",
    "outputs = final_output_layer(x)  # change2: adds softmax\n",
    "orig_TF_paradigm_model = tf.keras.Model(inputs, outputs)   # this is now the traditional leanring network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 3:SparseCategoricalCrossentropy from BinaryCrossentropy because it is multiclass\n",
    "\n",
    "base_learning_rate = 0.0001\n",
    "orig_TF_paradigm_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True,) # Now I compile it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 160, 160, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 160, 160, 3)       0         \n",
      "                                                                 \n",
      " tf.math.truediv (TFOpLambda  (None, 160, 160, 3)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " tf.math.subtract (TFOpLambd  (None, 160, 160, 3)      0         \n",
      " a)                                                              \n",
      "                                                                 \n",
      " mobilenetv2_1.00_160 (Funct  (None, 5, 5, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 2562      \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "orig_TF_paradigm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the version of the base model called base_layer that will feed inputs to OM layer \n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "outputs = global_average_layer(x)\n",
    "#outputs = prediction_layer(x)   # It is basically the same as the original notebook but no prediction layer\n",
    "base_layer = tf.keras.Model(inputs, outputs) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 160, 160, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 160, 160, 3)       0         \n",
      "                                                                 \n",
      " tf.math.truediv_1 (TFOpLamb  (None, 160, 160, 3)      0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " tf.math.subtract_1 (TFOpLam  (None, 160, 160, 3)      0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " mobilenetv2_1.00_160 (Funct  (None, 5, 5, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 0\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_layer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs=base_layer.output.shape[1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is an equivalent TF model that will recieve the OM weights\n",
    "prediction_layer2 = tf.keras.layers.Dense(num_outputs)  # top nodes\n",
    "inputs = tf.keras.Input(shape=(160, 160, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "x = prediction_layer2(x)  # nodes whose weights I will be changed by om\n",
    "outputs = final_output_layer(x)  #softmax\n",
    "tf_model_tobe_trained_by_OM = tf.keras.Model(inputs, outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it needs compile if I do validation but I never really do any learning so most of this doesnt really matter\n",
    "base_learning_rate = 0.0001\n",
    "tf_model_tobe_trained_by_OM.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  #loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True,) # needs to be added to prevent execution optimization (not data optimiziation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 160, 160, 3)]     0         \n",
      "                                                                 \n",
      " sequential (Sequential)     (None, 160, 160, 3)       0         \n",
      "                                                                 \n",
      " tf.math.truediv_2 (TFOpLamb  (None, 160, 160, 3)      0         \n",
      " da)                                                             \n",
      "                                                                 \n",
      " tf.math.subtract_2 (TFOpLam  (None, 160, 160, 3)      0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " mobilenetv2_1.00_160 (Funct  (None, 5, 5, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 2562      \n",
      "                                                                 \n",
      " softmax (Softmax)           (None, 2)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,260,546\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tf_model_tobe_trained_by_OM.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next section are shortcuts for validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs through stored top layer input validation set and gives validation %correct\n",
    "def validation_shortcut(x_in,y_train,u_model, num_outputs,verbose=False,report_progress=0,name='',fpfn=False): \n",
    "    if np.any(~np.isfinite(x_in)): print('ERROR indefinate value in X:',x_in)\n",
    "\n",
    "    if (len(x_in.shape)==1):  #only one dimentions\n",
    "        x_temp=np.zeros((1,x_in.shape[0]))\n",
    "        x_temp[0,:]=x_in\n",
    "        x_in=x_temp   \n",
    "    correct=0\n",
    "    incorrect=0  \n",
    "    fp=np.zeros(num_outputs)\n",
    "    fn=np.zeros(num_outputs)\n",
    "\n",
    "    if report_progress:\n",
    "        report=int(np.max([100, x_in.shape[0]/report_progress]))\n",
    "        print('Report every ',report)\n",
    "\n",
    "    remember_which=np.zeros([x_in.shape[0],1])\n",
    "\n",
    "    y_out=np.zeros((x_in.shape[0],num_outputs))\n",
    "    for i in range(x_in.shape[0]):         \n",
    "        if report_progress:\n",
    "            if (i % report)==0: print('On test number:',i)\n",
    "        x_test=prepare_input(x_in[i])  # assures arrays are the correct dimensions\n",
    "\n",
    "        res=(np.dot(u_model,x_test.T)).T\n",
    "        y_out[i,:]=res\n",
    "        dummy,indx=np.unravel_index(np.argsort(res, axis=1), res.shape)  #find most acive node(s)\n",
    "        if (indx[0,-1])==y_train[i]:\n",
    "            correct=correct+1\n",
    "            remember_which[i]=1\n",
    "        else: \n",
    "            incorrect=incorrect+1\n",
    "            # calculates false positives and negative but this is not displayed unless fpfn=True\n",
    "            fn[y_train[i]]=fn[y_train[i]]+1\n",
    "            fp[indx[0,-1]]=fp[indx[0,-1]]+1  \n",
    "\n",
    "\n",
    "    last_score_accuracy=correct/(i+1)\n",
    "    if fpfn:\n",
    "        print(name,'(Number correct, number incorrect, number of tests): (',correct,incorrect,i+1,') %Correct: ',100*last_score_accuracy,'%')\n",
    "\n",
    "        print('False Positives by label:',fp)\n",
    "        print('False Negatives by label:',fn)\n",
    "\n",
    "    return last_score_accuracy  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_input(x_in, verbose=False):  # make sure matrix dimensions are ok\n",
    "\n",
    "    if (len(x_in.shape)==1):  #only one dimenion\n",
    "        x_train=np.zeros((1,x_in.shape[0]))\n",
    "        x_train[0,:]=x_in\n",
    "    else:\n",
    "        x_train=x_in\n",
    "        if x_in.shape[0]>10: print('warning having many cases may eat up memory',x_in.shape[0])\n",
    "\n",
    "    if x_train.shape[1] != num_inputs:         # sanity check\n",
    "        print('Error: Raw input length does not match expected')\n",
    "\n",
    "    if verbose: \n",
    "        print('orig input ',(x_in),'prepared input',(x_train),'final matrix ',x_train.shape)\n",
    "        print('orig input dimensions',len(x_in.shape),'prepared input',len(x_train.shape),'final matrix ',x_train.shape)\n",
    "    return x_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800\n"
     ]
    }
   ],
   "source": [
    "# find the number of entries in validation set: does tf.data.experimental.cardinality(validation_dataset)) do this?\n",
    "# This works no matter what the batch size is\n",
    "num_data_in_validation_dataset=0\n",
    "for data, label in validation_dataset: #.as_numpy_iterator():\n",
    "    num_data_in_validation_dataset=num_data_in_validation_dataset+len(data)\n",
    "print(num_data_in_validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 615ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 149ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "# setup validation manually so we dont have to go through base for every validation (saves time because validation is the same)\n",
    "if base_inference_shortcut:\n",
    "    i=0\n",
    "    validation_labels=np.ones((1,num_data_in_validation_dataset))*50  #crazy initial values for debug\n",
    "    base_layer_output_for_validation=np.ones((num_data_in_validation_dataset,num_inputs))*50  \n",
    "\n",
    "    for data, label in validation_dataset: #.as_numpy_iterator():\n",
    "        #print(\"Batch\",i,\"out of\",num_data_in_validation_dataset,\"size:\",len(data),len(label))\n",
    "        len_batch=len(label)\n",
    "        #print(label)\n",
    "        validation_labels[0,(BATCH_SIZE*i):(BATCH_SIZE*(i)+len_batch)]=label  # set up validation truth table\n",
    "        temp=base_layer.predict(data)  #run bottom transfer layers pre-rfn\n",
    "        base_layer_output_for_validation[BATCH_SIZE*i:(BATCH_SIZE*i+len_batch),:]=temp[0:len_batch,:]\n",
    "        i=i+1\n",
    "    validation_labels=validation_labels.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up validation intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# does not validate at every point for graphing (otherwise things take forever)\n",
    "# test first 30 data points only.  This takes long enough verification is done in for each model independently\n",
    "#validate_points=np.concatenate([np.arange(0,30,3)+2])\n",
    "\n",
    "# extended validations used to graph charts in Optimizing Mind video and briefs\n",
    "#validate_points=np.concatenate([np.arange(30),np.arange(30,40,2),np.arange(40,60,3),np.arange(60,100,5),np.arange(100,300,7),np.arange(300,700,10),np.arange(700,1500,15),np.arange(1500,3000,20)])\n",
    "validate_points=np.concatenate([np.arange(30),np.arange(30,40,2),np.arange(40,60,3)]) #,np.arange(60,100,5)]) #,np.arange(100,300,7),np.arange(300,700,10),np.arange(700,1500,15),np.arange(1500,3000,20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 32, 34, 36,\n",
       "       38, 40, 43, 46, 49, 52, 55, 58])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validate_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sample=max(validate_points)\n",
    "graph_points=list(validate_points.copy()) # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of data of train batches in a single epoch 2000 batch size = 1 epochs= 1\n"
     ]
    }
   ],
   "source": [
    "# variables to store performance of batches for both approaches\n",
    "num_batches_in_train_dataset=len(train_dataset)              # number of batches in train_dataset\n",
    "print(\"num of data of train batches in a single epoch\",num_batches_in_train_dataset,\"batch size =\",BATCH_SIZE,\"epochs=\",initial_epochs)\n",
    "OM_learn_Vacc=[] \n",
    "tf_learn_Vacc=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=False\n",
    "labels_presented=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "benchmark cats_and_dogs_filtered using MobileNetV2 11-20-2023 01:54\n"
     ]
    }
   ],
   "source": [
    "model_name='benchmark '+dataset_name+' using '+MODEL+now.strftime(\" %m-%d-%Y %H:%M\")\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Batch Sample 0, Epoch 0, 1 entries accuracy of OM 0.4912 and TF 0.4962 \n",
      "Training: Batch Sample 1, Epoch 0, 1 entries accuracy of OM 0.9188 and TF 0.5 \n",
      "Training: Batch Sample 2, Epoch 0, 1 entries accuracy of OM 0.7438 and TF 0.5025 \n",
      "Training: Batch Sample 3, Epoch 0, 1 entries accuracy of OM 0.8475 and TF 0.5038 \n",
      "Training: Batch Sample 4, Epoch 0, 1 entries accuracy of OM 0.8988 and TF 0.505 \n",
      "Training: Batch Sample 5, Epoch 0, 1 entries accuracy of OM 0.835 and TF 0.5062 \n",
      "Training: Batch Sample 6, Epoch 0, 1 entries accuracy of OM 0.91 and TF 0.5075 \n",
      "Training: Batch Sample 7, Epoch 0, 1 entries accuracy of OM 0.9138 and TF 0.5075 \n",
      "Training: Batch Sample 8, Epoch 0, 1 entries accuracy of OM 0.935 and TF 0.5088 \n",
      "Training: Batch Sample 9, Epoch 0, 1 entries accuracy of OM 0.9325 and TF 0.5062 \n",
      "Training: Batch Sample 10, Epoch 0, 1 entries accuracy of OM 0.9338 and TF 0.5088 \n",
      "Training: Batch Sample 11, Epoch 0, 1 entries accuracy of OM 0.93 and TF 0.51 \n",
      "Training: Batch Sample 12, Epoch 0, 1 entries accuracy of OM 0.9375 and TF 0.5112 \n",
      "Training: Batch Sample 13, Epoch 0, 1 entries accuracy of OM 0.9388 and TF 0.5112 \n",
      "Training: Batch Sample 14, Epoch 0, 1 entries accuracy of OM 0.9338 and TF 0.5125 \n",
      "Training: Batch Sample 15, Epoch 0, 1 entries accuracy of OM 0.9312 and TF 0.515 \n",
      "Training: Batch Sample 16, Epoch 0, 1 entries accuracy of OM 0.9362 and TF 0.5162 \n",
      "Training: Batch Sample 17, Epoch 0, 1 entries accuracy of OM 0.9362 and TF 0.5175 \n",
      "Training: Batch Sample 18, Epoch 0, 1 entries accuracy of OM 0.945 and TF 0.5162 \n",
      "Training: Batch Sample 19, Epoch 0, 1 entries accuracy of OM 0.9388 and TF 0.5138 \n",
      "Training: Batch Sample 20, Epoch 0, 1 entries accuracy of OM 0.9375 and TF 0.5125 \n",
      "Training: Batch Sample 21, Epoch 0, 1 entries accuracy of OM 0.9412 and TF 0.5138 \n",
      "Training: Batch Sample 22, Epoch 0, 1 entries accuracy of OM 0.945 and TF 0.5188 \n",
      "Training: Batch Sample 23, Epoch 0, 1 entries accuracy of OM 0.9388 and TF 0.5212 \n",
      "Training: Batch Sample 24, Epoch 0, 1 entries accuracy of OM 0.9362 and TF 0.5212 \n",
      "Training: Batch Sample 25, Epoch 0, 1 entries accuracy of OM 0.9362 and TF 0.5238 \n",
      "Training: Batch Sample 26, Epoch 0, 1 entries accuracy of OM 0.945 and TF 0.5238 \n",
      "Training: Batch Sample 27, Epoch 0, 1 entries accuracy of OM 0.95 and TF 0.5312 \n",
      "Training: Batch Sample 28, Epoch 0, 1 entries accuracy of OM 0.95 and TF 0.5362 \n",
      "Training: Batch Sample 29, Epoch 0, 1 entries accuracy of OM 0.9512 and TF 0.5438 \n",
      "Training: Batch Sample 30, Epoch 0, 1 entries accuracy of OM 0.95 and TF 0.5462 \n",
      "Training: Batch Sample 32, Epoch 0, 1 entries accuracy of OM 0.9562 and TF 0.555 \n",
      "Training: Batch Sample 34, Epoch 0, 1 entries accuracy of OM 0.955 and TF 0.5588 \n",
      "Training: Batch Sample 36, Epoch 0, 1 entries accuracy of OM 0.9525 and TF 0.5625 \n",
      "Training: Batch Sample 38, Epoch 0, 1 entries accuracy of OM 0.9525 and TF 0.5638 \n",
      "Training: Batch Sample 40, Epoch 0, 1 entries accuracy of OM 0.9538 and TF 0.5688 \n",
      "Training: Batch Sample 43, Epoch 0, 1 entries accuracy of OM 0.95 and TF 0.5775 \n",
      "Training: Batch Sample 46, Epoch 0, 1 entries accuracy of OM 0.96 and TF 0.5812 \n",
      "Training: Batch Sample 49, Epoch 0, 1 entries accuracy of OM 0.965 and TF 0.5888 \n",
      "Training: Batch Sample 52, Epoch 0, 1 entries accuracy of OM 0.9638 and TF 0.5962 \n",
      "Training: Batch Sample 55, Epoch 0, 1 entries accuracy of OM 0.965 and TF 0.6038 \n",
      "Training: Batch Sample 58, Epoch 0, 1 entries accuracy of OM 0.965 and TF 0.6125 \n"
     ]
    }
   ],
   "source": [
    "for j in range(initial_epochs):\n",
    "    if j>0:  #update graph points if doing more than one epoch\n",
    "        validate_points=np.concatenate([np.arange(0,3000,30)])\n",
    "        graph_points[num_stored:]=[]\n",
    "        graph_points=graph_points+list(validate_points.copy()+len(labels_presented)+1)   #need to figure out graphing stuff\n",
    "\n",
    "    for i, data in train_dataset.enumerate():   \n",
    "\n",
    "        # stop if there will be no more validations\n",
    "        if i>max_sample:\n",
    "            break\n",
    "            \n",
    "        labels_presented.append(data[1])  # store record of labels presented\n",
    "                \n",
    "        #the original learning        \n",
    "        batch_logs = orig_TF_paradigm_model.train_step(data)\n",
    "                \n",
    "        # prepare data to send out to OM.        \n",
    "        net_out = base_layer.predict_on_batch(data[0])  # run through base layer to have data ready for OM\n",
    "                \n",
    "        # call to the API\n",
    "        json_back=Send_to_API(net_out, data[1].numpy(),num_outputs,model_name=model_name)  # optional: MODEL,dataset_name, \n",
    "                \n",
    "        # extract results from API\n",
    "        outfrom_Get_from_API=Get_from_API(json_back) \n",
    "        \n",
    "        # put results into array\n",
    "        u_model=np.array(outfrom_Get_from_API).T\n",
    "        \n",
    "        try:   # verify recieved a float values\n",
    "            u_model=np.float32(u_model) # cast array into float\n",
    "        except: # otherwise error from API\n",
    "            print('API error: ',outfrom_Get_from_API)\n",
    "            break\n",
    "                \n",
    "        # put weights that come back into final OM generated transfer learning network: \n",
    "        tf_model_tobe_trained_by_OM.trainable_weights[0].assign(tf.Variable(u_model)) \n",
    "        # this is now the complete OM learned net \n",
    "        \n",
    "        # validation code run in specified validation points\n",
    "        if i in validate_points:  # minimizing number of validations because it takes too long\n",
    "            \n",
    "            \n",
    "            if base_inference_shortcut:  # validation of only the top layer: shortcut (saves time)\n",
    "                # OM\n",
    "                accuracy_OMTF=validation_shortcut(base_layer_output_for_validation,validation_labels[0,:],np.float32(u_model).T, num_outputs,report_progress=0, name='OM')\n",
    "                # TF\n",
    "                accuracy_=validation_shortcut(base_layer_output_for_validation,validation_labels[0,:],np.float32(orig_TF_paradigm_model.trainable_weights[0]).T, num_outputs,report_progress=0,name='TF')\n",
    "            \n",
    "            \n",
    "            else:    # validation the original, long way, via no shortcut inference \n",
    "                # validating TF\n",
    "                loss_, accuracy_ = orig_TF_paradigm_model.evaluate(validation_dataset)\n",
    "\n",
    "                # validating OM \n",
    "                loss0, accuracy_OMTF = tf_model_tobe_trained_by_OM.evaluate(validation_dataset)  # it is validated and run just like the original TF\n",
    "\n",
    "                \n",
    "            # adding accuracies to records\n",
    "            tf_learn_Vacc.append(accuracy_)  \n",
    "            OM_learn_Vacc.append(accuracy_OMTF)\n",
    "\n",
    "            print(\"Training: Batch Sample {}, Epoch {}, {} entries accuracy of OM {} and TF {} \".format(i,j, len(data[0]), np.round(OM_learn_Vacc[-1],4),np.round(accuracy_,4)))\n",
    "            \n",
    "    \n",
    "    num_stored=len(OM_learn_Vacc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stored=len(OM_learn_Vacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first training instance where with all animals were presented at least once 2 first validation instance 2\n"
     ]
    }
   ],
   "source": [
    "# indicate when all animals are presented at least once\n",
    "all_presented=1\n",
    "while len(np.unique(labels_presented[0:all_presented])) != len(np.unique(labels_presented)):\n",
    "    all_presented=all_presented+1\n",
    "all_presented=np.where(np.array(graph_points) >= all_presented)[0][0]   #first index where all presented\n",
    "print('first training instance where with all animals were presented at least once',all_presented,'first validation instance',graph_points[all_presented])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stored=len(graph_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "grange=num_stored-1 # #31 #90 #121  # choosing a nice zoom range\n",
    "if grange==0:\n",
    "    grange=num_stored-1\n",
    "maxpoint=max(graph_points[0:grange])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 58 plots\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels   [[1 0 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 0 0 1 0 1 0 1 0 1 0 1 0 0 1 1 0 0 0 0\n",
      "  0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# plot figure\n",
    "print('first',graph_points[grange],'plots')\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "\n",
    "plt.plot(graph_points[0:(all_presented+1)],OM_learn_Vacc[0:(all_presented+1)],color='darkblue')\n",
    "plt.plot(graph_points[all_presented:grange],OM_learn_Vacc[all_presented:grange],color='cornflowerblue', label='OM Accuracy NEW')\n",
    "plt.plot(graph_points[0:grange],tf_learn_Vacc[0:grange],color='orange', label='TF Accuracy NEW')\n",
    "\n",
    "plt.legend(loc='right')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.ylim([min(plt.ylim()),1])\n",
    "#plt.ylim([0.35,1.0])\n",
    "plt.title(\"OM vs TF {} Accuracy within {} Batch={} Example(s) Trained\".format(dataset_name,graph_points[grange],BATCH_SIZE))\n",
    "\n",
    "plt.xlabel('Batch Number')\n",
    "plt.show()\n",
    "print('Labels  ',np.array(labels_presented)[0:grange].T)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
